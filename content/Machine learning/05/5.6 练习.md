5.1 软边缘超平面。用于软边缘超平面优化问题的松弛变量的函数形式为:$\xi \mapsto \mathop{\sum }\limits_{{i = 1}}^{m}{\xi }_{i}$。相反，我们可以使用$\xi \mapsto \mathop{\sum }\limits_{{i = 1}}^{m}{\xi }_{i}^{p}$，其中$p > 1$。

(a)给出这个一般情况下的对偶问题。

(b)这种更一般的公式$\left( {p > 1}\right)$与标准设置$\left( {p = 1}\right)$相比如何？在$p = 2$的情况下，优化仍然是凸的吗？

5.2 更紧的Rademacher界。推导定理5.9的以下更紧版本界限:对于任意的 $\delta > 0$ ，在至少 $1 - \delta$ 的概率下，对于所有的 $h \in \mathcal{H}$ 和 $\rho \in (0,1\rbrack$ ，以下成立:

$$
R\left( h\right) \leq {\widehat{R}}_{S,\rho }\left( h\right) + \frac{2\gamma }{\rho }{\Re }_{m}\left( \mathcal{H}\right) + \sqrt{\frac{\log {\log }_{\gamma }\frac{\gamma }{\rho }}{m}} + \sqrt{\frac{\log \frac{2}{\delta }}{2m}} \tag{5.49}
$$

对于任意的 $\gamma > 1$ 。

5.3 加权重要性的支持向量机(SVM)。假设你希望使用SVM来解决一个学习问题，其中某些训练数据点比其他点更重要。更正式地说，假设每个训练点由一个三元组 $\left( {{x}_{i},{y}_{i},{p}_{i}}\right)$ 组成，其中 $0 \leq {p}_{i} \leq 1$ 是第 $i$ 个点的重要性。重写原始SVM约束优化问题，使得错误标记一个点 ${x}_{i}$ 的惩罚按优先级 ${p}_{i}$ 缩放。然后将这个修改带到对偶解的推导中。

5.4 序贯最小优化(SMO)。SMO算法是一种优化算法，用于加速SVM的训练。SMO将一个(潜在的)大型二次规划(QP)优化问题简化为一系列只涉及两个拉格朗日乘子的优化。SMO减少了内存需求，绕过了数值QP优化的需要，并且易于实现。在这个问题中，我们将推导出在SVM问题的对偶形式中SMO算法的更新规则。

(a)假设我们只想优化方程(5.33)中的 ${\alpha }_{1}$ 和 ${\alpha }_{2}$ 。证明优化问题简化为

$$
\mathop{\max }\limits_{{{\alpha }_{1},{\alpha }_{2}}}\underset{{\Psi }_{1}\left( {{\alpha }_{1},{\alpha }_{2}}\right) }{\underbrace{{\alpha }_{1} + {\alpha }_{2} - \frac{1}{2}{K}_{11}{\alpha }_{1}^{2} - \frac{1}{2}{K}_{22}{\alpha }_{2}^{2} - s{K}_{12}{\alpha }_{1}{\alpha }_{2} - {y}_{1}{\alpha }_{1}{v}_{1} - {y}_{2}{\alpha }_{2}{v}_{2}}}
$$

约束条件: $0 \leq {\alpha }_{1},{\alpha }_{2} \leq C \land {\alpha }_{1} + s{\alpha }_{2} = \gamma$ ，

其中 $\gamma = {y}_{1}\mathop{\sum }\limits_{{i = 3}}^{m}{y}_{i}{\alpha }_{i}, s = {y}_{1}{y}_{2} \in \{ - 1, + 1\} ,{K}_{ij} = \left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right)$ 和 ${v}_{i} =$ $\mathop{\sum }\limits_{{j = 3}}^{m}{\alpha }_{j}{y}_{j}{K}_{ij}$ 对于 $i = 1,2$ 。

(b)将线性约束 ${\alpha }_{1} = \gamma - s{\alpha }_{2}$ 代入 ${\Psi }_{1}$ 以获得一个新的目标函数 ${\Psi }_{2}$ ，该函数仅依赖于 ${\alpha }_{2}$ 。证明使 ${\Psi }_{2}$ 最小化的 ${\alpha }_{2}$ (在不考虑约束 $\left. {0 \leq {\alpha }_{1},{\alpha }_{2} \leq C}\right)$ 的情况下)可以表示为

$$
{\alpha }_{2} = \frac{s\left( {{K}_{11} - {K}_{12}}\right) \gamma + {y}_{2}\left( {{v}_{1} - {v}_{2}}\right) - s + 1}{\eta },
$$

其中 $\eta = {K}_{11} + {K}_{22} - 2{K}_{12}$ .

(c) 证明

$$
{v}_{1} - {v}_{2} = f\left( {\mathbf{x}}_{1}\right) - f\left( {\mathbf{x}}_{2}\right) + {\alpha }_{2}^{ * }{y}_{2}\eta - s{y}_{2}\gamma \left( {{K}_{11} - {K}_{12}}\right)
$$

其中 $f\left( \mathbf{x}\right) = \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}^{ * }{y}_{i}\left( {{\mathbf{x}}_{i} \cdot \mathbf{x}}\right) + {b}^{ * }$ 和 ${\alpha }_{i}^{ * }$ 是在 ${\alpha }_{1}$ 和 ${\alpha }_{2}$ 优化之前的拉格朗日乘数值(同样，${b}^{ * }$ 是偏移量之前的值)。

(d) 证明

$$
{\alpha }_{2} = {\alpha }_{2}^{ * } + {y}_{2}\frac{\left( {{y}_{2} - f\left( {\mathbf{x}}_{2}\right) }\right) - \left( {{y}_{1} - f\left( {\mathbf{x}}_{1}\right) }\right) }{\eta }.
$$

(e) 对于 $s = + 1$ ，定义 $L = \max \{ 0,\gamma - C\}$ 和 $H = \min \{ C,\gamma \}$ 为 ${\alpha }_{2}$ 的下界和上界。类似地，对于 $s = - 1$ ，定义 $L = \max \{ 0, - \gamma \}$ 和 $H = \min \{ C, C - \gamma \}$ 。SMO的更新规则涉及对 $s = + 1$ 值的“剪辑”

即，

$$
{\alpha }_{2}^{clip} = \left\{ {\begin{array}{ll} {\alpha }_{2} & \text{if }L < {\alpha }_{2} < H \\ L & \text{if }{\alpha }_{2} \leq L \\ H & \text{if }{\alpha }_{2} \geq H \end{array}.}\right.
$$

我们随后求解 ${\alpha }_{1}$ ，以满足等式约束，从而得到 ${\alpha }_{1} = {\alpha }_{1}^{ * } + s\left( {{\alpha }_{2}^{ * } - {\alpha }_{2}^{\text{clip }}}\right)$ 。为什么需要“剪辑”？在 $s = + 1$ 的情况下，$L$ 和 $H$ 是如何推导出来的？

5.5 SVMs 实践。

(a) 从以下地址下载并安装 libsvm 软件库:

$$
\text{http://www.csie.ntu.edu.tw/~cjlin/libsvm/.}
$$

(b) 下载位于以下位置的 satimage 数据集:

$$
\text{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.}
$$

将训练集和验证集合并为一个。从此我们将这个集合称为训练集。归一化训练和测试向量。

(c) 考虑二元分类问题，即区分类别6和其他数据点。使用结合多项式核的SVM(见第6章)来解决这个分类问题。为此，将训练数据随机分成十个大小相等的互不相交的集合。对于每个多项式度数 $d = 1,2,3,4$ ，绘制平均交叉验证误差加上或减去一个标准差作为 $C$ 的函数。让libsvm中多项式核的其他参数 $\gamma$ 和 $c$ 保持默认值1。报告在验证集上测量的最佳折中常数 $C$ 的值。

(d) 设 $\left( {{C}^{ * },{d}^{ * }}\right)$ 为之前找到的最佳对。将 $C$ 固定为 ${C}^{ * }$ 。绘制十折交叉验证的训练和测试误差，作为 $d$ 的函数得到的假设。绘制 $d$ 的函数，得到支持向量的平均数量的图像。

(e) 有多少支持向量位于边缘超平面上？

(f) 在标准二分类问题中，正类和负类上的错误以相同的方式处理。然而，假设我们希望对负点上的错误(假阳性错误)$k > 0$ 倍于正点上的错误进行惩罚。给出相应于以这种方式修改的SVM的对偶优化问题。

(g) 假设 $k$ 是一个整数。展示如何在不编写任何额外代码的情况下使用libsvm找到刚刚描述的修改后SVM的解。

(h) 将修改后的SVM应用于之前研究过的分类任务，并与 $k = 2,4,8,{16}$ 的先前SVM结果进行比较。

5.6 稀疏SVM。可以提出两种支持SVM算法的论点:一种基于支持向量的稀疏性，另一种基于边缘的概念。假设我们不是选择最大化边缘，而是选择通过最小化 ${L}_{p}$ 范数来最大化稀疏性，该范数定义了权重向量 $\mathbf{\alpha }$ 的向量 $\mathbf{w}$ ，对于某些 $p \geq 1$ 。首先，考虑 $p = 2$ 的情况。这给出了以下优化问题:

$$
\mathop{\min }\limits_{{\mathbf{\alpha }, b}}\frac{1}{2}\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}^{2} + C\mathop{\sum }\limits_{{i = 1}}^{m}{\xi }_{i} \tag{5.50}
$$

$$
\text{subject to}{y}_{i}\left( {\mathop{\sum }\limits_{{j = 1}}^{m}{\alpha }_{j}{y}_{j}{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j} + b}\right) \geq 1 - {\xi }_{i}, i \in \left\lbrack m\right\rbrack
$$

$$
{\xi }_{i},{\alpha }_{i} \geq 0, i \in \left\lbrack m\right\rbrack \text{.}
$$

(a) 证明在 $\mathbf{\alpha }$ 的非负约束下，该问题与SVM原优化问题的一个实例相一致。

(b) 推导 (5.50) 问题的对偶优化。

(c) 设置 $p = 1$ 将导致 $\mathbf{\alpha }$ 更加稀疏。推导这种情况下的对偶优化。

5.7 标准超平面的 VC 维数。这个问题的目标是推导出标准超平面 VC 维数的上界，该上界不依赖于

特征空间的维度。设 $S \subseteq \{ \mathbf{x} : \parallel \mathbf{x}\parallel \leq r\}$ 。我们将证明集合的标准超平面的 VC 维数 $d$ $\left\{ {x \mapsto \operatorname{sgn}\left( {\mathbf{w} \cdot \mathbf{x}}\right) : \mathop{\min }\limits_{{x \in S}} \mid \mathbf{w}}\right.$ 。 $\mathbf{x} \mid = 1 \land \parallel \mathbf{w}\parallel \leq \Lambda \}$ 验证

$$
d \leq {r}^{2}{\Lambda }^{2} \tag{5.51}
$$

(a) 设 $\left\{ {{\mathbf{x}}_{1},\ldots ,{\mathbf{x}}_{d}}\right\}$ 是一个可以被击碎的集合。证明对于所有的 $\mathbf{y} =$ $\left( {{y}_{1},\ldots ,{y}_{d}}\right) \in \{ - 1, + 1{\} }^{d}, d \leq \Lambda \begin{Vmatrix}{\mathop{\sum }\limits_{{i = 1}}^{d}{y}_{i}{\mathbf{x}}_{i}}\end{Vmatrix}.$

(b) 使用标签的随机化 $\mathbf{y}$ 和 Jensen 不等式来证明 $d \leq \Lambda \sqrt{\mathop{\sum }\limits_{{i = 1}}^{d}{\begin{Vmatrix}{\mathbf{x}}_{i}\end{Vmatrix}}^{2}}$

(c) 得出结论 $d \leq {r}^{2}{\Lambda }^{2}$ 。