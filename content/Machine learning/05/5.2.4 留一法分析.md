### 5.2.4 留一法分析

我们现在使用留一法错误的概念来导出基于训练集中支持向量比例的第一个SVM学习保证。

定义5.2(留一法错误)令 ${h}_{S}$ 表示学习算法 $\mathcal{A}$ 在固定样本 $S$ 上训练时返回的假设。那么，$\mathcal{A}$ 在大小为 $m$ 的样本 $S$ 上的留一法错误定义为:

$$
{\widehat{R}}_{\mathrm{{LOO}}}\left( \mathcal{A}\right) = \frac{1}{m}\mathop{\sum }\limits_{{i = 1}}^{m}{1}_{{h}_{S-\{ {x}_{i}\} }\left( {x}_{i}\right) \neq {y}_{i}}.
$$

因此，对于每一个 $i \in \left\lbrack m\right\rbrack ,\mathcal{A}$ 都在除了 ${x}_{i}$ 之外的所有 $S$ 上的点进行训练，即 $S - \left\{ {x}_{i}\right\}$ ，然后使用 ${x}_{i}$ 计算其误差。留一法误差是这些误差的平均值。我们将使用留一法误差的一个重要性质，如下面的引理所述。

引理5.3 对于大小为 $m \geq 2$ 的样本的留一法误差的平均值是无偏估计，用于大小为 $m - 1$ 的样本的平均泛化误差:

$$
\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{E}}\left\lbrack {{\widehat{R}}_{\mathrm{{LOO}}}\left( \mathcal{A}\right) }\right\rbrack = \underset{{S}^{\prime } \sim {\mathcal{D}}^{m - 1}}{\mathbb{E}}\left\lbrack {R\left( {h}_{{S}^{\prime }}\right) }\right\rbrack \tag{5.20}
$$

其中 $\mathcal{D}$ 表示从中抽取点的分布。

证明:由于期望的线性，我们可以写出

$$
\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{E}}\left\lbrack {{\widehat{R}}_{\mathrm{{LOO}}}\left( \mathcal{A}\right) }\right\rbrack = \frac{1}{m}\mathop{\sum }\limits_{{i = 1}}^{m}\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{E}}\left\lbrack {1}_{{h}_{S-\{ {x}_{i}\} }\left( {x}_{i}\right) \neq {y}_{i}}\right\rbrack
$$

$$
= \underset{S \sim {\mathcal{D}}^{m}}{\mathbb{E}}\left\lbrack {1}_{{h}_{S-\{ {x}_{1}\} }\left( {x}_{1}\right) \neq {y}_{1}}\right\rbrack
$$

$$
= \underset{{S}^{\prime } \sim {\mathcal{D}}^{m - 1},{x}_{1} \sim \mathcal{D}}{\mathbb{E}}\left\lbrack {1}_{{h}_{{S}^{\prime }}\left( {x}_{1}\right) \neq {y}_{1}}\right\rbrack
$$

$$
= \underset{{S}^{\prime } \sim {\mathcal{D}}^{m - 1}}{\mathbb{E}}\left\lbrack {\underset{{x}_{1} \sim \mathcal{D}}{\mathbb{E}}\left\lbrack {1}_{{h}_{{S}^{\prime }}\left( {x}_{1}\right) \neq {y}_{1}}\right\rbrack }\right\rbrack
$$

$$
= \underset{{S}^{\prime } \sim {\mathcal{D}}^{m - 1}}{\mathbb{E}}\left\lbrack {R\left( {h}_{{S}^{\prime }}\right) }\right\rbrack
$$

对于第二个等式，我们使用了这样一个事实:由于 $S$ 的点是独立同分布地抽取的，所以期望 ${\mathbb{E}}_{S \sim {\mathcal{D}}^{m}}\left\lbrack {1}_{{h}_{S-\{ {x}_{i}\} }\left( {x}_{i}\right) \neq {y}_{i}}\right\rbrack$ 不依赖于 $i \in \left\lbrack m\right\rbrack$ 的选择，因此等于 ${\mathbb{E}}_{S \sim {\mathcal{D}}^{m}}\left\lbrack {1}_{{h}_{S-\{ {x}_{1}\} }\left( {x}_{1}\right) \neq {y}_{1}}\right\rbrack$ 。

通常，计算留一法误差可能是昂贵的，因为它需要在大小为 $m - 1$ 的样本上训练 $m$ 次。然而，在某些情况下，可以更有效地推导出 ${\widehat{R}}_{\mathrm{{LOO}}}\left( \mathcal{A}\right)$ 的表达式(参见练习11.9)。

定理5.4 设 ${h}_{S}$ 是SVMs对于样本 $S$ 返回的假设，并且设 ${N}_{SV}\left( S\right)$ 是定义 ${h}_{S}$ 的支持向量的数量。那么，

$$
\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{E}}\left\lbrack {R\left( {h}_{S}\right) }\right\rbrack \leq \underset{S \sim {\mathcal{D}}^{m + 1}}{\mathbb{E}}\left\lbrack \frac{{N}_{\mathrm{{SV}}}\left( S\right) }{m + 1}\right\rbrack .
$$

证明:设 $S$ 是线性可分样本 $m + 1$ 。如果 $x$ 不是 ${h}_{S}$ 的支持向量，移除它不会改变SVM的解。因此， ${h}_{S-\{ x\} } = {h}_{S}$ 和 ${h}_{S-\{ x\} }$ 正确分类 $x$ 。通过反证法，如果 ${h}_{S-\{ x\} }$ 错误分类 $x, x$ 必须是一个支持向量，这意味着

$$
{\widehat{R}}_{\mathrm{{LOO}}}\left( \mathrm{{SVM}}\right) \leq \frac{{N}_{\mathrm{{SV}}}\left( S\right) }{m + 1}. \tag{5.21}
$$

对两边取期望并使用引理5.3得到结果。

定理5.4为支持向量机提供了一个稀疏性论证:算法的平均误差被支持向量的平均分数上界。人们可能希望对于实践中看到的许多分布，相对较少的训练点将位于边缘超平面上。那么解在意义上将是稀疏的，即一小部分对偶变量 ${\alpha }_{i}$ 将是非零的。

![0192515b-435f-75ef-9b27-37409ba7b98f_8_516_241_768_479_0.jpg](images/0192515b-435f-75ef-9b27-37409ba7b98f_8_516_241_768_479_0.jpg)

图5.4

一个分离超平面，点 ${\mathbf{x}}_{i}$ 被错误分类，而点 ${\mathbf{x}}_{j}$ 被正确分类，但边缘小于1。

零。然而，请注意，这个界限相对较弱，因为它仅适用于所有大小为 $m$ 的样本的算法平均泛化误差。它没有提供关于泛化误差方差的信息。在5.4节中，我们使用基于边缘概念的不同论证，提出了更强的高概率界限。