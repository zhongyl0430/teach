## 5.2.1 原始优化问题

我们现在推导出定义SVM解决方案的方程和优化问题。根据几何间隔的定义(也见图5.2)，分隔超平面的最大间隔$\rho$由以下公式给出

$$
\rho = \mathop{\max }\limits_{{\mathbf{w}, b : {y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) \geq 0}}\mathop{\min }\limits_{{i \in \left\lbrack m\right\rbrack }}\frac{\left| \mathbf{w} \cdot {\mathbf{x}}_{i} + b\right| }{\parallel \mathbf{w}\parallel } = \mathop{\max }\limits_{{\mathbf{w}, b}}\mathop{\min }\limits_{{i \in \left\lbrack m\right\rbrack }}\frac{{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) }{\parallel \mathbf{w}\parallel }. \tag{5.5}
$$

第二个等式源于这样一个事实:由于样本是线性可分的，对于最大化对$\left( {\mathbf{w}, b}\right) ,{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right)$来说，必须对所有$i \in \left\lbrack m\right\rbrack$非负。现在，注意到最后一个表达式对于$\left( {\mathbf{w}, b}\right)$乘以正数标量是不变的。因此，我们可以限制自己考虑那些使得$\mathop{\min }\limits_{{i \in \left\lbrack m\right\rbrack }}{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1 :$的成对$\left( {\mathbf{w}, b}\right)$。

$$
\rho = \mathop{\max }\limits_{\substack{{\mathbf{w}, b : } \\ {\mathop{\min }\limits_{{i \in \left\lbrack m\right\rbrack }}{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1} }}\frac{1}{\parallel \mathbf{w}\parallel } = \mathop{\max }\limits_{\substack{{\mathbf{w}, b : } \\ {\forall i \in \left\lbrack m\right\rbrack ,{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) \geq 1} }}\frac{1}{\parallel \mathbf{w}\parallel }. \tag{5.6}
$$

第二个等式源于这样一个事实:对于最大化对$\left( {\mathbf{w}, b}\right)$，${y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right)$的最小值是1。

图5.3展示了最大化(5.6)的解$\left( {\mathbf{w}, b}\right)$。除了最大间隔超平面外，它还显示了边缘超平面，这些是

![0192515b-435f-75ef-9b27-37409ba7b98f_3_555_239_699_414_0.jpg](images/0192515b-435f-75ef-9b27-37409ba7b98f_3_555_239_699_414_0.jpg)

图5.3

(5.6)的最大间隔超平面解。边际超平面在图中由虚线表示。

与分离超平面平行且通过负侧或正侧最近点的超平面。由于它们与分离超平面平行，因此具有相同的法向量 $\mathbf{w}$ 。此外，由于 $\left| {\mathbf{w} \cdot \mathbf{x} + b}\right| = 1$ 对于最近点，边际超平面的方程为 $\mathbf{w} \cdot \mathbf{x} + b = \pm 1$ 。

由于最大化 $1/\parallel \mathbf{w}\parallel$ 等同于最小化 $\frac{1}{2}\parallel \mathbf{w}{\parallel }^{2}$ ，考虑到 (5.6)，SVM 在可分情况下返回的 $\left( {\mathbf{w}, b}\right)$ 对是以下凸优化问题的解:

$$
\mathop{\min }\limits_{{\mathbf{w}, b}}\frac{1}{2}\parallel \mathbf{w}{\parallel }^{2} \tag{5.7}
$$

$$
\text{subject to:}{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) \geq 1,\forall i \in \left\lbrack m\right\rbrack \text{.}
$$

目标函数 $F : \mathbf{w} \mapsto \frac{1}{2}\parallel \mathbf{w}{\parallel }^{2}$ 是无限可微的。它的梯度是 $\nabla F\left( \mathbf{w}\right) = \mathbf{w}$ ，它的Hessian矩阵是单位矩阵 ${\nabla }^{2}F\left( \mathbf{w}\right) = \mathbf{I}$ ，其特征值均为严格正数。因此， ${\nabla }^{2}F\left( \mathbf{w}\right) \succ \mathbf{0}$ 且 $F$ 是严格凸的。约束都由仿射函数 ${g}_{i} : \left( {\mathbf{w}, b}\right) \mapsto 1 - {y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right)$ 定义，因此是合格的。因此，考虑到凸优化已知的结果(详见附录B)，(5.7)的优化问题具有唯一解，这是一个重要且有利的特点，并非所有学习算法都具备。

此外，由于目标函数是二次的，约束是仿射的，(5.7)的优化问题实际上是二次规划(QP)的一个特例，QP 是优化领域中广泛研究的一类问题。有许多商业和开源求解器可用于解决凸QP问题。另外，受到SVM在经验上的成功及其丰富的理论基础的启发，已经开发了专门的方法来更有效地解决这个特定的凸QP问题，尤其是只有两个坐标的块坐标下降算法。

### 5.2.2 支持向量

回到优化问题(5.7)，我们注意到约束是仿射的，因此是合格的。目标函数以及仿射约束是凸的且可微的。因此，定理B.30的要求成立，KKT条件在最优解处适用。我们将使用这些条件来分析算法，并证明它的几个关键性质，然后推导出与SVMs相关的对偶优化问题，在5.2.3节中给出。

我们引入拉格朗日变量 ${\alpha }_{i} \geq 0, i \in \left\lbrack m\right\rbrack$，与 $m$ 约束相关，并用 $\mathbf{\alpha }$ 表示向量 ${\left( {\alpha }_{1},\ldots ,{\alpha }_{m}\right) }^{\top }$。拉格朗日函数可以定义为所有 $\mathbf{w} \in {\mathbb{R}}^{N}, b \in \mathbb{R}$ 和 $\mathbf{\alpha } \in {\mathbb{R}}_{ + }^{m}$ 的函数。

$$
\mathcal{L}\left( {\mathbf{w}, b,\mathbf{\alpha }}\right) = \frac{1}{2}\parallel \mathbf{w}{\parallel }^{2} - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}\left\lbrack {{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) - 1}\right\rbrack . \tag{5.8}
$$

KKT条件是通过将拉格朗日函数对原变量 $\mathbf{w}$ 和 $b$ 的梯度设为零，并写出互补条件得到的。

$$
{\nabla }_{\mathbf{w}}\mathcal{L} = \mathbf{w} - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i} = 0\; \Rightarrow \;\mathbf{w} = \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i} \tag{5.9}
$$

$$
{\nabla }_{b}\mathcal{L} = - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0\; \Rightarrow \;\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0 \tag{5.10}
$$

$$
\forall i,{\alpha }_{i}\left\lbrack {{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) - 1}\right\rbrack = 0\; \Rightarrow \;{\alpha }_{i} = 0 \vee {y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1. \tag{5.11}
$$

由方程(5.9)，在SVM问题解中的权重向量 $\mathbf{w}$ 是训练集向量的 ${\mathbf{x}}_{1},\ldots ,{\mathbf{x}}_{m}$ 的线性组合。一个向量 ${\mathbf{x}}_{i}$ 出现在该展开中当且仅当 ${\alpha }_{i} \neq 0$。这样的向量被称为支持向量。由互补条件(5.11)，如果 ${\alpha }_{i} \neq 0$，那么 ${y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1$。因此，支持向量位于边缘超平面上 $\mathbf{w} \cdot {\mathbf{x}}_{i} + b = \pm 1$。

支持向量完全定义了最大间隔超平面或SVM解，这证明了该算法名称的合理性。按照定义，不在边缘超平面上的向量不影响这些超平面的定义 - 在它们缺失的情况下，SVM问题的解仍然不变。注意，尽管SVM问题的解 $\mathbf{w}$ 是唯一的，但支持向量不是。在 $N, N + 1$ 维度中，点足以定义一个超平面。因此，当超过 $N + 1$ 个点位于边缘超平面上时，对于 $N + 1$ 支持向量的选择是可能的。

### 5.2.3 对偶优化问题

为了推导约束优化问题(5.7)的对偶形式，我们将 $\mathbf{w}$ 的定义代入拉格朗日函数，该定义涉及对偶变量，如(5.9)所示

并应用约束(5.10)。这得到

$$
\mathcal{L} = \underset{-\frac{1}{2}\mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) }{\underbrace{\frac{1}{2}{\begin{Vmatrix}\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i}\end{Vmatrix}}^{2} - \mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) }} - \underset{0}{\underbrace{\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}b}} + \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}, \tag{5.12}
$$

这简化为

$$
\mathcal{L} = \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i} - \frac{1}{2}\mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) . \tag{5.13}
$$

这导致以下可分情况下SVM的对偶优化问题:

$$
\mathop{\max }\limits_{\mathbf{\alpha }}\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i} - \frac{1}{2}\mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) \tag{5.14}
$$

$$
\text{subject to:}{\alpha }_{i} \geq 0 \land \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0,\forall i \in \left\lbrack m\right\rbrack \text{.}
$$

目标函数 $G : \mathbf{\alpha } \mapsto \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i} - \frac{1}{2}\mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right)$ 是无限可微的。其Hessian矩阵由 ${\nabla }^{2}G = - \mathbf{A}$ 给出，其中 $\mathbf{A} = {\left( {y}_{i}{\mathbf{x}}_{i} \cdot {y}_{j}{\mathbf{x}}_{j}\right) }_{ij}$ 。 $\mathbf{A}$ 是与向量 ${y}_{1}{\mathbf{x}}_{1},\ldots ,{y}_{m}{\mathbf{x}}_{m}$ 相关的格拉姆矩阵，因此是半正定的(参见第A.2.3节)，这表明 ${\nabla }^{2}G \preccurlyeq \mathbf{0}$ 且 $G$ 是一个凹函数。由于约束是仿射的和凸的，最大化问题(5.14)是一个凸优化问题。由于 $G$ 是 $\mathbf{\alpha }$ 的二次函数，这个对偶优化问题也是一个二次规划问题，正如原优化问题一样，再次可以使用通用和专门的二次规划求解器来获得解(参见练习5.4，了解关于SMO算法的详细信息，该算法通常用于在更一般的非可分设置中解决SVM问题的对偶形式)。

此外，由于约束是仿射的，它们是合格的，强对偶性成立(见附录B)。因此，原问题和对偶问题是等价的，即对偶问题(5.14)的解 $\mathbf{\alpha }$ 可以直接用来通过方程(5.9)确定SVM返回的假设:

$$
h\left( \mathbf{x}\right) = \operatorname{sgn}\left( {\mathbf{w} \cdot \mathbf{x} + b}\right) = \operatorname{sgn}\left( {\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}\left( {{\mathbf{x}}_{i} \cdot \mathbf{x}}\right) + b}\right) . \tag{5.15}
$$

由于支持向量位于边缘超平面上，对于任意支持向量 ${\mathbf{x}}_{i}$ ，$\mathbf{w} \cdot {\mathbf{x}}_{i} + b = {y}_{i}$ ，因此 $b$ 可以通过以下方式获得:

$$
b = {y}_{i} - \mathop{\sum }\limits_{{j = 1}}^{m}{\alpha }_{j}{y}_{j}\left( {{\mathbf{x}}_{j} \cdot {\mathbf{x}}_{i}}\right) \tag{5.16}
$$

对偶优化问题(5.14)以及表达式(5.15)和(5.16)揭示了SVM的一个重要性质:假设解只依赖于向量之间的内积，而不是直接依赖于向量本身。这个观察是关键，其重要性将在第6章中变得清晰，在那里我们引入了核方法。

方程(5.16)现在可以用来推导几何边缘 $\rho$ 关于 $\mathbf{\alpha }$ 的简单表达式。由于(5.16)对于所有 $i$ 成立，且 ${\alpha }_{i} \neq 0$ ，将两边乘以 ${\alpha }_{i}{y}_{i}$ 并求和得到:

$$
\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}b = \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}^{2} - \mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) . \tag{5.17}
$$

利用 ${y}_{i}^{2} = 1$ 的事实以及方程(5.9)，然后得到:

$$
0 = \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i} - \parallel \mathbf{w}{\parallel }^{2} \tag{5.18}
$$

注意到 ${\alpha }_{i} \geq 0$ ，我们得到边缘 $\rho$ 关于以下表达式的形式:

${L}_{1}$ 的 $\mathbf{\alpha }$ 范数:

$$
{\rho }^{2} = \frac{1}{\parallel \mathbf{w}{\parallel }_{2}^{2}} = \frac{1}{\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}} = \frac{1}{\parallel \mathbf{\alpha }{\parallel }_{1}}. \tag{5.19}
$$

### 5.2.4 留一法分析

我们现在使用留一法错误的概念来导出基于训练集中支持向量比例的第一个SVM学习保证。

定义5.2(留一法错误)令 ${h}_{S}$ 表示学习算法 $\mathcal{A}$ 在固定样本 $S$ 上训练时返回的假设。那么，$\mathcal{A}$ 在大小为 $m$ 的样本 $S$ 上的留一法错误定义为:

$$
{\widehat{R}}_{\mathrm{{LOO}}}\left( \mathcal{A}\right) = \frac{1}{m}\mathop{\sum }\limits_{{i = 1}}^{m}{1}_{{h}_{S-\{ {x}_{i}\} }\left( {x}_{i}\right) \neq {y}_{i}}.
$$

因此，对于每一个 $i \in \left\lbrack m\right\rbrack ,\mathcal{A}$ 都在除了 ${x}_{i}$ 之外的所有 $S$ 上的点进行训练，即 $S - \left\{ {x}_{i}\right\}$ ，然后使用 ${x}_{i}$ 计算其误差。留一法误差是这些误差的平均值。我们将使用留一法误差的一个重要性质，如下面的引理所述。

引理5.3 对于大小为 $m \geq 2$ 的样本的留一法误差的平均值是无偏估计，用于大小为 $m - 1$ 的样本的平均泛化误差:

$$
\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{E}}\left\lbrack {{\widehat{R}}_{\mathrm{{LOO}}}\left( \mathcal{A}\right) }\right\rbrack = \underset{{S}^{\prime } \sim {\mathcal{D}}^{m - 1}}{\mathbb{E}}\left\lbrack {R\left( {h}_{{S}^{\prime }}\right) }\right\rbrack \tag{5.20}
$$

其中 $\mathcal{D}$ 表示从中抽取点的分布。

证明:由于期望的线性，我们可以写出

$$
\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{E}}\left\lbrack {{\widehat{R}}_{\mathrm{{LOO}}}\left( \mathcal{A}\right) }\right\rbrack = \frac{1}{m}\mathop{\sum }\limits_{{i = 1}}^{m}\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{E}}\left\lbrack {1}_{{h}_{S-\{ {x}_{i}\} }\left( {x}_{i}\right) \neq {y}_{i}}\right\rbrack
$$

$$
= \underset{S \sim {\mathcal{D}}^{m}}{\mathbb{E}}\left\lbrack {1}_{{h}_{S-\{ {x}_{1}\} }\left( {x}_{1}\right) \neq {y}_{1}}\right\rbrack
$$

$$
= \underset{{S}^{\prime } \sim {\mathcal{D}}^{m - 1},{x}_{1} \sim \mathcal{D}}{\mathbb{E}}\left\lbrack {1}_{{h}_{{S}^{\prime }}\left( {x}_{1}\right) \neq {y}_{1}}\right\rbrack
$$

$$
= \underset{{S}^{\prime } \sim {\mathcal{D}}^{m - 1}}{\mathbb{E}}\left\lbrack {\underset{{x}_{1} \sim \mathcal{D}}{\mathbb{E}}\left\lbrack {1}_{{h}_{{S}^{\prime }}\left( {x}_{1}\right) \neq {y}_{1}}\right\rbrack }\right\rbrack
$$

$$
= \underset{{S}^{\prime } \sim {\mathcal{D}}^{m - 1}}{\mathbb{E}}\left\lbrack {R\left( {h}_{{S}^{\prime }}\right) }\right\rbrack
$$

对于第二个等式，我们使用了这样一个事实:由于 $S$ 的点是独立同分布地抽取的，所以期望 ${\mathbb{E}}_{S \sim {\mathcal{D}}^{m}}\left\lbrack {1}_{{h}_{S-\{ {x}_{i}\} }\left( {x}_{i}\right) \neq {y}_{i}}\right\rbrack$ 不依赖于 $i \in \left\lbrack m\right\rbrack$ 的选择，因此等于 ${\mathbb{E}}_{S \sim {\mathcal{D}}^{m}}\left\lbrack {1}_{{h}_{S-\{ {x}_{1}\} }\left( {x}_{1}\right) \neq {y}_{1}}\right\rbrack$ 。

通常，计算留一法误差可能是昂贵的，因为它需要在大小为 $m - 1$ 的样本上训练 $m$ 次。然而，在某些情况下，可以更有效地推导出 ${\widehat{R}}_{\mathrm{{LOO}}}\left( \mathcal{A}\right)$ 的表达式(参见练习11.9)。

定理5.4 设 ${h}_{S}$ 是SVMs对于样本 $S$ 返回的假设，并且设 ${N}_{SV}\left( S\right)$ 是定义 ${h}_{S}$ 的支持向量的数量。那么，

$$
\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{E}}\left\lbrack {R\left( {h}_{S}\right) }\right\rbrack \leq \underset{S \sim {\mathcal{D}}^{m + 1}}{\mathbb{E}}\left\lbrack \frac{{N}_{\mathrm{{SV}}}\left( S\right) }{m + 1}\right\rbrack .
$$

证明:设 $S$ 是线性可分样本 $m + 1$ 。如果 $x$ 不是 ${h}_{S}$ 的支持向量，移除它不会改变SVM的解。因此， ${h}_{S-\{ x\} } = {h}_{S}$ 和 ${h}_{S-\{ x\} }$ 正确分类 $x$ 。通过反证法，如果 ${h}_{S-\{ x\} }$ 错误分类 $x, x$ 必须是一个支持向量，这意味着

$$
{\widehat{R}}_{\mathrm{{LOO}}}\left( \mathrm{{SVM}}\right) \leq \frac{{N}_{\mathrm{{SV}}}\left( S\right) }{m + 1}. \tag{5.21}
$$

对两边取期望并使用引理5.3得到结果。

定理5.4为支持向量机提供了一个稀疏性论证:算法的平均误差被支持向量的平均分数上界。人们可能希望对于实践中看到的许多分布，相对较少的训练点将位于边缘超平面上。那么解在意义上将是稀疏的，即一小部分对偶变量 ${\alpha }_{i}$ 将是非零的。

![0192515b-435f-75ef-9b27-37409ba7b98f_8_516_241_768_479_0.jpg](images/0192515b-435f-75ef-9b27-37409ba7b98f_8_516_241_768_479_0.jpg)

图5.4

一个分离超平面，点 ${\mathbf{x}}_{i}$ 被错误分类，而点 ${\mathbf{x}}_{j}$ 被正确分类，但边缘小于1。

零。然而，请注意，这个界限相对较弱，因为它仅适用于所有大小为 $m$ 的样本的算法平均泛化误差。它没有提供关于泛化误差方差的信息。在5.4节中，我们使用基于边缘概念的不同论证，提出了更强的高概率界限。