To derive the dual form of the constrained optimization problem (5.24), we plug into the Lagrangian the definition of $\mathbf{w}$ in terms of the dual variables (5.26) and apply the constraint (5.27). This yields

$$
\mathcal{L} = \underset{-\frac{1}{2}\mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) }{\underbrace{\frac{1}{2}{\begin{Vmatrix}\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i}\end{Vmatrix}}^{2} - \mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) }} - \underset{0}{\underbrace{\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}b}} + \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}. \tag{5.31}
$$

Remarkably, we find that the objective function is no different than in the separable

case:

$$
\mathcal{L} = \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i} - \frac{1}{2}\mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) \tag{5.32}
$$

However, here, in addition to ${\alpha }_{i} \geq 0$ , we must impose the constraint on the Lagrange variables ${\beta }_{i} \geq 0$ . In view of (5.28), this is equivalent to ${\alpha }_{i} \leq C$ . This leads to the following dual optimization problem for SVMs in the non-separable case, which only differs from that of the separable case (5.14) by the constraints ${\alpha }_{i} \leq C$ :

$$
\mathop{\max }\limits_{\mathbf{\alpha }}\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i} - \frac{1}{2}\mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) \tag{5.33}
$$

$$
\text{ subject to: }0 \leq {\alpha }_{i} \leq C \land \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0, i \in \left\lbrack m\right\rbrack .
$$

因此，我们之前关于优化问题(5.14)的评论也适用于(5.33)。特别是，目标函数是凹的且可无限次微分，(5.33)等价于一个凸二次规划问题。该问题等价于原问题(5.24)。

对偶问题(5.33)的解 $\mathbf{\alpha }$ 可以直接用来确定 SVMs 返回的假设，使用方程(5.26):

$$
h\left( \mathbf{x}\right) = \operatorname{sgn}\left( {\mathbf{w} \cdot \mathbf{x} + b}\right) = \operatorname{sgn}\left( {\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}\left( {{\mathbf{x}}_{i} \cdot \mathbf{x}}\right) + b}\right) . \tag{5.34}
$$

此外，$b$ 可以从任意位于边缘超平面上的支持向量 ${\mathbf{x}}_{i}$ 中获得，即任意满足 $0 < {\alpha }_{i} < C$ 的向量 ${\mathbf{x}}_{i}$ 。对于这样的支持向量，$\mathbf{w} \cdot {\mathbf{x}}_{i} + b = {y}_{i}$

因此

$$
b = {y}_{i} - \mathop{\sum }\limits_{{j = 1}}^{m}{\alpha }_{j}{y}_{j}\left( {{\mathbf{x}}_{j} \cdot {\mathbf{x}}_{i}}\right) \tag{5.35}
$$

与可分情况一样，对偶优化问题(5.33)以及表达式(5.34)和(5.35)显示了 SVMs 的一个重要性质:假设解仅依赖于向量之间的内积，而不是直接依赖于向量本身。这一事实可以用来将 SVMs 扩展到定义非线性决策边界，我们将在第6章中看到。