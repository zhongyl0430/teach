回到 [[5.2.1 原始优化问题#^def-5-7|优化问题 (5.7)]]，我们注意到约束是仿射的，因此是合格的。
目标函数以及仿射约束是凸的且可微的。
因此，[[定理B.30 Karush-Kuhn-Tucker定理]] 的要求成立，KKT条件在最优解处适用。
我们将使用这些条件来分析算法，并证明它的几个关键性质，然后推导出与 SVM 相关的对偶优化问题，在 [[5.2.3 对偶优化问题]] 中给出。

我们引入
- 拉格朗日变量 $${\alpha }_{i} \geq 0, \quad i \in \left\lbrack m\right\rbrack$$与 $m$ 约束相关，
- 并用 $\mathbf{\alpha }$ 表示向量 ${\left( {\alpha }_{1},\ldots ,{\alpha }_{m}\right) }^{\top }$。

> [!definition] 拉格朗日函数
> 拉格朗日函数可以定义为所有 $\mathbf{w} \in {\mathbb{R}}^{N}, b \in \mathbb{R}$ 和 $\mathbf{\alpha } \in {\mathbb{R}}_{ + }^{m}$ 的函数
> $$
> \begin{align}
> \mathcal{L}\left( {\mathbf{w}, b,\mathbf{\alpha }}\right) := &\frac{1}{2}\parallel \mathbf{w}{\parallel }^{2} \\
> &- \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}\left\lbrack {{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) - 1}\right\rbrack . \tag{5.8}
> \end{align}
> $$

KKT条件是通过将拉格朗日函数对原变量 $\mathbf{w}$ 和 $b$ 的梯度设为零，并写出互补条件得到的。

$$
\begin{align}
&{\nabla }_{\mathbf{w}}\mathcal{L} = \mathbf{w} - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i} = 0 \\
&\Rightarrow \;\mathbf{w} = \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i} \tag{5.9}
\end{align}
$$

$$
\begin{align}
&{\nabla }_{b}\mathcal{L} = - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0\\ 
&\Rightarrow \;\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0 \tag{5.10}
\end{align}
$$

$$
\begin{align}
&\forall i,{\alpha }_{i}\left\lbrack {{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) - 1}\right\rbrack = 0\\
&\Rightarrow \;{\alpha }_{i} = 0 \vee {y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1. \tag{5.11}
\end{align}
$$

- 由方程(5.9)，在SVM问题解中的权重向量 $\mathbf{w}$ 是训练集向量的 ${\mathbf{x}}_{1},\ldots ,{\mathbf{x}}_{m}$ 的线性组合。
	- 一个向量 ${\mathbf{x}}_{i}$ 出现在该展开中当且仅当 ${\alpha }_{i} \neq 0$。
	- 这样的向量被称为 **支持向量**。
- 由互补条件 (5.11)，如果 ${\alpha }_{i} \neq 0$，那么 $${y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1$$
	- 因此，支持向量位于边缘超平面上 $$\mathbf{w} \cdot {\mathbf{x}}_{i} + b = \pm 1$$

支持向量完全定义了最大间隔超平面或SVM解，这证明了该算法名称的合理性。
按照定义，不在边缘超平面上的向量不影响这些超平面的定义 - 在它们缺失的情况下，SVM问题的解仍然不变。

> [!remark]
> 注意，尽管SVM问题的解 $\mathbf{w}$ 是唯一的，但支持向量不是。
> 在 $N, N + 1$ 维度中，点足以定义一个超平面。因此，当超过 $N + 1$ 个点位于边缘超平面上时，对于 $N + 1$ 支持向量的选择是可能的。