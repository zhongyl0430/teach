考虑一个输入空间 $x$，它是 ${\mathbb{R}}^{N}$ 的子集，具有 $N \geq 1$，输出或目标空间 $y = \{ - 1, + 1\}$，设 $f : \mathcal{X} \rightarrow \mathcal{Y}$ 为目标函数。
给定一个从 $\mathcal{X}$ 映射到 $\mathcal{Y}$ 的函数假设集 $\mathcal{H}$，二分类任务可以表述如下。
学习者接收到一个大小为 $m$ 的训练样本 $S$，该样本独立同分布地从 $\mathcal{X}$ 中根据某种未知的分布 $\mathcal{D}, S = \left( {\left( {{x}_{1},{y}_{1}}\right) ,\ldots ,\left( {{x}_{m},{y}_{m}}\right) }\right) \in$ ${\left( \mathcal{X} \times \mathcal{Y}\right) }^{m}$ 抽取，对于所有 $i \in \left\lbrack m\right\rbrack$ 有 ${y}_{i} = f\left( {x}_{i}\right)$。
问题在于确定一个假设 $h \in \mathcal{H}$，一个二分类器，具有较小的泛化误差:
$$
{R}_{\mathcal{D}}\left( h\right) = \underset{x \sim \mathcal{D}}{\mathbb{P}}\left\lbrack {h\left( x\right) \neq f\left( x\right) }\right\rbrack \tag{5.1}
$$

对于这个任务，可以选择不同的假设集 $\mathcal{H}$。
鉴于第3章中呈现的结果，该结果形式化了奥卡姆剃刀原则，具有较小复杂度的假设集 (例如，较小的VC维数或Rademacher复杂度) 在所有其他条件相同时提供更好的学习保证。
一个具有相对较小复杂度的自然假设集是线性分类器或超平面的集合， 其定义如下:
$$
\mathcal{H} = \left\{ {\mathbf{x} \mapsto \operatorname{sign}\left( {\mathbf{w} \cdot \mathbf{x} + b}\right) : \mathbf{w} \in {\mathbb{R}}^{N}, b \in \mathbb{R}}\right\} . \tag{5.2}
$$

然后，该学习问题被称为 **线性分类问题**。
在 ${\mathbb{R}}^{N}$ 中超平面的通用方程是 
$$\mathbf{w} \cdot \mathbf{x} + b = 0$$
其中 $\mathbf{w} \in {\mathbb{R}}^{N}$ 是一个非零向量，垂直于超平面并且 $b \in \mathbb{R}$ 一个标量。形如 $\mathbf{x} \mapsto \operatorname{sign}\left( {\mathbf{w} \cdot \mathbf{x} + b}\right)$ 的假设因此将所有落在超平面一侧的点标记为正，将所有其他点标记为负 $\mathbf{w} \cdot \mathbf{x} + b = 0$。

图 5.1
两个可能的分隔超平面。
右侧的图形显示了一个最大化边缘的超平面。
![0192515b-435f-75ef-9b27-37409ba7b98f_1_409_249_997_365_0.jpg](images/0192515b-435f-75ef-9b27-37409ba7b98f_1_409_249_997_365_0.jpg)