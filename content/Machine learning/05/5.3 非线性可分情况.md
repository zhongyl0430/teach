## 5.3 非线性可分情况

在大多数实际情况下，训练数据不是线性可分的，这意味着对于任何超平面 $\mathbf{w} \cdot \mathbf{x} + b = 0$ ，都存在 ${\mathbf{x}}_{i} \in S$ 使得

$$
{y}_{i}\left\lbrack {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right\rbrack \ngeqslant 1 \tag{5.22}
$$

因此，在5.2节中讨论的线性可分情况下施加的约束不能同时成立。然而，这些约束的放松版本确实可以成立，即对于每个 $i \in \left\lbrack m\right\rbrack$ ，都存在 ${\xi }_{i} \geq 0$ 使得

$$
{y}_{i}\left\lbrack {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right\rbrack \geq 1 - {\xi }_{i} \tag{5.23}
$$

变量 ${\xi }_{i}$ 被称为松弛变量，通常用于优化中定义约束的松弛版本。在这里，一个松弛变量 ${\xi }_{i}$ 衡量向量 ${\mathbf{x}}_{i}$ 违反期望不等式 ${y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) \geq 1$ 的程度。图 5.4 描述了这种情况。对于一个超平面 $\mathbf{w} \cdot \mathbf{x} + b = 0$ ，一个具有 ${\xi }_{i} > 0$ 的向量 ${\mathbf{x}}_{i}$ 可以被视为异常值。每个 ${\mathbf{x}}_{i}$ 都必须定位在正确的边际超平面的一侧，以不被视为异常值。因此，一个具有 $0 < {y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) < 1$ 的向量 ${\mathbf{x}}_{i}$ 虽然被超平面 $\mathbf{w} \cdot \mathbf{x} + b = 0$ 正确分类，但仍然被视为异常值，即 ${\xi }_{i} > 0$ 。如果我们忽略异常值，训练数据就可以通过 $\mathbf{w} \cdot \mathbf{x} + b = 0$ 以一个称为软边界的边缘 $\rho = 1/\parallel \mathbf{w}\parallel$ 被正确分开，这与可分情况下的硬边界相对。

在不可分情况下，我们应该如何选择超平面？一个想法是选择使经验误差最小化的超平面。但是，这个解决方案将无法从我们将在第 5.4 节中提出的大的边界保证中受益。此外，确定具有最小零一损失的超平面的问题，即最小化误分类数量的问题，作为空间维度 $N$ 的函数是 NP-难问题。

在这里，有两个相互冲突的目标:一方面，我们希望限制由于异常值造成的总松弛量，这可以通过 $\mathop{\sum }\limits_{{i = 1}}^{m}{\xi }_{i}$ 来衡量，或者更一般地通过 $\mathop{\sum }\limits_{{i = 1}}^{m}{\xi }_{i}^{p}$ 对于某些 $p \geq 1$ 来衡量；另一方面，我们寻求具有大边界的超平面，尽管更大的边界可能导致更多的异常值，从而产生更多的松弛量。

### 5.3.1 原始优化问题

This leads to the following general optimization problem defining SVMs in the non-separable case where the parameter $C \geq 0$ determines the trade-off between margin-maximization (or minimization of $\parallel \mathbf{w}{\parallel }^{2}$ ) and the minimization of the slack penalty $\mathop{\sum }\limits_{{i = 1}}^{m}{\xi }_{i}^{p}$ :

$$
\mathop{\min }\limits_{{\mathbf{w}, b,\mathbf{\xi }}}\frac{1}{2}\parallel \mathbf{w}{\parallel }^{2} + C\mathop{\sum }\limits_{{i = 1}}^{m}{\xi }_{i}^{p} \tag{5.24}
$$

$$
\text{subject to}{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) \geq 1 - {\xi }_{i} \land {\xi }_{i} \geq 0, i \in \left\lbrack m\right\rbrack \text{,}
$$

where $\mathbf{\xi } = {\left( {\xi }_{1},\ldots ,{\xi }_{m}\right) }^{\top }$ . The parameter $C$ is typically determined via $n$ -fold cross-validation (see section 4.5).

As in the separable case, (5.24) is a convex optimization problem since the constraints are affine and thus convex and since the objective function is convex for any $p \geq 1$ . In particular, $\mathbf{\xi } \mapsto \mathop{\sum }\limits_{{i = 1}}^{m}{\xi }_{i}^{p} = \parallel \mathbf{\xi }{\parallel }_{p}^{p}$ is convex in view of the convexity of the norm $\parallel \cdot {\parallel }_{p}$ .

There are many possible choices for $p$ leading to more or less aggressive penaliza-tions of the slack terms (see exercise 5.1). The choices $p = 1$ and $p = 2$ lead to the most straightforward solutions and analyses. The loss functions associated with $p = 1$ and $p = 2$ are called the hinge loss and the quadratic hinge loss, respectively. Figure 5.5 shows the plots of these loss functions as well as that of the standard zero-one loss function. Both hinge losses are convex upper bounds on the zero-one loss, thus making them well suited for optimization. In what follows, the analysis is presented in the case of the hinge loss $\left( {p = 1}\right)$ , which is the most widely used loss function for SVMs.

![0192515b-435f-75ef-9b27-37409ba7b98f_10_495_257_716_524_0.jpg](images/0192515b-435f-75ef-9b27-37409ba7b98f_10_495_257_716_524_0.jpg)

Figure 5.5

Both the hinge loss and the quadratic hinge loss provide convex upper bounds on the binary zero-one loss.

### 5.3.2 Support vectors

与可分离情况一样，约束是仿射的，因此是合格的。目标函数以及仿射约束是凸的且可微的。因此，定理B.30的假设成立，KKT条件在最优解处适用。我们使用这些条件来分析算法并展示其几个关键性质，并随后推导出与SVMs相关的对偶优化问题，在5.3.3节中。

我们引入拉格朗日变量 ${\alpha }_{i} \geq 0, i \in \left\lbrack m\right\rbrack$，与第一个 $m$ 约束相关，以及 ${\beta }_{i} \geq 0, i \in \left\lbrack m\right\rbrack$ 与松弛变量的非负约束相关。我们用 $\mathbf{\alpha }$ 表示向量 ${\left( {\alpha }_{1},\ldots ,{\alpha }_{m}\right) }^{\top }$，用 $\mathbf{\beta }$ 表示向量 ${\left( {\beta }_{1},\ldots ,{\beta }_{m}\right) }^{\top }$。拉格朗日函数可以定义为所有 $\mathbf{w} \in {\mathbb{R}}^{N}, b \in \mathbb{R}$ 和 $\mathbf{\xi },\mathbf{\alpha },\mathbf{\beta } \in {\mathbb{R}}_{ + }^{m}$ 的函数。

$$
\mathcal{L}\left( {\mathbf{w}, b,\mathbf{\xi },\mathbf{\alpha },\mathbf{\beta }}\right) = \frac{1}{2}\parallel \mathbf{w}{\parallel }^{2} + C\mathop{\sum }\limits_{{i = 1}}^{m}{\xi }_{i} - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}\left\lbrack {{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) - 1 + {\xi }_{i}}\right\rbrack - \mathop{\sum }\limits_{{i = 1}}^{m}{\beta }_{i}{\xi }_{i}. \tag{5.25}
$$

KKT条件是通过将拉格朗日函数相对于原变量的梯度 $\mathbf{w}, b$ 和 ${\xi }_{i}$ s 设为零，并写出互补条件得到的。

$$
{\nabla }_{\mathbf{w}}\mathcal{L} = \mathbf{w} - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i} = 0\; \Rightarrow \;\mathbf{w} = \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i} \tag{5.26}
$$

$$
{\nabla }_{b}\mathcal{L} = - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0\; \Rightarrow \;\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0 \tag{5.27}
$$

$$
{\nabla }_{{\xi }_{i}}\mathcal{L} = C - {\alpha }_{i} - {\beta }_{i} = 0\; \Rightarrow \;{\alpha }_{i} + {\beta }_{i} = C \tag{5.28}
$$

$$
\forall i,{\alpha }_{i}\left\lbrack {{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) - 1 + {\xi }_{i}}\right\rbrack = 0\; \Rightarrow \;{\alpha }_{i} = 0 \vee {y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1 - {\xi }_{i}\; \tag{5.29}
$$

$$
\forall i,{\beta }_{i}{\xi }_{i} = 0\; \Rightarrow \;{\beta }_{i} = 0 \vee {\xi }_{i} = 0. \tag{5.30}
$$

By equation (5.26), as in the separable case, the weight vector $\mathbf{w}$ at the solution of the SVM problem is a linear combination of the training set vectors ${\mathbf{x}}_{1},\ldots ,{\mathbf{x}}_{m}$ . A vector ${\mathbf{x}}_{i}$ appears in that expansion iff ${\alpha }_{i} \neq 0$ . Such vectors are called support vectors. Here, there are two types of support vectors. By the complementarity condition (5.29), if ${\alpha }_{i} \neq 0$ , then ${y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1 - {\xi }_{i}$ . If ${\xi }_{i} = 0$ , then ${y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1$ and ${\mathbf{x}}_{i}$ lies on a marginal hyperplane, as in the separable case. Otherwise, ${\xi }_{i} \neq 0$ and ${\mathbf{x}}_{i}$ is an outlier. In this case,(5.30) implies ${\beta }_{i} = 0$ and (5.28) then requires ${\alpha }_{i} = C$ . Thus, support vectors ${\mathbf{x}}_{i}$ are either outliers, in which case ${\alpha }_{i} = C$ , or vectors lying on the marginal hyperplanes. As in the separable case, note that while the weight vector $\mathbf{w}$ solution is unique, the support vectors are not.

### 5.3.3 Dual optimization problem

To derive the dual form of the constrained optimization problem (5.24), we plug into the Lagrangian the definition of $\mathbf{w}$ in terms of the dual variables (5.26) and apply the constraint (5.27). This yields

$$
\mathcal{L} = \underset{-\frac{1}{2}\mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) }{\underbrace{\frac{1}{2}{\begin{Vmatrix}\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i}\end{Vmatrix}}^{2} - \mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) }} - \underset{0}{\underbrace{\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}b}} + \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}. \tag{5.31}
$$

Remarkably, we find that the objective function is no different than in the separable

case:

$$
\mathcal{L} = \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i} - \frac{1}{2}\mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) \tag{5.32}
$$

However, here, in addition to ${\alpha }_{i} \geq 0$ , we must impose the constraint on the Lagrange variables ${\beta }_{i} \geq 0$ . In view of (5.28), this is equivalent to ${\alpha }_{i} \leq C$ . This leads to the following dual optimization problem for SVMs in the non-separable case, which only differs from that of the separable case (5.14) by the constraints ${\alpha }_{i} \leq C$ :

$$
\mathop{\max }\limits_{\mathbf{\alpha }}\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i} - \frac{1}{2}\mathop{\sum }\limits_{{i, j = 1}}^{m}{\alpha }_{i}{\alpha }_{j}{y}_{i}{y}_{j}\left( {{\mathbf{x}}_{i} \cdot {\mathbf{x}}_{j}}\right) \tag{5.33}
$$

$$
\text{ subject to: }0 \leq {\alpha }_{i} \leq C \land \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0, i \in \left\lbrack m\right\rbrack .
$$

因此，我们之前关于优化问题(5.14)的评论也适用于(5.33)。特别是，目标函数是凹的且可无限次微分，(5.33)等价于一个凸二次规划问题。该问题等价于原问题(5.24)。

对偶问题(5.33)的解 $\mathbf{\alpha }$ 可以直接用来确定 SVMs 返回的假设，使用方程(5.26):

$$
h\left( \mathbf{x}\right) = \operatorname{sgn}\left( {\mathbf{w} \cdot \mathbf{x} + b}\right) = \operatorname{sgn}\left( {\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}\left( {{\mathbf{x}}_{i} \cdot \mathbf{x}}\right) + b}\right) . \tag{5.34}
$$

此外，$b$ 可以从任意位于边缘超平面上的支持向量 ${\mathbf{x}}_{i}$ 中获得，即任意满足 $0 < {\alpha }_{i} < C$ 的向量 ${\mathbf{x}}_{i}$ 。对于这样的支持向量，$\mathbf{w} \cdot {\mathbf{x}}_{i} + b = {y}_{i}$

因此

$$
b = {y}_{i} - \mathop{\sum }\limits_{{j = 1}}^{m}{\alpha }_{j}{y}_{j}\left( {{\mathbf{x}}_{j} \cdot {\mathbf{x}}_{i}}\right) \tag{5.35}
$$

与可分情况一样，对偶优化问题(5.33)以及表达式(5.34)和(5.35)显示了 SVMs 的一个重要性质:假设解仅依赖于向量之间的内积，而不是直接依赖于向量本身。这一事实可以用来将 SVMs 扩展到定义非线性决策边界，我们将在第6章中看到。