与可分离情况一样，约束是仿射的，因此是合格的。目标函数以及仿射约束是凸的且可微的。因此，定理B.30的假设成立，KKT条件在最优解处适用。我们使用这些条件来分析算法并展示其几个关键性质，并随后推导出与SVMs相关的对偶优化问题，在5.3.3节中。

我们引入拉格朗日变量 ${\alpha }_{i} \geq 0, i \in \left\lbrack m\right\rbrack$，与第一个 $m$ 约束相关，以及 ${\beta }_{i} \geq 0, i \in \left\lbrack m\right\rbrack$ 与松弛变量的非负约束相关。我们用 $\mathbf{\alpha }$ 表示向量 ${\left( {\alpha }_{1},\ldots ,{\alpha }_{m}\right) }^{\top }$，用 $\mathbf{\beta }$ 表示向量 ${\left( {\beta }_{1},\ldots ,{\beta }_{m}\right) }^{\top }$。拉格朗日函数可以定义为所有 $\mathbf{w} \in {\mathbb{R}}^{N}, b \in \mathbb{R}$ 和 $\mathbf{\xi },\mathbf{\alpha },\mathbf{\beta } \in {\mathbb{R}}_{ + }^{m}$ 的函数。

$$
\mathcal{L}\left( {\mathbf{w}, b,\mathbf{\xi },\mathbf{\alpha },\mathbf{\beta }}\right) = \frac{1}{2}\parallel \mathbf{w}{\parallel }^{2} + C\mathop{\sum }\limits_{{i = 1}}^{m}{\xi }_{i} - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}\left\lbrack {{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) - 1 + {\xi }_{i}}\right\rbrack - \mathop{\sum }\limits_{{i = 1}}^{m}{\beta }_{i}{\xi }_{i}. \tag{5.25}
$$

KKT条件是通过将拉格朗日函数相对于原变量的梯度 $\mathbf{w}, b$ 和 ${\xi }_{i}$ s 设为零，并写出互补条件得到的。

$$
{\nabla }_{\mathbf{w}}\mathcal{L} = \mathbf{w} - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i} = 0\; \Rightarrow \;\mathbf{w} = \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i}{\mathbf{x}}_{i} \tag{5.26}
$$

$$
{\nabla }_{b}\mathcal{L} = - \mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0\; \Rightarrow \;\mathop{\sum }\limits_{{i = 1}}^{m}{\alpha }_{i}{y}_{i} = 0 \tag{5.27}
$$

$$
{\nabla }_{{\xi }_{i}}\mathcal{L} = C - {\alpha }_{i} - {\beta }_{i} = 0\; \Rightarrow \;{\alpha }_{i} + {\beta }_{i} = C \tag{5.28}
$$

$$
\forall i,{\alpha }_{i}\left\lbrack {{y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) - 1 + {\xi }_{i}}\right\rbrack = 0\; \Rightarrow \;{\alpha }_{i} = 0 \vee {y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1 - {\xi }_{i}\; \tag{5.29}
$$

$$
\forall i,{\beta }_{i}{\xi }_{i} = 0\; \Rightarrow \;{\beta }_{i} = 0 \vee {\xi }_{i} = 0. \tag{5.30}
$$

By equation (5.26), as in the separable case, the weight vector $\mathbf{w}$ at the solution of the SVM problem is a linear combination of the training set vectors ${\mathbf{x}}_{1},\ldots ,{\mathbf{x}}_{m}$ . A vector ${\mathbf{x}}_{i}$ appears in that expansion iff ${\alpha }_{i} \neq 0$ . Such vectors are called support vectors. Here, there are two types of support vectors. By the complementarity condition (5.29), if ${\alpha }_{i} \neq 0$ , then ${y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1 - {\xi }_{i}$ . If ${\xi }_{i} = 0$ , then ${y}_{i}\left( {\mathbf{w} \cdot {\mathbf{x}}_{i} + b}\right) = 1$ and ${\mathbf{x}}_{i}$ lies on a marginal hyperplane, as in the separable case. Otherwise, ${\xi }_{i} \neq 0$ and ${\mathbf{x}}_{i}$ is an outlier. In this case,(5.30) implies ${\beta }_{i} = 0$ and (5.28) then requires ${\alpha }_{i} = C$ . Thus, support vectors ${\mathbf{x}}_{i}$ are either outliers, in which case ${\alpha }_{i} = C$ , or vectors lying on the marginal hyperplanes. As in the separable case, note that while the weight vector $\mathbf{w}$ solution is unique, the support vectors are not.