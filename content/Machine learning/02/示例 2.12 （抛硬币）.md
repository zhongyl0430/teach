
> [!example] 
> - 想象抛一个偏斜的硬币，正面出现的概率为 $p$ ，
> - 我们的假设取总是猜测反面。

[[Why Do Cats Land on Their Feet Physics Explains  Scientific American]]

---
那么
- 真实的错误率为 $R\left( h\right) = p$ ，
- 经验错误率为 ${\widehat{R}}_{S}\left( h\right) = \widehat{p}$
- 其中 $\widehat{p}$ 是基于独立同分布抽取的训练样本中正面的经验概率。

---
- 因此， [[2.3 对于有限假设集的不一致情况下的保证#^cor-2-10|推论 2.10]] 保证至少以 $1 - \delta$ 的概率
$$\left| {p - \widehat{p}}\right| \leq \sqrt{\frac{\log \frac{2}{\delta }}{2m}} \tag{2.18}$$
- 如果我们选择 $\delta = {0.02}$ 并使用大小为 500 的样本，至少以 ${98}\%$ 的概率，以下近似质量对于 $\widehat{p}$ 是有保证的：
$$\left| {p - \widehat{p}}\right| \leq \sqrt{\frac{\log \left( {10}\right) }{1000}} \approx {0.048} \tag{2.19}$$
---

我们能否直接将 [[2.3 对于有限假设集的不一致情况下的保证#^cor-2-11|推论 2.11]] 应用于限制由学习算法在样本 $S$ 上训练得到的假设 ${h}_{S}$ 的泛化错误？
- 不能，因为 ${h}_{S}$ 不是一个固定的假设，而是依赖于训练样本 $S$ 的随机变量。
- 请注意，与固定假设的情况不同
	- 对于固定假设，经验误差的期望是泛化错误（方程 (2.3)），
	- 泛化错误 $R\left( {h}_{S}\right)$ 是一个随机变量， 通常与期望 $\mathbb{E}\left\lbrack {{\widehat{R}}_{S}\left( {h}_{S}\right) }\right\rbrack$ 不同， 后者是一个常数。
