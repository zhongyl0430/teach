# 4.5 $n$ -Fold cross-validation
In practice, the amount of labeled data available is often too small to
set aside a validation sample since that would leave an insufficient
amount of training data. Instead, a widely adopted method known as
n-fold cross-validation is used to exploit the labeled data both for
model selection and for training.

Let $\mathbf{\theta }$ denote the vector of free parameters of the
algorithm. For a fixed value of $\mathbf{\theta }$ , the method consists
of first randomly partitioning a given sample $S$ of $m$ labeled
examples into $n$ subsamples, or folds. The $i$ th fold is thus a
labeled sample
$\left( {\left( {{x}_{i1},{y}_{i1}}\right) ,\ldots ,\left( {{x}_{i{m}_{i}},{y}_{i{m}_{i}}}\right) }\right)$
of size ${m}_{i}$ . Then, for any $i \in \left\lbrack n\right\rbrack$ ,
the learning algorithm is trained on all but the $i$ th fold to generate
a hypothesis ${h}_{i}$ , and the performance of ${h}_{i}$ is tested on
the $i$ th fold, as illustrated in figure 4.5a. The parameter value
$\mathbf{\theta }$ is evaluated based on the average error of the
hypotheses ${h}_{i}$ , which is called the cross-validation error. This
quantity is denoted by
${\widehat{R}}_{\mathrm{{CV}}}\left( \mathbf{\theta }\right)$ and
defined by

$${\widehat{R}}_{\mathrm{{CV}}}\left( \mathbf{\theta }\right) = \frac{1}{n}\mathop{\sum }\limits_{{i = 1}}^{n}\underset{\text{error of }{h}_{i}\text{ on the }i\text{ th fold }}{\underbrace{\frac{1}{{m}_{i}}\mathop{\sum }\limits_{{j = 1}}^{{m}_{i}}L\left( {{h}_{i}\left( {x}_{ij}\right) ,{y}_{ij}}\right) }}.$$

The folds are generally chosen to have equal size, that is
${m}_{i} = m/n$ for all $i \in \left\lbrack n\right\rbrack$ . How should
$n$ be chosen? The appropriate choice is subject to a trade-off. For a
large $n$ , each training sample used in $n$ -fold cross-validation has
size $m - m/n =$ $m\left( {1 - 1/n}\right)$ (illustrated by the right
vertical red line in figure 4.5b), which is close to $m$ , the size of
the full sample, and also implies all training samples are quite
similar. At the same time, the $i$ th fold used to measure the error is
relatively small and thus the cross-validation error tends to have a
small bias but a large variance. In contrast, smaller values of $n$ lead
to more diverse training samples but their size (shown by the left
vertical red line in figure 4.5b) is significantly less than $m$ . In
this regime, the ith fold is relatively large and thus the
cross-validation error tends to have a smaller variance but a larger
bias.

In applications, $n$ is typically chosen to be 5 or 10. $n$ -fold
cross-validation is used as follows in model selection. The full labeled
data is first split into a training and a test sample. The training
sample of size $m$ is then used to compute the $n$ - fold
cross-validation error
${\widehat{R}}_{\mathrm{{CV}}}\left( \mathbf{\theta }\right)$ for a
small number of possible values of $\mathbf{\theta }$ . The free
parameter $\mathbf{\theta }$ is next set to the value
${\mathbf{\theta }}_{0}$ for which
${\widehat{R}}_{\mathrm{{CV}}}\left( \mathbf{\theta }\right)$ is
smallest and the algorithm is trained with the parameter setting
${\mathbf{\theta }}_{0}$ over the full training sample of size $m$ . Its
performance is evaluated on the test sample as already described in the
previous section.

The special case of $n$ -fold cross-validation where $n = m$ is called
leave-one-out cross-validation, since at each iteration exactly one
instance is left out of the training sample. As shown in chapter 5, the
average leave-one-out error is an approximately unbiased estimate of the
average error of an algorithm and can be used to derive simple
guarantees for some algorithms. In general, the leave-one-out error is
very costly to compute, since it requires training $m$ times on samples
of size $m - 1$ , but for some algorithms it admits a very efficient
computation (see exercise 11.9).

::: center
![image](images/019145d2-cc04-715a-aec7-5f9a00e87681_44_344163.jpg){width="60%"}
:::

Figure 4.5

$n$ -fold cross-validation. (a) Illustration of the partitioning of the
training data into 5 folds. (b) Typical plot of a classifier's
prediction error as a function of the size of the training sample $m$ :
the error decreases as a function of the number of training points. The
red line on the left side marks the region for small values of $n$ ,
while the red line on the right side marks the region for large values
of $n$ .

In addition to model selection, $n$ -fold cross-validation is also
commonly used for performance evaluation. In that case, for a fixed
parameter setting $\mathbf{\theta }$ , the full labeled sample is
divided into $n$ random folds with no distinction between training and
test samples. The performance reported is the $n$ -fold cross-validation
error on the full sample as well as the standard deviation of the errors
measured on each fold.