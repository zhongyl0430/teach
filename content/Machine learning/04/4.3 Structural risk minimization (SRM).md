# 4.3 Structural risk minimization (SRM) 

In the previous section, we showed that the estimation error can be
sometimes bounded or estimated. But, since the approximation error
cannot be estimated, how should we choose $\mathcal{H}$ ? One way to
proceed is to choose a very complex family $\mathcal{H}$ with no
approximation error or a very small one. $\mathcal{H}$ may be too rich
for generalization bounds to hold for $\mathcal{H}$ , but suppose we can
decompose $\mathcal{H}$ as a union of increasingly complex hypothesis
sets ${\mathcal{H}}_{\gamma }$ , that is
$\mathcal{H} = \mathop{\bigcup }\limits_{{\gamma \in \Gamma }}{\mathcal{H}}_{\gamma }$
, with the complexity of ${\mathcal{H}}_{\gamma }$ increasing with
$\gamma$ , for some set $\Gamma$ . Figure 4.2 illustrates this
decomposition. The problem then consists of selecting the parameter
${\gamma }^{ * } \in \Gamma$ and thus the hypothesis set
${\mathcal{H}}_{{\gamma }^{ * }}$ with the most favorable trade-off
between estimation and approximation errors. Since these quantities are
not known, instead, as illustrated by Figure 4.3, a uniform upper bound
on their sum, the excess error (also called excess risk), can be used.

This is precisely the idea behind the Structural Risk Minimization (SRM)
method. For SRM, $\mathcal{H}$ is assumed to be decomposable into a
countable set, thus, we will write its decomposition as
$\mathcal{H} = \mathop{\bigcup }\limits_{{k \geq 1}}{\mathcal{H}}_{k}$ .
Additionally, the hypothesis sets ${\mathcal{H}}_{k}$ are assumed to be
nested: ${\mathcal{H}}_{k} \subset {\mathcal{H}}_{k + 1}$ for all
$k \geq 1$ . However, many of the results presented in this section also
hold for non-nested hypothesis sets. Thus, we will not make use of that
assumption, unless explicitly specified. SRM consists of choosing the
index ${k}^{ * } \geq 1$ and the ERM hypothesis $h$ in
${\mathcal{H}}_{{k}^{ * }}$ that minimize an upper bound on the excess
error.

::: center
![image](images/019145d2-cc04-715a-aec7-5f9a00e87681_37_857655.jpg){width="50%"}
:::

Figure 4.4

Illustration of structural risk minimization. The plots of three errors
are shown as a function of the index $k$ . Clearly, as $k$ , or
equivalently the complexity the hypothesis set ${\mathcal{H}}_{k}$ ,
increases, the training error decreases, while the penalty term
increases. SRM selects the hypothesis minimizing a bound on the
generalization error, which is a sum of the empirical error and the
penalty term.

As we shall see, the following learning bound holds for all
$h \in \mathcal{H}$ : for any $\delta > 0$ , with probability at least
$1 - \delta$ over the draw of a sample $S$ of size $m$ from
${\mathcal{D}}^{m}$ , for all $h \in {\mathcal{H}}_{k}$ and $k \geq 1$ ,

$$R\left( h\right) \leq {\widehat{R}}_{S}\left( h\right) + {\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) + \sqrt{\frac{\log k}{m}} + \sqrt{\frac{\log \frac{2}{\delta }}{2m}}.$$

Thus, to minimize the resulting bound on the excess error
$\left( {R\left( h\right) - {R}^{ * }}\right)$ , the index $k$ and the
hypothesis $h \in {\mathcal{H}}_{k}$ should be chosen to minimize the
following objective

function:

$${F}_{k}\left( h\right) = {\widehat{R}}_{S}\left( h\right) + {\Re }_{m}\left( {\mathcal{H}}_{k}\right) + \sqrt{\frac{\log k}{m}}.$$

This is precisely the definition of the SRM solution
${h}_{S}^{\mathrm{{SRM}}}$ :

$${h}_{S}^{\mathrm{{SRM}}} = \mathop{\operatorname{argmin}}\limits_{{k \geq 1,h \in {\mathcal{H}}_{k}}}{F}_{k}\left( h\right) = \mathop{\operatorname{argmin}}\limits_{{k \geq 1,h \in {\mathcal{H}}_{k}}}{\widehat{R}}_{S}\left( h\right) + {R}_{m}\left( {\mathcal{H}}_{k}\right) + \sqrt{\frac{\log k}{m}}. \tag{4.4}$$

Thus, SRM identifies an optimal index ${k}^{ * }$ and therefore
hypothesis set ${\mathcal{H}}_{{k}^{ * }}$ , and returns the ERM
solution based on that hypothesis set. Figure 4.4 further illustrates
the selection of the index ${k}^{ * }$ and hypothesis set
${\mathcal{H}}_{{k}^{ * }}$ by SRM by minimizing an upper bound on the
sum of the training error and the penalty term
${R}_{m}\left( {\mathcal{H}}_{k}\right) + \sqrt{\log k/m}$ . The
following theorem shows that the SRM solution benefits from a strong
learning guarantee. For any $h \in \mathcal{H}$ , we will denote by
${\mathcal{H}}_{k\left( h\right) }$ the least complex hypothesis set
among the ${\mathcal{H}}_{k}\mathrm{\;s}$ that contain $h$ .

Theorem 4.2 (SRM Learning guarantee) For any $\delta > 0$ , with
probability at least $1 - \delta$ over the draw of an i.i.d. sample $S$
of size $m$ from ${\mathcal{D}}^{m}$ , the generalization error of the
hypothesis ${h}_{S}^{\mathrm{{SRM}}}$ returned by the SRM method is
bounded as follows:

$$R\left( {h}_{S}^{\mathrm{{SRM}}}\right) \leq \mathop{\inf }\limits_{{h \in \mathcal{H}}}\left( {R\left( h\right) + 2{\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) + \sqrt{\frac{\log k\left( h\right) }{m}}}\right) + \sqrt{\frac{2\log \frac{3}{\delta }}{m}}.$$

Proof: Observe first that, by the union bound, the following general
inequality holds:

$$\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{h \in \mathcal{H}}}R\left( h\right) - {F}_{k\left( h\right) }\left( h\right) > \epsilon }\right\rbrack$$

$$= \mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{k \geq 1}}\mathop{\sup }\limits_{{h \in {\mathcal{H}}_{k}}}R\left( h\right) - {F}_{k}\left( h\right) > \epsilon }\right\rbrack$$

$$\leq \mathop{\sum }\limits_{{k = 1}}^{\infty }\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{h \in {\mathcal{H}}_{k}}}R\left( h\right) - {F}_{k}\left( h\right) > \epsilon }\right\rbrack$$

$$= \mathop{\sum }\limits_{{k = 1}}^{\infty }\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{h \in {\mathcal{H}}_{k}}}R\left( h\right) - {\widehat{R}}_{S}\left( h\right) - {\Re }_{m}\left( {\mathcal{H}}_{k}\right) > \epsilon + \sqrt{\frac{\log k}{m}}}\right\rbrack$$

$$\leq \mathop{\sum }\limits_{{k = 1}}^{\infty }\exp \left( {-{2m}{\left\lbrack \epsilon + \sqrt{\frac{\log k}{m}}\right\rbrack }^{2}}\right) \tag{4.5}$$

$$\leq \mathop{\sum }\limits_{{k = 1}}^{\infty }{e}^{-{2m}{\epsilon }^{2}}{e}^{-2\log k}$$

$$= {e}^{-{2m}{\epsilon }^{2}}\mathop{\sum }\limits_{{k = 1}}^{\infty }\frac{1}{{k}^{2}} = \frac{{\pi }^{2}}{6}{e}^{-{2m}{\epsilon }^{2}} \leq 2{e}^{-{2m}{\epsilon }^{2}}.$$

Next, for any two random variables ${X}_{1}$ and ${X}_{2}$ , if
${X}_{1} + {X}_{2} > \epsilon$ , then either ${X}_{1}$ or ${X}_{2}$ must
be larger than $\epsilon /2$ . In view of that, by the union bound,
$\mathbb{P}\left\lbrack {{X}_{1} + {X}_{2} > \epsilon }\right\rbrack \leq$
$\mathbb{P}\left\lbrack {{X}_{1} > \frac{\epsilon }{2}}\right\rbrack + \mathbb{P}\left\lbrack {{X}_{2} > \frac{\epsilon }{2}}\right\rbrack$
. Using this inequality, inequality (4.5), and the inequality
${F}_{k\left( {h}_{S}^{\mathrm{{SRM}}}\right) }\left( {h}_{S}^{\mathrm{{SRM}}}\right) \leq {F}_{k\left( h\right) }\left( h\right)$
, which holds for all $h \in \mathcal{H}$ , by definition of
${h}_{S}^{\mathrm{{SRM}}}$ , we can write, for any $h \in \mathcal{H}$ ,

$$\mathbb{P}\left\lbrack {R\left( {h}_{S}^{\mathrm{{SRM}}}\right) - R\left( h\right) - 2{\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) - \sqrt{\frac{\log k\left( h\right) }{m}} > \epsilon }\right\rbrack$$

$$\leq \mathbb{P}\left\lbrack {R\left( {h}_{S}^{\mathrm{{SRM}}}\right) - {F}_{k\left( {h}_{S}^{\mathrm{{SRM}}}\right) }\left( {h}_{S}^{\mathrm{{SRM}}}\right) > \frac{\epsilon }{2}}\right\rbrack$$

$$+ \mathbb{P}\left\lbrack {{F}_{k\left( {h}_{S}^{\mathrm{{SRM}}}\right) }\left( {h}_{S}^{\mathrm{{SRM}}}\right) - R\left( h\right) - 2{\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) - \sqrt{\frac{\log k\left( h\right) }{m}} > \frac{\epsilon }{2}}\right\rbrack$$

$$\leq 2{e}^{-\frac{m{\epsilon }^{2}}{2}} + \mathbb{P}\left\lbrack {{F}_{k\left( h\right) }\left( h\right) - R\left( h\right) - 2{\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) - \sqrt{\frac{\log k\left( h\right) }{m}} > \frac{\epsilon }{2}}\right\rbrack$$

$$= 2{e}^{-\frac{m{\epsilon }^{2}}{2}} + \mathbb{P}\left\lbrack {{\widehat{R}}_{S}\left( h\right) - R\left( h\right) - {\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) > \frac{\epsilon }{2}}\right\rbrack$$

$$= 2{e}^{-\frac{m{\epsilon }^{2}}{2}} + {e}^{-\frac{m{\epsilon }^{2}}{2}} = 3{e}^{-\frac{m{\epsilon }^{2}}{2}}.$$

Setting the right-hand side to be equal to $\delta$ completes the proof.

The learning guarantee just proven for SRM is remarkable. To simplify
its discussion, let us assume that there exists ${h}^{ * }$ such that
$R\left( {h}^{ * }\right) = \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right)$
, that is, that there exists a best-in-class classifier
${h}^{ * } \in \mathcal{H}$ . Then, the theorem implies in particular
that, with probability at least $1 - \delta$ , the following inequality
holds for all $h \in \mathcal{H}$ :

$$R\left( {h}_{S}^{\mathrm{{SRM}}}\right) \leq R\left( {h}^{ * }\right) + 2{\Re }_{m}\left( {\mathcal{H}}_{k\left( {h}^{ * }\right) }\right) + \sqrt{\frac{\log k\left( {h}^{ * }\right) }{m}} + \sqrt{\frac{2\log \frac{3}{\delta }}{m}}. \tag{4.6}$$

Observe that, remarkably, this bound is similar to the estimation error
bound for ${\mathcal{H}}_{k\left( {h}^{ * }\right) }$ : it differs from
it only by the term $\sqrt{\log k\left( {h}^{ * }\right) /m}$ . Thus,
modulo that term, the guarantee for SRM is as favorable as the one we
would have obtained, had an oracle informed us of the index
$k\left( {h}^{ * }\right)$ of the best-in-class classifier's hypothesis
set.

Furthermore, observe that when $\mathcal{H}$ is rich enough that
$R\left( {h}^{ * }\right)$ is close to the Bayes error, the learning
bound (4.6) is approximately a bound on the excess error of the SRM
solution. Note that, if for some ${k}_{0}$ , the empirical error of the
ERM solution for ${\mathcal{H}}_{{k}_{0}}$ is zero, which holds in
particular if ${\mathcal{H}}_{{k}_{0}}$ contains the Bayes error, then,
we have
$\mathop{\min }\limits_{{h \in {\mathcal{H}}_{k}}}{F}_{{k}_{0}}\left( h\right) \leq \mathop{\min }\limits_{{h \in {\mathcal{H}}_{k}}}{F}_{k}\left( h\right)$
for all $k > {k}_{0}$ and only finitely many indices need to be
considered in SRM.

Assume more generally that if
$\mathop{\min }\limits_{{h \in {\mathcal{H}}_{k}}}{F}_{k}\left( h\right) \leq \mathop{\min }\limits_{{h \in {\mathcal{H}}_{k + 1}}}{F}_{k}\left( h\right)$
for some $k$ , then indices beyond $k + 1$ need not be inspected. This
property may hold for example if the empirical error cannot be further
improved after some index $k$ . In that case, the minimizing index
${k}^{ * }$ can be determined via a binary search in the interval
$\left\lbrack {1,{k}_{\max }}\right\rbrack$ , given some maximum value
${k}_{\max }.{k}_{\max }$ itself can be found by inspecting
$\mathop{\min }\limits_{{h \in {\mathcal{H}}_{{2}^{n}}}}{F}_{k}\left( h\right)$
for exponentially growing indices ${2}^{n},n \geq 1$ , and setting
${k}_{\max } = {2}^{n}$ for $n$ such that
$\mathop{\min }\limits_{{h \in {\mathcal{H}}_{{2}^{n}}}}{F}_{k}\left( h\right) \leq \mathop{\min }\limits_{{h \in {\mathcal{H}}_{{2}^{n + 1}}}}{F}_{k}\left( h\right)$
. The number of ERM computations needed to find ${k}_{\max }$ is in
$O\left( n\right) = O\left( {\log {k}_{\max }}\right)$ and similarly the
number of ERM computations due to the binary search is in
$O\left( {\log {k}_{\max }}\right)$ . Thus, if $n$ is the smallest
integer such that ${k}^{ * } < {2}^{n}$ , the overall number of ERM
computations is in $O\left( {\log {k}^{ * }}\right)$ .

While it benefits from a very favorable guarantee, SRM admits several
drawbacks. First, the decomposability of $\mathcal{H}$ into countably
many hypothesis sets, each with a converging Rademacher complexity,
remains a strong assumption. As an example, the family of all measurable
functions cannot be written as a union of countably many hypothesis sets
with finite VC-dimension. Thus, the choice of $\mathcal{H}$ or that of
the hypothesis sets ${\mathcal{H}}_{k}$ is a key component of SRM.
Second, and this is the main disadvantage of SRM, the method is
typically computationally intractable: for most hypothesis sets, finding
the solution of ERM is NP-hard and in general SRM requires determining
that solution for a large number of indices $k$ .