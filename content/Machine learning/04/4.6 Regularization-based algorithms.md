# 4.6 Regularization-based algorithms

A broad family of algorithms inspired by the SRM method is that of
regularization-based algorithm. This consists of selecting a very
complex family $\mathcal{H}$ that is an uncountable union of nested
hypothesis sets
${\mathcal{H}}_{\gamma } : \mathcal{H} = \mathop{\bigcup }\limits_{{\gamma > 0}}{\mathcal{H}}_{\gamma }$
. $\mathcal{H}$ is often chosen to be dense in the space of continuous
functions over $x$ . For example, $\mathcal{H}$ may be chosen to be the
set of all linear functions in some high-dimensional space and
${\mathcal{H}}_{\gamma }$ the subset of those functions whose norm is
bounded by $\gamma : {\mathcal{H}}_{\gamma } = \{ x \mapsto$
$\mathbf{w} \cdot \mathbf{\Phi }\left( x\right) : \parallel \mathbf{w}\parallel \leq \gamma \}$
. For some choices of $\mathbf{\Phi }$ and the high-dimensional space,
it can be shown that $\mathcal{H}$ is indeed dense in the space of
continuous functions over $x$ .

Given a labeled sample $S$ , the extension of the SRM method to an
uncountable union would then suggest selecting $h$ based on the
following optimization problem:

$$\mathop{\operatorname{argmin}}\limits_{{\gamma > 0,h \in {H}_{\gamma }}}{\widehat{R}}_{S}\left( h\right) + {\Re }_{m}\left( {\mathcal{H}}_{\gamma }\right) + \sqrt{\frac{\log \gamma }{m}}$$

where other penalty terms pen $\left( {\gamma ,m}\right)$ can be chosen
in lieu of the specific choice
$\operatorname{pen}\left( {\gamma ,m}\right) = {\mathfrak{R}}_{m}\left( {\mathcal{H}}_{\gamma }\right) + \sqrt{\frac{\log \gamma }{m}}$
. Often, there exists a function
$\mathcal{R} : \mathcal{H} \rightarrow \mathbb{R}$ such that, for any
$\gamma > 0$ , the constrained optimization problem
${\operatorname{argmin}}_{\gamma > 0,h \in {H}_{\gamma }}{\widehat{R}}_{S}\left( h\right) +$
$\operatorname{pen}\left( {\gamma ,m}\right)$ can be equivalently
written as the unconstrained optimization problem

$$\mathop{\operatorname{argmin}}\limits_{{h \in \mathcal{H}}}{\widehat{R}}_{S}\left( h\right) + \lambda \mathcal{R}\left( h\right)$$

for some $\lambda > 0.\mathcal{R}\left( h\right)$ is called a
regularization term and $\lambda > 0$ is treated as a hyperparameter
since its optimal value is often not known. For most algorithms, the
regularization term $\mathcal{R}\left( h\right)$ is chosen to be an
increasing function of $\parallel h\parallel$ for some choice of the
norm $\parallel \cdot \parallel$ , when $\mathcal{H}$ is the subset of a
Hilbert space. The variable $\lambda$ is often called a regularization
parameter. Larger values of $\lambda$ further penalize more complex
hypotheses, while, for $\lambda$ close or equal to zero, the
regularization term has no effect and the algorithm coincides with ERM.
In practice, $\lambda$ is typically selected via cross-validation or
using $n$ -fold cross-validation.

When the regularization term is chosen to be
$\parallel h{\parallel }_{p}$ for some choice of the norm and $p \geq 1$
, then it is a convex function of $h$ , since any norm is convex.
However, for the zero-one loss, the first term of the objective function
is non-convex, thereby making the optimization problem computationally
hard. In practice, most regularization-based algorithms instead use a
convex upper bound on the zero-one loss and replace the empirical
zero-one term with the empirical value of that convex surrogate. The
resulting optimization problem is then convex and therefore admits more
efficient solutions than SRM. The next section studies the properties of
such convex surrogate losses.