一个可以界定估计误差的标准算法是 **经验风险最小化(ERM)**。
ERM旨在最小化训练样本上的误差:
$$
{h}_{S}^{\mathrm{{ERM}}} = \mathop{\operatorname{argmin}}\limits_{{h \in \mathcal{H}}}{\widehat{R}}_{S}\left( h\right) \tag{4.2}
$$

> [!proposition] 命题 4.1 
> 对于任何样本$S$，ERM 返回的假设以下不等式成立:
> $$
> \begin{align*}
> &\mathbb{P}\left[ R\left( h_{S}^{\mathrm{ERM}} \right) 
>     - \inf_{h \in \mathcal{H}} R(h) > \epsilon \right] \\
>     \leq &\mathbb{P}\left[ \sup_{h \in \mathcal{H}} \left| R(h) 
>     - \widehat{R}_{S}(h) \right| > \frac{\epsilon}{2} \right] 
>     \tag{4.3}
> \end{align*}
> $$

`\begin{proof}`
根据 $\mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right)$ 的定义，对于任何$\epsilon > 0$，存在 ${h}_{\epsilon }$ 使得
$$R\left( {h}_{\epsilon }\right) \leq \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) + \epsilon$$因此，使用${\widehat{R}}_{S}\left( {h}_{S}^{\mathrm{{ERM}}}\right) \leq {\widehat{R}}_{S}\left( {h}_{\epsilon }\right)$ (因为 ${h}_{S}^{\mathrm{{ERM}}}$ 的定义)

根据算法的定义，我们可以写出
$$
\begin{align}
&R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) \\
&= R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - R\left( {h}_{\epsilon }\right) + R\left( {h}_{\epsilon }\right) - \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) \\
&\leq R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - R\left( {h}_{\epsilon }\right) + \epsilon\\
&= R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - {\widehat{R}}_{S}\left( {h}_{S}^{\mathrm{{ERM}}}\right) + {\widehat{R}}_{S}\left( {h}_{S}^{\mathrm{{ERM}}}\right) \\
&\qquad- R\left( {h}_{\epsilon }\right) + \epsilon\\
&\leq R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - {\widehat{R}}_{S}\left( {h}_{S}^{\mathrm{{ERM}}}\right) + {\widehat{R}}_{S}\left( {h}_{\epsilon }\right) \\ &\qquad - R\left( {h}_{\epsilon }\right) + \epsilon\\
&\leq 2\mathop{\sup }\limits_{{h \in \mathcal{H}}}\left| {R\left( h\right) - {\widehat{R}}_{S}\left( h\right) }\right| + \epsilon.
\end{align}
$$

由于不等式对所有$\epsilon > 0$都成立，因此它暗示了以下内容:
$$
R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) \leq 2\mathop{\sup }\limits_{{h \in \mathcal{H}}}\left| {R\left( h\right) - {\widehat{R}}_{S}\left( h\right) }\right|
$$
这就完成了证明。
`\end{proof}`

- ERM 的右侧界定
	* 可以通过上一章中介绍的一般化边界来用 Rademacher 复杂度、增长函数或 $\mathcal{H}$ 的 VC 维来界定。
	* 特别是，可以被 $2e^{-2m\left[\epsilon - \Re_m(\mathcal{H})\right]^2}$ 所界定。
- ERM 的局限性
	* 当 $\mathcal{H}$ 具有有利的 Rademacher 复杂度时 (例如有限的 VC 维)，对于足够大的样本，以高概率保证估计误差较小。
	* 然而，ERM 的性能通常非常差。
	    + 因为算法忽视了假设集 $\mathcal{H}$ 的复杂性：在实践中，
		    + 要么 $\mathcal{H}$ 不够复杂，这种情况下近似误差可能非常大，
		    + 要么 $\mathcal{H}$ 非常丰富，这种情况下对估计误差的界定变得非常宽松。
	* 还有，确定 ERM 解在计算上是不可行的：
	    + 例如，寻找在训练样本上误差最小的线性假设，作为空间维度的函数，是 NP 难的。
