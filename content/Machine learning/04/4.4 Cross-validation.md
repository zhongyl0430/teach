# 4.4 Cross-validation
An alternative method for model selection, cross-validation, consists of
using some fraction of the training sample as a validation set to select
a hypothesis set ${\mathcal{H}}_{k}$ . This is in contrast with the SRM
model which relies on a theoretical learning bound assigning a penalty
to each hypothesis set. In this section, we analyze the cross-validation
method and compare its performance to that of SRM.

As in the previous section, let
${\left( {\mathcal{H}}_{k}\right) }_{k \geq 1}$ be a countable sequence
of hypothesis sets with increasing complexities. The cross-validation
(CV) solution is obtained as follows. Let $S$ be an i.i.d. labeled
sample of size $m.S$ is divided into a sample ${S}_{1}$ of size
$\left( {1 - \alpha }\right) m$ and a sample ${S}_{2}$ of size
${\alpha m}$ , with $\alpha \in \left( {0,1}\right)$ typically chosen to
be relatively small. ${S}_{1}$ is reserved for training, ${S}_{2}$ for
validation. For any $k \in \mathbb{N}$ , let
${h}_{{S}_{1},k}^{\mathrm{{ERM}}}$ denote the solution of ERM run on
${S}_{1}$ using the hypothesis set ${\mathcal{H}}_{k}$ . The hypothesis
${h}_{S}^{\mathrm{{CV}}}$ returned by cross-validation is the ERM
solution ${h}_{{S}_{1},k}^{\mathrm{{ERM}}}$ with the best performance on
${S}_{2}$ :

$${h}_{S}^{\mathrm{{CV}}} = \mathop{\operatorname{argmin}}\limits_{{h \in \left\{ {{h}_{{S}_{1},k}^{\mathrm{{ENM}}} : k \geq 1}\right\} }}{\widehat{R}}_{{S}_{2}}\left( h\right) . \tag{4.7}$$

The following general result will help us derive learning guarantees for
cross-validation.

Proposition 4.3 For any $\alpha > 0$ and any sample size $m \geq 1$ ,
the following general inequality holds:

$$\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{k \geq 1}}\left| {R\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) - {\widehat{R}}_{{S}_{2}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) }\right| > \epsilon + \sqrt{\frac{\log k}{\alpha m}}}\right\rbrack \leq 4{e}^{-{2\alpha m}{\epsilon }^{2}}.$$

Proof: By the union bound, we can write

$$\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{k \geq 1}}\left| {R\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) - {\widehat{R}}_{{S}_{2}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) }\right| > \epsilon + \sqrt{\frac{\log k}{\alpha m}}}\right\rbrack$$

$$\leq \mathop{\sum }\limits_{{k = 1}}^{\infty }\mathbb{P}\left\lbrack {\left| {R\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) - {\widehat{R}}_{{S}_{2}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) }\right| > \epsilon + \sqrt{\frac{\log k}{\alpha m}}}\right\rbrack \tag{4.8}$$

$$= \mathop{\sum }\limits_{{k = 1}}^{\infty }\mathbb{E}\left\lbrack {\mathbb{P}\left\lbrack {\left| {R\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) - {\widehat{R}}_{{S}_{2}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) }\right| > \epsilon + \sqrt{\frac{\log k}{\alpha m}} \mid {S}_{1}}\right\rbrack }\right\rbrack .$$

The hypothesis ${h}_{{S}_{1},k}^{\mathrm{{ERM}}}$ is fixed conditioned
on ${S}_{1}$ . Furthermore, the sample ${S}_{2}$ is independent from
${S}_{1}$ . Therefore, by Hoeffding's inequality, we can bound the
conditional probability as follows:

$$\mathbb{P}\left\lbrack {\left| {R\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) - {\widehat{R}}_{{S}_{2}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) }\right| > \epsilon + \sqrt{\frac{\log k}{\alpha m}} \mid {S}_{1}}\right\rbrack \leq 2{e}^{-{2\alpha m}{\left( \epsilon + \sqrt{\frac{\log k}{\alpha m}}\right) }^{2}}.$$

$$\leq 2{e}^{-{2\alpha m}{\epsilon }^{2} - 2\log k}$$

$$= \frac{2}{{k}^{2}}{e}^{-{2\alpha m}{\epsilon }^{2}}.$$

Plugging in the right-hand side of this bound in (4.8) and summing over
$k$ yields

$$\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{k \geq 1}}\left| {R\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) - {\widehat{R}}_{{S}_{2}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) }\right| > \epsilon + \sqrt{\frac{\log k}{\alpha m}}}\right\rbrack \leq \frac{{\pi }^{2}}{3}{e}^{-{2\alpha m}{\epsilon }^{2}} < 4{e}^{-{2\alpha m}{\epsilon }^{2}},$$

which completes the proof.

Let $R\left( {h}_{{S}_{1}}^{\mathrm{{SRM}}}\right)$ be the
generalization error of the SRM solution using a sample ${S}_{1}$ of
size $\left( {1 - {\alpha m}}\right)$ and
$R\left( {{h}_{S}^{\mathrm{{CV}}},S}\right)$ the generalization error of
the cross-validation solution using a sample $S$ of size $m$ . Then,
using Proposition 4.3, the following learning guarantee can be derived
which compares the error of the CV method to that of SRM.

Theorem 4.4 (Cross-validation versus SRM) For any $\delta > 0$ , with
probability at least $1 - \delta$ , the following holds:

$$R\left( {h}_{S}^{\mathrm{{CV}}}\right) - R\left( {h}_{{S}_{1}}^{\mathrm{{SRM}}}\right) \leq 2\sqrt{\frac{\log \max \left( {k\left( {h}_{S}^{\mathrm{{CV}}}\right) ,k\left( {h}_{{S}_{1}}^{\mathrm{{SRM}}}\right) }\right) }{\alpha m}} + 2\sqrt{\frac{\log \frac{4}{\delta }}{2\alpha m}},$$

where, for any $h,k\left( h\right)$ denotes the smallest index of a
hypothesis set containing $h$ . Proof: By Proposition 4.3 and Theorem
4.2, using the property of ${h}_{S}^{\mathrm{{CV}}}$ as a minimizer, for
any $\delta > 0$ , with probability at least $1 - \delta$ , the
following inequalities

hold:

$$R\left( {h}_{S}^{\mathrm{{CV}}}\right) \leq {\widehat{R}}_{{S}_{2}}\left( {h}_{S}^{\mathrm{{CV}}}\right) + \sqrt{\frac{\log \left( {k\left( {h}_{S}^{\mathrm{{CV}}}\right) }\right) }{\alpha m}} + \sqrt{\frac{\log \frac{4}{\delta }}{2\alpha m}}$$

$$\leq {\widehat{R}}_{{S}_{2}}\left( {h}_{{S}_{1}}^{\mathrm{{SRM}}}\right) + \sqrt{\frac{\log \left( {k\left( {h}_{S}^{\mathrm{{CV}}}\right) }\right) }{\alpha m}} + \sqrt{\frac{\log \frac{4}{\delta }}{2\alpha m}}$$

$$\leq R\left( {h}_{{S}_{1}}^{\mathrm{{SRM}}}\right) + \sqrt{\frac{\log \left( {k\left( {h}_{S}^{\mathrm{{CV}}}\right) }\right) }{\alpha m}} + \sqrt{\frac{\log \left( {k\left( {h}_{{S}_{1}}^{\mathrm{{SRM}}}\right) }\right) }{\alpha m}} + 2\sqrt{\frac{\log \frac{4}{\delta }}{2\alpha m}}$$

$$\leq R\left( {h}_{{S}_{1}}^{\mathrm{{SRM}}}\right) + 2\sqrt{\frac{\log \left( {\max \left( {k\left( {h}_{S}^{\mathrm{{CV}}}\right) ,k\left( {h}_{{S}_{1}}^{\mathrm{{SRM}}}\right) }\right) }\right) }{\alpha m}} + 2\sqrt{\frac{\log \frac{4}{\delta }}{2\alpha m}},$$

which completes the proof.

The learning guarantee just proven shows that, with high probability,
the generalization error of the CV solution for a sample of size $m$ is
close to that of the SRM solution for a sample of size
$\left( {1 - \alpha }\right) m$ . For $\alpha$ relatively small, this
suggests a guarantee similar to that of SRM, which, as previously
discussed, is very favorable. However, in some unfavorable regimes, an
algorithm (here SRM) trained on $\left( {1 - \alpha }\right) m$ points
may have a significantly worse performance than when trained on $m$
points (avoiding this phase transition issue is one of the main
motivations behind the use of the $n$ -fold cross-validation method in
practice, see section 4.5). Thus, the bound suggests in fact a
trade-off: $\alpha$ should be chosen sufficiently small to avoid the
unfavorable regimes just mentioned and yet sufficiently large for the
right-hand side of the bound to be small and thus informative.

The learning bound for CV can be made more explicit in some cases in
practice. Assume for example that the hypothesis sets
${\mathcal{H}}_{k}$ are nested and that the empirical errors of the ERM
solutions ${h}_{{S}_{1},k}^{\mathrm{{ERM}}}$ are decreasing before
reaching zero: for any $k$ ,
${\widehat{R}}_{{S}_{1}}\left( {h}_{{S}_{1},k + 1}^{\mathrm{{ERM}}}\right) < {\widehat{R}}_{{S}_{1}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right)$
for all $k$ such that
${\widehat{R}}_{{S}_{1}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) > 0$
and
${\widehat{R}}_{{S}_{1}}\left( {h}_{{S}_{1},k + 1}^{\mathrm{{ERM}}}\right) \leq$
${\widehat{R}}_{{S}_{1}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right)$
otherwise. Observe that
${\widehat{R}}_{{S}_{1}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) > 0$
implies at least one error for ${h}_{{S}_{1},k}^{\mathrm{{ERM}}}$ ,
therefore
${\widehat{R}}_{{S}_{1}}\left( {h}_{{S}_{1},k}^{\mathrm{{ERM}}}\right) > \frac{1}{m}$
. In view of that, we must then have
${\widehat{R}}_{{S}_{1}}\left( {h}_{{S}_{1},n}^{\mathrm{{ERM}}}\right) =$
0 for all $n \geq m + 1$ . Thus, we have
${h}_{{S}_{1},n}^{\mathrm{{ERM}}} = {h}_{{S}_{1},m + 1}^{\mathrm{{ERM}}}$
for all $n \geq m + 1$ and we can assume that
$k\left( {f}_{CV}\right) \leq m + 1$ . Since the complexity of
${\mathcal{H}}_{k}$ increases with $k$ we also have
$k\left( {f}_{SRM}\right) \leq m + 1$ . In view of that, we obtain the
following more explicit learning bound for cross-validation:

$$R\left( {{f}_{CV},S}\right) - R\left( {{f}_{SRM},{S}_{1}}\right) \leq 2\sqrt{\frac{\log \left( \frac{4}{\delta }\right) }{2\alpha m}} + 2\sqrt{\frac{\log \left( {m + 1}\right) }{\alpha m}}.$$