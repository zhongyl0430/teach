# 4.7 Convex surrogate losses

The guarantees for the estimation error that we presented in previous
sections hold either for ERM or for SRM, which itself is defined in
terms of ERM. However, as already mentioned, for many choices of the
hypothesis set $\mathcal{H}$ , including that of linear functions,
solving the ERM optimization problem is NP-hard mainly because the
zero-one loss function is not convex. One common method for addressing
this problem consists of using a convex surrogate loss function that
upper bounds the zero-one loss. This section analyzes learning
guarantees for such surrogate losses in terms of the original loss.

The hypotheses we consider are real-valued functions
$h : X \rightarrow \mathbb{R}$ . The sign of $h$ defines a binary
classifier ${f}_{h} : \mathcal{X} \rightarrow \{ - 1, + 1\}$ defined for
all $x \in \mathcal{X}$ by

$${f}_{h}\left( x\right) = \left\{ \begin{array}{ll} + 1 & \text{ if }h\left( x\right) \geq 0 \\ - 1 & \text{ if }h\left( x\right) < 0 \end{array}\right.$$

The loss or error of $h$ at point
$\left( {x,y}\right) \in \mathfrak{X} \times \{ - 1, + 1\}$ is defined
as the binary classification error of ${f}_{h}$ :

$${1}_{{f}_{h}\left( x\right) \neq y} = {1}_{{yh}\left( x\right) < 0} + {1}_{h\left( x\right) = 0 \land y = - 1} \leq {1}_{{yh}\left( x\right) \leq 0}.$$

We will denote by $R\left( h\right)$ the expected error of
$h : R\left( h\right) = {\mathbb{E}}_{\left( {x,y}\right) \sim \mathcal{D}}\left\lbrack {1}_{{f}_{h}\left( x\right) \neq y}\right\rbrack$
. For any $x \in X$ , let $\eta \left( x\right)$ denote
$\eta \left( x\right) = \mathbb{P}\left\lbrack {y = + 1 \mid x}\right\rbrack$
and let ${\mathcal{D}}_{X}$ denote the marginal distribution over $X$ .
Then, for any $h$ , we can write

$$R\left( h\right) = \underset{\left( {x,y}\right) \sim \mathcal{D}}{\mathbb{E}}\left\lbrack {1}_{{f}_{h}\left( x\right) \neq y}\right\rbrack$$

$$= \underset{x \sim {\mathcal{D}}_{x}}{\mathbb{E}}\left\lbrack {\eta \left( x\right) {1}_{h\left( x\right) < 0} + \left( {1 - \eta \left( x\right) }\right) {1}_{h\left( x\right) > 0} + \left( {1 - \eta \left( x\right) }\right) {1}_{h\left( x\right) = 0}}\right\rbrack$$

$$= \underset{x \sim {\mathcal{D}}_{X}}{\mathbb{E}}\left\lbrack {\eta \left( x\right) {1}_{h\left( x\right) < 0} + \left( {1 - \eta \left( x\right) }\right) {1}_{h\left( x\right) \geq 0}}\right\rbrack .$$

In view of that, the Bayes classifier can be defined as assigning label
+1 to $x$ when $\eta \left( x\right) \geq \frac{1}{2}, - 1$ otherwise.
It can therefore be induced by the function ${h}^{ * }$ defined by

$${h}^{ * }\left( x\right) = \eta \left( x\right) - \frac{1}{2}. \tag{4.9}$$

We will refer to ${h}^{ * } : X \rightarrow \mathbb{R}$ as the Bayes
scoring function and will denote by ${R}^{ * }$ the error of the Bayes
classifier or Bayes scoring function:
${R}^{ * } = R\left( {h}^{ * }\right)$ .

Lemma 4.5 The excess error of any hypothesis
$h : X \rightarrow \mathbb{R}$ can be expressed as follows in terms of
$\eta$ and the Bayes scoring function ${h}^{ * }$ :

$$R\left( h\right) - {R}^{ * } = 2\underset{x \sim {\mathcal{D}}_{x}}{\mathbb{E}}\left\lbrack {\left| {{h}^{ * }\left( x\right) }\right| {1}_{h\left( x\right) {h}^{ * }\left( x\right) \leq 0}}\right\rbrack .$$

Proof: For any $h$ , we can write

$$R\left( h\right) = \underset{x \sim {\mathcal{D}}_{X}}{\mathbb{E}}\left\lbrack {\eta \left( x\right) {1}_{h\left( x\right) < 0} + \left( {1 - \eta \left( x\right) }\right) {1}_{h\left( x\right) \geq 0}}\right\rbrack$$

$$= \underset{x \sim {\mathcal{D}}_{x}}{\mathbb{E}}\left\lbrack {\eta \left( x\right) {1}_{h\left( x\right) < 0} + \left( {1 - \eta \left( x\right) }\right) \left( {1 - {1}_{h\left( x\right) < 0}}\right) }\right\rbrack$$

$$= \underset{x \sim {\mathcal{D}}_{x}}{\mathbb{E}}\left\lbrack {\left\lbrack {{2\eta }\left( x\right) - 1}\right\rbrack {1}_{h\left( x\right) < 0} + \left( {1 - \eta \left( x\right) }\right) }\right\rbrack$$

$$= \underset{x \sim {\mathcal{D}}_{x}}{\mathbb{E}}\left\lbrack {2{h}^{ * }\left( x\right) {1}_{h\left( x\right) < 0} + \left( {1 - \eta \left( x\right) }\right) }\right\rbrack ,$$

where we used for the last step equation (4.9). In view of that, for any
$h$ , the following holds:

$$R\left( h\right) - R\left( {h}^{ * }\right) = \underset{x \sim {\mathcal{D}}_{x}}{\mathbb{E}}\left\lbrack {2\left\lbrack {{h}^{ * }\left( x\right) }\right\rbrack \left( {{1}_{h\left( x\right) \leq 0} - {1}_{{h}^{ * }\left( x\right) \leq 0}}\right) }\right\rbrack$$

$$= \underset{x \sim {\mathcal{D}}_{X}}{\mathbb{E}}\left\lbrack {2\left\lbrack {{h}^{ * }\left( x\right) }\right\rbrack \operatorname{sgn}\left( {{h}^{ * }\left( x\right) }\right) {1}_{\left( {h\left( x\right) {h}^{ * }\left( x\right) \leq 0}\right) \land \left( {\left( {h\left( x\right) ,{h}^{ * }\left( x\right) }\right) \neq \left( {0,0}\right) }\right) }}\right\rbrack$$

$$= 2\underset{x \sim {\mathcal{D}}_{x}}{\mathbb{E}}\left\lbrack {\left| {{h}^{ * }\left( x\right) }\right| {1}_{h\left( x\right) {h}^{ * }\left( x\right) \leq 0}}\right\rbrack ,$$

which completes the proof, since $R\left( {h}^{ * }\right) = {R}^{ * }$
.

Let $\Phi : \mathbb{R} \rightarrow \mathbb{R}$ be a convex and
non-decreasing function so that for any $u \in \mathbb{R}$ ,
${1}_{u \leq 0} \leq \Phi \left( {-u}\right)$ . The $\Phi$ -loss of a
function $h : X \rightarrow \mathbb{R}$ at point
$\left( {x,y}\right) \in X \times \{ - 1, + 1\}$ is defined as
$\Phi \left( {-{yh}\left( x\right) }\right)$ and its expected loss given
by

$${\mathcal{L}}_{\Phi }\left( h\right) = \underset{\left( {x,y}\right) \sim \mathcal{D}}{\mathbb{E}}\left\lbrack {\Phi \left( {-{yh}\left( x\right) }\right) }\right\rbrack$$

$$= \underset{x \sim {\mathcal{D}}_{x}}{\mathbb{E}}\left\lbrack {\eta \left( x\right) \Phi \left( {-h\left( x\right) }\right) + \left( {1 - \eta \left( x\right) }\right) \Phi \left( {h\left( x\right) }\right) }\right\rbrack . \tag{4.10}$$

Notice that since
${1}_{{yh}\left( x\right) \leq 0} \leq \Phi \left( {-{yh}\left( x\right) }\right)$
, we have $R\left( h\right) \leq {\mathcal{L}}_{\Phi }\left( h\right)$ .
For any $x \in X$ , let $u \mapsto {L}_{\Phi }\left( {x,u}\right)$ be
the function defined for all $u \in \mathbb{R}$ by

$${L}_{\Phi }\left( {x,u}\right) = \eta \left( x\right) \Phi \left( {-u}\right) + \left( {1 - \eta \left( x\right) }\right) \Phi \left( u\right) .$$

Then,
${\mathcal{L}}_{\Phi }\left( h\right) = {\mathbb{E}}_{x \sim {\mathcal{D}}_{X}}\left\lbrack {{L}_{\Phi }\left( {x,h\left( x\right) }\right) }\right\rbrack$
. Since $\Phi$ is convex, $u \mapsto {L}_{\Phi }\left( {x,u}\right)$ is
convex as a sum of two convex functions. Define
${h}_{\Phi }^{ * } : X \rightarrow \left\lbrack {-\infty , + \infty }\right\rbrack$
as the Bayes solution for the loss function ${L}_{\Phi }$ . That is, for
any $x,{h}_{\Phi }^{ * }\left( x\right)$ is a solution of the following
convex optimization problem:

$${h}_{\Phi }^{ * }\left( x\right) = \mathop{\operatorname{argmin}}\limits_{{u \in \left\lbrack {-\infty , + \infty }\right\rbrack }}{L}_{\Phi }\left( {x,u}\right)$$

$$= \mathop{\operatorname{argmin}}\limits_{{u \in \left\lbrack {-\infty , + \infty }\right\rbrack }}\eta \left( x\right) \Phi \left( {-u}\right) + \left( {1 - \eta \left( x\right) }\right) \Phi \left( u\right) .$$

The solution of this optimization is in general not unique. When
$\eta \left( x\right) = 0,{h}_{\Phi }^{ * }\left( x\right)$ is a
minimizer of $u \mapsto \Phi \left( u\right)$ and since $\Phi$ is
non-decreasing, we can choose
${h}_{\Phi }^{ * }\left( x\right) = - \infty$ in that case. Similarly,
when $\eta \left( x\right) = 1$ , we can choose
${h}_{\Phi }^{ * }\left( x\right) = + \infty$ . When
$\eta \left( x\right) = \frac{1}{2}$ ,
${L}_{\Phi }\left( {x,u}\right) = \frac{1}{2}\left\lbrack {\Phi \left( {-u}\right) + \Phi \left( u\right) }\right\rbrack$
, thus, by convexity,
${L}_{\Phi }\left( {x,u}\right) \geq \Phi \left( {-\frac{u}{2} + \frac{u}{2}}\right) = \Phi \left( 0\right)$
. Thus, we can choose ${h}_{\Phi }^{ * }\left( x\right) = 0$ in that
case. For all other values of $\eta \left( x\right)$ , in case of
non-uniqueness, an arbitrary minimizer is chosen in this definition. We
will denote by ${\mathcal{L}}_{\Phi }^{ * }$ the $\Phi$ -loss of
${h}_{\Phi }^{ * } : {\mathcal{L}}_{\Phi }^{ * } = {\mathbb{E}}_{\left( {x,y}\right) \sim \mathcal{D}}\left\lbrack {\Phi \left( {-y{h}_{\Phi }^{ * }\left( x\right) }\right) }\right\rbrack$
.

Proposition 4.6 Let $\Phi$ be a convex and non-decreasing function that
is differentiable at 0 with ${\Phi }^{\prime }\left( 0\right) > 0$ .
Then, the minimizer of $\Phi$ defines the Bayes classifier: for any
$x \in X,{h}_{\Phi }^{ * }\left( x\right) > 0$ iff
${h}^{ * }\left( x\right) > 0$ and ${h}^{ * }\left( x\right) = 0$ iff
${h}_{\Phi }^{ * }\left( x\right) = 0$ , which implies
${\mathcal{L}}_{\Phi }^{ * } = {R}^{ * }$ . Proof: Fix $x \in X$ . If
$\eta \left( x\right) = 0$ , then
${h}^{ * }\left( x\right) = - \frac{1}{2}$ and
${h}_{\Phi }^{ * }\left( x\right) = - \infty$ , thus
${h}^{ * }\left( x\right)$ and ${h}_{\Phi }^{ * }\left( x\right)$ admit
the same sign. Similarly, if $\eta \left( x\right) = 1$ , then
${h}^{ * }\left( x\right) = + \frac{1}{2}$ and
${h}_{\Phi }^{ * }\left( x\right) = + \infty$ , and
${h}^{ * }\left( x\right)$ and ${h}_{\Phi }^{ * }\left( x\right)$ admit
the same sign.

Let ${u}^{ * }$ denote the minimizer defining
${h}_{\Phi }^{ * }\left( x\right) .{u}^{ * }$ is a minimizer of
$u \mapsto {L}_{\Phi }\left( {x,u}\right)$ iff the subdifferential of
that function at ${u}^{ * }$ contains 0, that is, since
$\partial {L}_{\Phi }\left( {x,{u}^{ * }}\right) =$
$- \eta \left( x\right) \partial \Phi \left( {-{u}^{ * }}\right) + \left( {1 - \eta \left( x\right) }\right) \partial \Phi \left( {u}^{ * }\right)$
, iff there exist ${v}_{1} \in \partial \Phi \left( {-{u}^{ * }}\right)$
and ${v}_{2} \in \partial \Phi \left( {u}^{ * }\right)$ such that

$$\eta \left( x\right) {v}_{1} = \left( {1 - \eta \left( x\right) }\right) {v}_{2} \tag{4.11}$$

If ${u}^{ * } = 0$ , by the differentiability of $\Phi$ at 0 we have
${v}_{1} = {v}_{2} = {\Phi }^{\prime }\left( 0\right) > 0$ and thus
$\eta \left( x\right) = \frac{1}{2}$ , that is
${h}^{ * }\left( x\right) = 0$ . Conversely, If
${h}^{ * }\left( x\right) = 0$ , that is
$\eta \left( x\right) = \frac{1}{2}$ , then, by definition, we have
${h}_{\Phi }^{ * }\left( x\right) = 0$ . Thus,
${h}^{ * }\left( x\right) = 0$ iff
${h}_{\Phi }^{ * }\left( x\right) = 0$ iff
$\eta \left( x\right) = \frac{1}{2}$ .

We can assume now that $\eta \left( x\right)$ is not in
$\left\{ {0,1,\frac{1}{2}}\right\}$ . We first show that for any
${u}_{1},{u}_{2} \in \mathbb{R}$ with ${u}_{1} < {u}_{2}$ , and any two
choices of the subgradients at ${u}_{1}$ and ${u}_{2}$ ,
${v}_{1} \in \partial \Phi \left( {u}_{1}\right)$ and
${v}_{2} \in \partial \Phi \left( {u}_{2}\right)$ , we have
${v}_{1} \leq {v}_{2}$ . By definition of the subgradients at ${u}_{1}$
and ${u}_{2}$ , the following inequalities hold:

$$\Phi \left( {u}_{2}\right) - \Phi \left( {u}_{1}\right) \geq {v}_{1}\left( {{u}_{2} - {u}_{1}}\right) \;\Phi \left( {u}_{1}\right) - \Phi \left( {u}_{2}\right) \geq {v}_{2}\left( {{u}_{1} - {u}_{2}}\right) .$$

Summing up these inequalities yields
${v}_{2}\left( {{u}_{2} - {u}_{1}}\right) \geq {v}_{1}\left( {{u}_{2} - {u}_{1}}\right)$
and thus ${v}_{2} \geq {v}_{1}$ , since ${u}_{1} < {u}_{2}$ .

Now, if ${u}^{ * } > 0$ , then we have $- {u}^{ * } < {u}^{ * }$ . By
the property shown above, this implies ${v}_{1} \leq {v}_{2}$ . We
cannot have ${v}_{1} = {v}_{2} \neq 0$ since (4.11) would then imply
$\eta \left( x\right) = \frac{1}{2}$ . We also cannot have
${v}_{1} = {v}_{2} = 0$ since by the property shown above, we must have
${\Phi }^{\prime }\left( 0\right) \leq {v}_{2}$ and thus ${v}_{2} > 0$ .
Thus, we must have ${v}_{1} < {v}_{2}$ with ${v}_{2} > 0$ , which, by
(4.11), implies $\eta \left( x\right) > 1 - \eta \left( x\right)$ , that
is ${h}^{ * }\left( x\right) > 0$ .

Conversely, if ${h}^{ * }\left( x\right) > 0$ then
$\eta \left( x\right) > 1 - \eta \left( x\right)$ . We cannot have
${v}_{1} = {v}_{2} = 0$ or ${v}_{1} = {v}_{2} \neq 0$ as already shown.
Thus, since $\eta \left( x\right) \neq 1$ , by (4.11), this implies
${v}_{1} < {v}_{2}$ . We cannot have ${u}^{ * } < - {u}^{ * }$ since, by
the property shown above, this would imply ${v}_{2} \leq {v}_{1}$ .
Thus, we must have $- {u}^{ * } \leq {u}^{ * }$ , that is
${u}^{ * } \geq 0$ , and more specifically ${u}^{ * } > 0$ since, as
already shown above, ${u}^{ * } = 0$ implies
${h}^{ * }\left( x\right) = 0$ .

Theorem 4.7 Let $\Phi$ be a convex and non-decreasing function. Assume
that there exists $s \geq 1$ and $c > 0$ such that the following holds
for all $x \in X$ :

$${\left| {h}^{ * }\left( x\right) \right| }^{s} = {\left| \eta \left( x\right) - \frac{1}{2}\right| }^{s} \leq {c}^{s}\left\lbrack {{L}_{\Phi }\left( {x,0}\right) - {L}_{\Phi }\left( {x,{h}_{\Phi }^{ * }\left( x\right) }\right) }\right\rbrack .$$

Then, for any hypothesis $h$ , the excess error of $h$ is bounded as
follows:

$$R\left( h\right) - {R}^{ * } \leq {2c}{\left\lbrack {\mathcal{L}}_{\Phi }\left( h\right) - {\mathcal{L}}_{\Phi }^{ * }\right\rbrack }^{\frac{1}{s}}$$

Proof: We will use the following inequality which holds by the convexity
of $\Phi$ :

$$\Phi \left( {-2{h}^{ * }\left( x\right) h\left( x\right) }\right) = \Phi \left( {\left( {1 - {2\eta }\left( x\right) }\right) h\left( x\right) }\right)$$

$$= \Phi \left( {\eta \left( x\right) \left( {-h\left( x\right) }\right) + \left( {1 - \eta \left( x\right) }\right) h\left( x\right) }\right)$$

$$\leq \eta \left( x\right) \Phi \left( \left( {-h\left( x\right) }\right) \right) + \left( {1 - \eta \left( x\right) }\right) \Phi \left( {h\left( x\right) }\right) = {L}_{\Phi }\left( {x,h\left( x\right) }\right) . \tag{4.12}$$

By Lemma 4.5, Jensen's inequality, and
${h}^{ * }\left( x\right) = \eta \left( x\right) - \frac{1}{2}$ , we can
write

$$R\left( h\right) - R\left( {h}^{ * }\right)$$

$$= \underset{x \sim {\mathcal{D}}_{X}}{\mathbb{E}}\left\lbrack {\left| {{2\eta }\left( x\right) - 1}\right| {1}_{h\left( x\right) {h}^{ * }\left( x\right) \leq 0}}\right\rbrack$$

$$\leq \underset{x \sim {\mathcal{D}}_{X}}{\mathbb{E}}{\left\lbrack {\left| 2\eta \left( x\right) - 1\right| }^{s}{1}_{h\left( x\right) {h}^{ * }\left( x\right) \leq 0}\right\rbrack }^{\frac{1}{s}} \tag{Jensen's ineq.}$$

$$\leq {2c}\underset{x \sim {\mathcal{D}}_{x}}{\mathbb{E}}{\left\lbrack \left\lbrack \Phi \left( 0\right) - {L}_{\Phi }\left( x,{h}_{\Phi }^{ * }\left( x\right) \right) \right\rbrack {1}_{h\left( x\right) {h}^{ * }\left( x\right) \leq 0}\right\rbrack }^{\frac{1}{s}} \tag{assumption}$$

$$\leq {2c}\underset{x \sim {\mathcal{D}}_{X}}{\mathbb{E}}{\left\lbrack \left\lbrack \Phi \left( -2{h}^{ * }\left( x\right) h\left( x\right) \right) - {L}_{\Phi }\left( x,{h}_{\Phi }^{ * }\left( x\right) \right) \right\rbrack {1}_{h\left( x\right) {h}^{ * }\left( x\right) \leq 0}\right\rbrack }^{\frac{1}{s}}\;\text{ (}\Phi \text{ non-decreasing) }$$

$$\leq {2c}\underset{x \sim {\mathcal{D}}_{X}}{\mathbb{E}}{\left\lbrack \left\lbrack {L}_{\Phi }\left( x,h\left( x\right) \right) - {L}_{\Phi }\left( x,{h}_{\Phi }^{ * }\left( x\right) \right) \right\rbrack {1}_{h\left( x\right) {h}^{ * }\left( x\right) \leq 0}\right\rbrack }^{\frac{1}{s}}\;\text{ (convexity ineq. (4.12)) }$$

$$\leq {2c}\underset{x \sim {\mathcal{D}}_{x}}{\mathbb{E}}{\left\lbrack {L}_{\Phi }\left( x,h\left( x\right) \right) - {L}_{\Phi }\left( x,{h}_{\Phi }^{ * }\left( x\right) \right) \right\rbrack }^{\frac{1}{s}},$$

which completes the proof, since
${\mathbb{E}}_{x \sim {\mathcal{D}}_{x}}\left\lbrack {{L}_{\Phi }\left( {x,{h}_{\Phi }^{ * }\left( x\right) }\right) }\right\rbrack = {L}_{\Phi }^{ * }$
.

The theorem shows that, when the assumption holds, the excess error of
$h$ can be upper bounded in terms of the excess $\Phi$ -loss. The
assumption of the theorem holds in particular for the following convex
loss functions:

-   Hinge loss, where
- $\Phi \left( u\right) = \max \left( {0,1 + u}\right)$ , with $s = 1$ and $c = \frac{1}{2}$ .

-   Exponential loss, where
    $\Phi \left( u\right) = \exp \left( u\right)$ , with $s = 2$ and
    $c = \frac{1}{\sqrt{2}}$ .
-   Logistic loss, where
    $\Phi \left( u\right) = {\log }_{2}\left( {1 + {e}^{u}}\right)$ ,
    with $s = 2$ and $c = \frac{1}{\sqrt{2}}$ .

They also hold for the square loss and the squared Hinge loss (see
Exercises 4.2 and 4.3).