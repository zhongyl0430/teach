广泛的算法家族受到SRM方法的启发，这就是基于正则化的算法家族。这包括选择一个非常复杂的家族$\mathcal{H}$，它是嵌套假设集${\mathcal{H}}_{\gamma } : \mathcal{H} = \mathop{\bigcup }\limits_{{\gamma > 0}}{\mathcal{H}}_{\gamma }$的不可数并集。$\mathcal{H}$通常被选择为在$x$上的连续函数空间中稠密。例如，$\mathcal{H}$可能被选择为某个高维空间中所有线性函数的集合，以及${\mathcal{H}}_{\gamma }$其中范数被 $\gamma : {\mathcal{H}}_{\gamma } = \{ x \mapsto \mathbf{w} \cdot \mathbf{\Phi }\left( x\right) : \parallel \mathbf{w}\parallel \leq \gamma \}$ 限制的函数子集。对于某些$\mathbf{\Phi }$的选择和高维空间，可以证明$\mathcal{H}$确实在$X$上的连续函数空间中稠密。

给定一个标记样本$S$，将SRM方法扩展到不可数并集，然后建议根据以下优化问题选择$h$:

$$
\mathop{\operatorname{argmin}}\limits_{{\gamma > 0, h \in {H}_{\gamma }}}{\widehat{R}}_{S}\left( h\right) + {\Re }_{m}\left( {\mathcal{H}}_{\gamma }\right) + \sqrt{\frac{\log \gamma }{m}}
$$

其中其他惩罚项$\operatorname{pen}\left( {\gamma , m}\right)$可以代替特定的选择$\operatorname{pen}\left( {\gamma , m}\right) = {\mathfrak{R}}_{m}\left( {\mathcal{H}}_{\gamma }\right) + \sqrt{\frac{\log \gamma }{m}}$。通常存在一个函数$\mathcal{R} : \mathcal{H} \rightarrow \mathbb{R}$，使得对于任何$\gamma > 0$，约束优化问题${\operatorname{argmin}}_{\gamma > 0, h \in {H}_{\gamma }}{\widehat{R}}_{S}\left( h\right) + \operatorname{pen}\left( {\gamma , m}\right)$可以等价地写为无约束优化问题
$$
\mathop{\operatorname{argmin}}\limits_{{h \in \mathcal{H}}}{\widehat{R}}_{S}\left( h\right) + \lambda \mathcal{R}\left( h\right)
$$

对于某些$\lambda > 0.\mathcal{R}\left( h\right)$被称为正则项，而$\lambda > 0$被视为超参数，因为其最优值通常未知。对于大多数算法，正则项$\mathcal{R}\left( h\right)$被选为$\parallel h\parallel$的增函数，对于某些范数$\parallel \cdot \parallel$的选择，当$\mathcal{H}$是希尔伯特空间的一个子集时。变量$\lambda$通常被称为正则化参数。较大的$\lambda$值会进一步惩罚更复杂的假设，而对于接近或等于零的$\lambda$，正则项没有效果，算法与ERM相一致。在实践中，$ \lambda$通常通过交叉验证或使用$n$折交叉验证来选择。

当正则项被选为$\parallel h{\parallel }_{p}$对于某些范数和$p \geq 1$的选择时，它是对$h$的凸函数，因为任何范数都是凸的。然而，对于零一损失，目标函数的第一项是非凸的，从而使优化问题在计算上变得困难。在实践中，大多数基于正则化的算法转而使用零一损失的凸上界，并用该凸替代的实证值替换实证零一项。因此，得到的优化问题是凸的，因此比SRM有更有效的解决方案。下一节研究这类凸替代损失的属性。
