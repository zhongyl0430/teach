# 4.2 Empirical risk minimization (ERM) 
A standard algorithm for which the estimation error can be bounded is
Empirical Risk Minimization (ERM). ERM seeks to minimize the error on
the training sample: ${}^{4}$

$${h}_{S}^{\mathrm{{ERM}}} = \mathop{\operatorname{argmin}}\limits_{{h \in \mathcal{H}}}{\widehat{R}}_{S}\left( h\right) . \tag{4.2}$$

Proposition 4.1 For any sample $S$ , the following inequality holds for
the hypothesis returned by ERM:

$$\mathbb{P}\left\lbrack {R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) > \epsilon }\right\rbrack \leq \mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{h \in \mathcal{H}}}\left| {R\left( h\right) - {\widehat{R}}_{S}\left( h\right) }\right| > \frac{\epsilon }{2}}\right\rbrack . \tag{4.3}$$

Proof: By definition of
$\mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right)$ , for any
$\epsilon > 0$ , there exists ${h}_{\epsilon }$ such that
$R\left( {h}_{\epsilon }\right) \leq \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) + \epsilon$
. Thus, using
${\widehat{R}}_{S}\left( {h}_{S}^{\mathrm{{ERM}}}\right) \leq {\widehat{R}}_{S}\left( {h}_{\epsilon }\right)$
, which holds by the

::: center
![image](images/019145d2-cc04-715a-aec7-5f9a00e87681_35_270236.jpg){width="60%"}
:::

Figure 4.2

Illustration of the decomposition of a rich family
$\mathcal{H} = \mathop{\bigcup }\limits_{{\gamma \in \Gamma }}{\mathcal{H}}_{\gamma }$
.

definition of the algorithm, we can write

$$R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) = R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - R\left( {h}_{\epsilon }\right) + R\left( {h}_{\epsilon }\right) - \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right)$$

$$\leq R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - R\left( {h}_{\epsilon }\right) + \epsilon$$

$$= R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - {\widehat{R}}_{S}\left( {h}_{S}^{\mathrm{{ERM}}}\right) + {\widehat{R}}_{S}\left( {h}_{S}^{\mathrm{{ERM}}}\right) - R\left( {h}_{\epsilon }\right) + \epsilon$$

$$\leq R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - {\widehat{R}}_{S}\left( {h}_{S}^{\mathrm{{ERM}}}\right) + {\widehat{R}}_{S}\left( {h}_{\epsilon }\right) - R\left( {h}_{\epsilon }\right) + \epsilon$$

$$\leq 2\mathop{\sup }\limits_{{h \in \mathcal{H}}}\left| {R\left( h\right) - {\widehat{R}}_{S}\left( h\right) }\right| + \epsilon$$

Since the inequality holds for all $\epsilon > 0$ , it implies the
following:

$$R\left( {h}_{S}^{\mathrm{{ERM}}}\right) - \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) \leq 2\mathop{\sup }\limits_{{h \in \mathcal{H}}}\left| {R\left( h\right) - {\widehat{R}}_{S}\left( h\right) }\right| ,$$

which concludes the proof.

The right-hand side of (4.3) can be upper-bounded using the
generalization bounds presented in the previous chapter in terms of the
Rademacher complexity, the growth function, or the VC-dimension of
$\mathcal{H}$ . In particular, it can be bounded by
$2{e}^{-{2m}{\left\lbrack \epsilon - {\Re }_{m}\left( \mathcal{H}\right) \right\rbrack }^{2}}$
. Thus, when $\mathcal{H}$ admits a favorable Rademacher complexity, for
example a finite VC-dimension, for a sufficiently large sample, with
high probability, the estimation error is guaranteed to be small.
Nevertheless, the performance of ERM is typically very poor. This is
because the algorithm disregards the complexity of the hypothesis set
$\mathcal{H}$ : in practice, either $\mathcal{H}$ is not complex enough,
in which case the approximation error can be very large, or
$\mathcal{H}$ is very rich, in which case the bound on the estimation
error becomes very loose. Additionally, in many cases, determining the
ERM solution is computationally intractable. For example, finding a
linear hypothesis with the smallest error on the training sample is
NP-hard, as a function of the dimension of the space.

::: center
![image](images/019145d2-cc04-715a-aec7-5f9a00e87681_36_798890.jpg){width="50%"}
:::

Figure 4.3

Choice of ${\gamma }^{ * }$ with the most favorable trade-off between
estimation and approximation errors.