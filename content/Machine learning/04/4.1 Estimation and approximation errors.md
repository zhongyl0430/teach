Let $\mathcal{H}$ be a family of functions mapping $x$ to
$\{ - 1, + 1\}$ . The excess error of a hypothesis $h$ chosen from
$\mathcal{H}$ , that is the difference between its error
$R\left( h\right)$ and the Bayes error ${R}^{ * }$ , can be decomposed
as follows:

$$R\left( h\right) - {R}^{ * } = \underset{\text{estimation }}{\underbrace{\left( R\left( h\right) - \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) \right) }} + \underset{\text{approximation }}{\underbrace{\left( \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) - {R}^{ * }\right) }}. \tag{4.1}$$

The first term is called the estimation error, the second term the
approximation error. The estimation error depends on the hypothesis $h$
selected. It measures the error of $h$ with respect to the infimum of
the errors achieved by hypotheses in $\mathcal{H}$ , or that of the
best-in-class hypothesis ${h}^{ * }$ when that infimum is reached. Note
that the definition of agnostic PAC-learning is precisely based on the
estimation error.

The approximation error measures how well the Bayes error can be
approximated using $\mathcal{H}$ . It is a property of the hypothesis
set $\mathcal{H}$ , a measure of its richness. For a more complex or
richer hypothesis $\mathcal{H}$ , the approximation error tends to be
smaller at the price of a larger estimation error. This is illustrated
by Figure 4.1.

::: center
![image](images/019145d2-cc04-715a-aec7-5f9a00e87681_34_676100.jpg){width="30%"}
:::

Figure 4.1

Illustration of the estimation error (in green) and approximation error
(in orange). Here, it is assumed that there exists a best-in-class
hypothesis, that is ${h}^{ * }$ such that
$R\left( {h}^{ * }\right) = \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right)$
.

Model selection consists of choosing $\mathcal{H}$ with a favorable
trade-off between the approximation and estimation errors. Note,
however, that the approximation error is not accessible, since in
general the underlying distribution $\mathcal{D}$ needed to determine
${R}^{ * }$ is not known. Even with various noise assumptions,
estimating the approximation error is difficult. In contrast, the
estimation error of an algorithm $\mathcal{A}$ , that is, the estimation
error of the hypothesis ${h}_{S}$ returned after training on a sample
$S$ , can sometimes be bounded using generalization bounds as shown in
the next section.