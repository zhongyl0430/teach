选择假设集 $\mathcal{H}$ 的策略
- 方法1: 選擇一個非常複雜的家族$\mathcal{H}$
	* 可以使近似误差不存在或非常小。
	* 然而，這可能導致泛化界限无法适用于$\mathcal{H}$。
- 方法2: 逐渐增加假设集的复杂性
	* 将$\mathcal{H}$分解为逐渐复杂化的假设集的并集${\mathcal{H}}_{\gamma }$，即$\mathcal{H} = \mathop{\bigcup }\limits_{{\gamma \in \Gamma }}{\mathcal{H}}_{\gamma }$。
	* 其中${\mathcal{H}}_{\gamma }$的复杂性随$\gamma$增加，对于某些集合$\Gamma$。

图 4.2
描述一个丰富家族$\mathcal{H} = \mathop{\bigcup }\limits_{{\gamma \in \Gamma }}{\mathcal{H}}_{\gamma }$的分解的图示。
![01924b36-62e2-7d6a-9feb-ac32232c9804_2_459_241_889_537_0.jpg](images/01924b36-62e2-7d6a-9feb-ac32232c9804_2_459_241_889_537_0.jpg)

- 选择参数 ${\gamma }^{ * } \in \Gamma$
	* 来决定应该选择哪个假设集${\mathcal{H}}_{{\gamma }^{ * }}$。
	* 目的是在估计误差和近似误差之间找到最有利的权衡。
- 過量誤差(也稱為過量風險)
	* 可以对其和，即$\mathop{\bigcup }\limits_{{\gamma \in \Gamma }}{\mathcal{H}}_{\gamma }$，使用统一的上限。

图4.3
选择具有最有利估计误差和近似误差权衡的 ${\gamma }^{ * }$。
![01924b36-62e2-7d6a-9feb-ac32232c9804_3_551_256_715_529_0.jpg](images/01924b36-62e2-7d6a-9feb-ac32232c9804_3_551_256_715_529_0.jpg)

结构风险最小化 (SRM) 方法
- 假设集 $\mathcal{H}$ 的分解
	* 可以将假设集 $\mathcal{H}$ 分解为可数集。
	* 例如，$\mathcal{H} = \mathop{\bigcup }\limits_{{k \geq 1}}{\mathcal{H}}_{k}$。
- 嵌套假设集
	* 假设集 ${\mathcal{H}}_{k}$ 被假设为嵌套的：${\mathcal{H}}_{k} \subset {\mathcal{H}}_{k + 1}$对于所有 $k \geq 1$。
	* 然而，这些结果也适用于非嵌套的假设集。
- SRM 流程
	* 选择索引 ${k}^{ * } \geq 1$ 和 ERM 假设 $h$ 在 ${\mathcal{H}}_{{k}^{ * }}$ 中，以最小化过量误差的上界。

如我们将看到的，以下学习界对所有 $h \in \mathcal{H}$ 都成立:
- 对于任何$\delta > 0$，
- 在从${\mathcal{D}}^{m}$中抽取大小为$m$的样本$S$的概率至少为$1 - \delta$的情况下，
- 对于所有 $h \in {\mathcal{H}}_{k}$ 和 $k \geq 1$，
$$
R\left( h\right) \leq {\widehat{R}}_{S}\left( h\right) + {\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) + \sqrt{\frac{\log k}{m}} + \sqrt{\frac{\log \frac{2}{\delta }}{2m}}.
$$

因此，为了最小化由此产生的过量误差
$\left( {R\left( h\right) - {R}^{ * }}\right)$ 的上界，索引 $k$ 和假设 $h \in {\mathcal{H}}_{k}$ 应该被选择来最小化以下目标函数
$$
{F}_{k}\left( h\right) = {\widehat{R}}_{S}\left( h\right) + {\Re }_{m}\left( {\mathcal{H}}_{k}\right) + \sqrt{\frac{\log k}{m}}.
$$

这正是 SRM 解 ${h}_{S}^{\mathrm{{SRM}}}$ 的定义:
$$
\begin{align}
{h}_{S}^{\mathrm{{SRM}}} 
&= \mathop{\operatorname{argmin}}\limits_{{k \geq 1, h \in {\mathcal{H}}_{k}}}{F}_{k}\left( h\right) \\
&= \mathop{\operatorname{argmin}}\limits_{{k \geq 1, h \in {\mathcal{H}}_{k}}}{\widehat{R}}_{S}\left( h\right) + {R}_{m}\left( {\mathcal{H}}_{k}\right) + \sqrt{\frac{\log k}{m}}. \tag{4.4}
\end{align}
$$

结构风险最小化 (SRM) 方法
- 确定最优索引 ${k}^{ * }$
	* SRM 确定了一个最优索引 ${k}^{ * }$。
	* 这意味着也确定了假设集 ${\mathcal{H}}_{{k}^{ * }}$。
- 返回 ERM 解
	* 基于选定的假设集 ${\mathcal{H}}_{{k}^{ * }}$，SRM 返回 ERM 解。
* SRM 通过最小化训练误差和惩罚项 ${R}_{m}\left( {\mathcal{H}}_{k}\right) + \sqrt{\log k/m}$ 之和的上界，进一步说明了对索引 ${k}^{ * }$ 和假设集 ${\mathcal{H}}_{{k}^{ * }}$ 的选择。
* 对于任何 $h \in \mathcal{H}$，我们将用 ${\mathcal{H}}_{k\left( h\right) }$ 表示包含 $h$ 的 ${\mathcal{H}}_{k}\mathrm{\;s}$ 中最简单的假设集。
* 任何 $h \in \mathcal{H}$，SRM 解都比 ERM 解在过量误差方面好。

图4.4
结构风险最小化的图示。显示了三个误差随索引$k$变化的曲线。显然，随着$k$的增加，或者说假设集${\mathcal{H}}_{k}$的复杂度增加，训练误差减小，而惩罚项增加。SRM 选择使泛化误差上界最小的假设，该上界是经验误差和惩罚项的和。
![01924b36-62e2-7d6a-9feb-ac32232c9804_4_567_253_672_497_0.jpg](images/01924b36-62e2-7d6a-9feb-ac32232c9804_4_567_253_672_497_0.jpg)

> [!theorem] 定理4.2 SRM学习保证
> - 对于任意的$\delta > 0$，
> 	- 从${\mathcal{D}}^{m}$中抽取大小为$m$的i.i.d.样本$S$
> - 至少为$1 - \delta$的概率下，
> 	- SRM方法返回的假设 ${h}_{S}^{\mathrm{{SRM}}}$ 的泛化误差有如下界限:
> $$
> R\left( {h}_{S}^{\mathrm{{SRM}}}\right) \leq \mathop{\inf }\limits_{{h \in \mathcal{H}}}\left( {R\left( h\right) + 2{\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) + \sqrt{\frac{\log k\left( h\right) }{m}}}\right) + \sqrt{\frac{2\log \frac{3}{\delta }}{m}}.
> $$

^thm-4-2

`\begin{proof}`
首先观察到，由联合界可知，以下不等式成立:
$$
\begin{align}
&\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{h \in \mathcal{H}}}R\left( h\right) - {F}_{k\left( h\right) }\left( h\right) > \epsilon }\right\rbrack \\
&= \mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{k \geq 1}}\mathop{\sup }\limits_{{h \in {\mathcal{H}}_{k}}}R\left( h\right) - {F}_{k}\left( h\right) > \epsilon }\right\rbrack \\
&\leq \mathop{\sum }\limits_{{k = 1}}^{\infty }\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{h \in {\mathcal{H}}_{k}}}R\left( h\right) - {F}_{k}\left( h\right) > \epsilon }\right\rbrack \\
&= \mathop{\sum }\limits_{{k = 1}}^{\infty }\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{h \in {\mathcal{H}}_{k}}}R\left( h\right) - {\widehat{R}}_{S}\left( h\right) - {\Re }_{m}\left( {\mathcal{H}}_{k}\right) > \epsilon + \sqrt{\frac{\log k}{m}}}\right\rbrack \\
&\leq \mathop{\sum }\limits_{{k = 1}}^{\infty }\exp \left( {-{2m}{\left\lbrack \epsilon + \sqrt{\frac{\log k}{m}}\right\rbrack }^{2}}\right) \tag{4.5} \\
&\leq \mathop{\sum }\limits_{{k = 1}}^{\infty }{e}^{-{2m}{\epsilon }^{2}}{e}^{-2\log k} \\
&= {e}^{-{2m}{\epsilon }^{2}}\mathop{\sum }\limits_{{k = 1}}^{\infty }\frac{1}{{k}^{2}} \\
&= \frac{{\pi }^{2}}{6}{e}^{-{2m}{\epsilon }^{2}} \\
&\leq 2{e}^{-{2m}{\epsilon }^{2}}.
\end{align}
$$

接下来，对于任意两个随机变量 ${X}_{1}$ 和 ${X}_{2}$， 如果${X}_{1} + {X}_{2} > \epsilon$， 那么 ${X}_{1}$ 或 ${X}_{2}$ 必须大于$\epsilon /2$。
基于这一点，由联合界可知，
$$\mathbb{P}\left\lbrack {{X}_{1} + {X}_{2} > \epsilon }\right\rbrack \leq \mathbb{P}\left\lbrack {{X}_{1} > \frac{\epsilon }{2}}\right\rbrack + \mathbb{P}\left\lbrack {{X}_{2} > \frac{\epsilon }{2}}\right\rbrack .$$
使用这个不等式、不等式(4.5)以及对于所有$h \in \mathcal{H}$都成立的不等式
$${F}_{k\left( {h}_{S}^{\mathrm{{SRM}}}\right) }\left( {h}_{S}^{\mathrm{{SRM}}}\right) \leq {F}_{k\left( h\right) }\left( h\right),$$
根据 ${h}_{S}^{\mathrm{{SRM}}}$ 的定义，我们可以写出，对于任意 $h \in \mathcal{H}$，
$$
\begin{align}
&\mathbb{P}\left\lbrack {R\left( {h}_{S}^{\mathrm{{SRM}}}\right) - R\left( h\right) - 2{\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) - \sqrt{\frac{\log k\left( h\right) }{m}} > \epsilon }\right\rbrack \\
&\leq \mathbb{P}\left\lbrack {R\left( {h}_{S}^{\mathrm{{SRM}}}\right) - {F}_{k\left( {h}_{S}^{\mathrm{{SRM}}}\right) }\left( {h}_{S}^{\mathrm{{SRM}}}\right) > \frac{\epsilon }{2}}\right\rbrack \\
&+ \mathbb{P}\left\lbrack {{F}_{k\left( {h}_{S}^{\mathrm{{SRM}}}\right) }\left( {h}_{S}^{\mathrm{{SRM}}}\right) - R\left( h\right) - 2{\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) - \sqrt{\frac{\log k\left( h\right) }{m}} > \frac{\epsilon }{2}}\right\rbrack \\
&\leq 2{e}^{-\frac{m{\epsilon }^{2}}{2}} + \mathbb{P}\left\lbrack {{F}_{k\left( h\right) }\left( h\right) - R\left( h\right) - 2{\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) - \sqrt{\frac{\log k\left( h\right) }{m}} > \frac{\epsilon }{2}}\right\rbrack \\
&= 2{e}^{-\frac{m{\epsilon }^{2}}{2}} + \mathbb{P}\left\lbrack {{\widehat{R}}_{S}\left( h\right) - R\left( h\right) - {\Re }_{m}\left( {\mathcal{H}}_{k\left( h\right) }\right) > \frac{\epsilon }{2}}\right\rbrack \\
&= 2{e}^{-\frac{m{\epsilon }^{2}}{2}} + {e}^{-\frac{m{\epsilon }^{2}}{2}} = 3{e}^{-\frac{m{\epsilon }^{2}}{2}}.
\end{align}
$$
将右侧设置为等于$\delta$，完成证明。
`\end{proof}`

刚才证明的SRM学习保证是引人注目的。
- 为了简化讨论，假设存在${h}^{ * }$使得 $$R\left( {h}^{ * }\right) = \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right)$$
	- 即存在一个最佳分类器${h}^{ * } \in \mathcal{H}$。
- 那么，定理特别暗示，
	- 在至少为$1 - \delta$的概率下，
	- 以下不等式对于所有$h \in \mathcal{H}$都成立:
$$
R\left( {h}_{S}^{\mathrm{{SRM}}}\right) \leq R\left( {h}^{ * }\right) + 2{\Re }_{m}\left( {\mathcal{H}}_{k\left( {h}^{ * }\right) }\right) + \sqrt{\frac{\log k\left( {h}^{ * }\right) }{m}} + \sqrt{\frac{2\log \frac{3}{\delta }}{m}}. \tag{4.6}
$$

- 注意到，值得注意的是，这个界限与 ${\mathcal{H}}_{k\left( {h}^{ * }\right) }$ 的估计误差界限相似:
	- 它仅与项 $\sqrt{\log k\left( {h}^{ * }\right) /m}$ 不同。
- 因此，除了该项之外，SRM的保证与如果我们有一个告知我们最佳分类器的假设集索引 $k\left( {h}^{ * }\right)$ 的预言者所获得的保证一样有利。

此外，观察到
- 当$\mathcal{H}$足够丰富
	- 以至于$R\left( {h}^{ * }\right)$接近贝叶斯误差时，
- 学习界限(4.6)大致上是SRM解的过量误差的界限。
- 注意，如果对于某些 ${k}_{0}$，ERM解的实证误差为零，
	- 特别是当${\mathcal{H}}_{{k}_{0}}$包含贝叶斯误差时，
	- 那么，对于所有$k > {k}_{0}$我们有$$\mathop{\min }\limits_{{h \in {\mathcal{H}}_{k}}}{F}_{{k}_{0}}\left( h\right) \leq \mathop{\min }\limits_{{h \in {\mathcal{H}}_{k}}}{F}_{k}\left( h\right) ,$$
	- 并且只需要在SRM中考虑有限多个指标。

更一般地假设，
- 如果 $\mathop{\min }\limits_{{h \in {\mathcal{H}}_{k}}}{F}_{k}\left( h\right) \leq \mathop{\min }\limits_{{h \in {\mathcal{H}}_{k + 1}}}{F}_{k}\left( h\right)$ 对于某些 $k$ 成立，那么无需检查$k + 1$以外的指标。
	- 例如，如果在某个指标$k$之后实证误差不能再进一步改善，那么这个性质可能成立。
- 在这种情况下，可以通过在区间$\left\lbrack {1,{k}_{\max }}\right\rbrack$中进行二分搜索来确定最小化指标${k}^{ * }$，给定某个最大值${k}_{\max }$。 
- ${k}_{\max }$，该最大值本身可以通过检查 $\mathop{\min }\limits_{{h \in {\mathcal{H}}_{{2}^{n}}}}{F}_{k}\left( h\right)$ 对于指数增长的指标 ${2}^{n}$, $n \geq 1$，并设置 ${k}_{\max } = {2}^{n}$ 对于 $n$ 使得 $\mathop{\min }\limits_{{h \in {\mathcal{H}}_{{2}^{n}}}}{F}_{k}\left( h\right) \leq \mathop{\min }\limits_{{h \in {\mathcal{H}}_{{2}^{n + 1}}}}{F}_{k}\left( h\right)$ 来找到。
- 找到 ${k}_{\max }$ 需要的ERM计算次数在$O\left( n\right) = O\left( {\log {k}_{\max }}\right)$ 之内，同样，由于二分搜索导致的ERM计算次数在 $O\left( {\log {k}_{\max }}\right)$ 之内。因此，如果 $n$ 是使得 ${k}^{ * } < {2}^{n}$ 成立的最小整数，那么总的ERM计算次数在 $O\left( {\log {k}^{ * }}\right)$之内。

结构风险最小化 (SRM) 方法的缺点
- 缺乏灵活性
	* SRM 依赖于 $\mathcal{H}$ 可分解为可数多个假设集，每个假设集的 Rademacher 复杂度都收敛。
	* 这仍然是一个很强的假设。
	* 例如，所有可测函数的族不能写成有限 VC维的可数多个假设集的并集。
- 需要选择 $\mathcal{H}$ 或 ${\mathcal{H}}_{k}$
	* 选择 $\mathcal{H}$ 或假设集 ${\mathcal{H}}_{k}$ 是 SRM 的关键组成部分。
	* 这需要对问题和数据进行详细了解。
- 计算成本高昂
	* SRM 的主要缺点是计算成本高昂：
	    + 对于大多数假设集，找到 ERM 的解是 NP 困难的。
	    + 通常，SRM 需要确定大量指标 $k$ 的解。
	    + 这使得 SRM 不适合大规模数据或复杂问题。
- 处理挑战
	* SRM 仍然存在处理挑战的问题：
	    + 可能需要人工调整参数和超参数。
	    + 需要仔细选择假设集 ${\mathcal{H}}_{k}$ 以确保收敛和性能。