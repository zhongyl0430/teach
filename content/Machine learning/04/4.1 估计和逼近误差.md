设 $\mathcal{H}$ 为一个从 $\mathcal{X}$ 映射到$\{ - 1, + 1\}$的函数族。
从$\mathcal{H}$中选择的假设$h$的过剩误差，即它的误差$R\left( h\right)$与贝叶斯误差${R}^{ * }$之间的差异，可以分解如下:
$$
\begin{align}
&R\left( h\right) - {R}^{ * } \\
= &\underset{\text{estimation }}{\underbrace{\left( R\left( h\right) - \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) \right) }} + \underset{\text{approximation }}{\underbrace{\left( \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right) - {R}^{ * }\right) }}. \tag{4.1}
\end{align}
$$
1. 第一项被称为估计误差，
2. 第二项是逼近误差。

估计误差取决于选择的假设 $h$。
- 它衡量的是 $h$ 相对于 $\mathcal{H}$ 中假设所能达到的最小误差的差距，或者在达到该最小值时，最佳类别假设 ${h}^{ * }$ 的误差。
- 请注意，不可知 PAC 学习的定义正是基于估计误差。

逼近误差衡量的是使用 $\mathcal{H}$ 逼近贝叶斯误差的效果。
- 它是假设集 $\mathcal{H}$ 的一个属性，是其丰富性的一个度量。
- 对于更复杂或更丰富的假设$\mathcal{H}$，逼近误差往往较小，但代价是估计误差较大。
- 这由图 4.1 说明。

图 4.1
描述估计误差(绿色)和近似误差(橙色)的图示。
在这里，假设存在一个最佳假设类别${h}^{ * }$，使得$$R\left( {h}^{ * }\right) = \mathop{\inf }\limits_{{h \in \mathcal{H}}}R\left( h\right)$$
![01924b36-62e2-7d6a-9feb-ac32232c9804_1_657_244_494_233_0.jpg](images/01924b36-62e2-7d6a-9feb-ac32232c9804_1_657_244_494_233_0.jpg)

- 模型选择包括选择$\mathcal{H}$，在近似误差和估计误差之间有利的权衡。
- 然而，需要注意的是，近似误差是不可访问的，因为在一般情况下，确定${R}^{ * }$所需的真实分布$\mathcal{D}$是未知的。
	- 即使在各种噪声假设下，估计近似误差也是困难的。
- 相比之下，算法$\mathcal{A}$的估计误差，即在对样本$S$进行训练后返回的假设${h}_{S}$的估计误差，有时可以使用下一节中展示的一般化界限来界定。