# 3.5 Chapter notes

The use of Rademacher complexity for deriving generalization bounds in
learning was first advocated by Koltchinskii \[2001\], Koltchinskii and
Panchenko \[2000\], and Bartlett, Boucheron, and Lugosi \[2002a\], see
also \[Koltchinskii and Panchenko, 2002, Bartlett and Mendelson, 2002\].
Bartlett, Bousquet, and Mendelson \[2002b\] introduced the notion of
local Rademacher complexity, that is the Rademacher complexity
restricted to a subset of the hypothesis set limited by a bound on the
variance. This can be used to derive better guarantees under some
regularity assumptions about the noise.

Theorem 3.7 is due to Massart \[2000\]. The notion of VC-dimension was
introduced by Vapnik and Chervonenkis \[1971\] and has been since
extensively studied \[Vapnik, 2006, Vapnik and Chervonenkis, 1974,
Blumer et al., 1989, Assouad, 1983, Dudley, 1999\]. In addition to the
key role it plays in machine learning, the VC-dimension is also widely
used in a variety of other areas of computer science and mathematics
(e.g., see Shelah \[1972\], Chazelle \[2000\]). Theorem 3.17 is known as
Sauer's lemma in the learning community, however the result was first
given by Vapnik and Cher-vonenkis \[1971\] (in a somewhat different
version) and later independently by Sauer \[1972\] and Shelah \[1972\].

In the realizable case, lower bounds for the expected error in terms of
the VC-dimension were given by Vapnik and Chervonenkis \[1974\] and
Haussler et al. \[1988\]. Later, a lower bound for the probability of
error such as that of theorem 3.20 was given by Blumer et al. \[1989\].
Theorem 3.20 and its proof, which improves upon this previous result,
are due to Ehrenfeucht, Haussler, Kearns, and Valiant \[1988\]. Devroye
and Lugosi \[1995\] gave slightly tighter bounds for the same problem
with a more complex expression. Theorem 3.23 giving a lower bound in the
non-realizable case and the proof presented are due to Anthony and
Bartlett \[1999\]. For other examples of application of the
probabilistic method demonstrating its full power, consult the reference
book of Alon and Spencer \[1992\].

There are several other measures of the complexity of a family of
functions used in machine learning, including covering numbers, packing
numbers, and some other complexity measures discussed in chapter 11. A
covering number ${\mathcal{N}}_{p}\left( {\mathcal{G},\epsilon }\right)$
is the minimal number of ${L}_{p}$ balls of radius $\epsilon > 0$ needed
to cover a family of loss functions $\mathcal{G}$ . A packing number
${\mathcal{M}}_{p}\left( {\mathcal{G},\epsilon }\right)$ is the maximum
number of non-overlapping ${L}_{p}$ balls of radius $\epsilon$ centered
in $\mathcal{G}$ . The two notions are closely related, in particular it
can be shown straightforwardly that
${\mathcal{M}}_{p}\left( {\mathcal{G},{2\epsilon }}\right) \leq {\mathcal{N}}_{p}\left( {\mathcal{G},\epsilon }\right) \leq {\mathcal{M}}_{p}\left( {\mathcal{G},\epsilon }\right)$
for $\mathcal{G}$ and $\epsilon > 0$ . Each complexity measure naturally
induces a different reduction of infinite hypothesis sets to finite
ones, thereby resulting in generalization bounds for infinite hypothesis
sets. Exercise 3.31 illustrates the use of covering numbers for deriving
generalization bounds using a very simple proof. There are also close
relationships between these complexity measures: for example, by
Dudley's theorem, the empirical Rademacher complexity can be bounded in
terms of ${\mathcal{N}}_{2}\left( {\mathcal{G},\epsilon }\right)$
\[Dudley, 1967, 1987\] and the covering and packing numbers can be
bounded in terms of the VC-dimension \[Haussler, 1995\]. See also
\[Ledoux and Talagrand, 1991, Alon et al., 1997, Anthony and Bartlett,
1999, Cucker and Smale, 2001, Vidyasagar, 1997\] for a number of upper
bounds on the covering number in terms of other complexity measures.