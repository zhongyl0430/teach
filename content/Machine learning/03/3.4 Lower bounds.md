

In the previous section, we presented several upper bounds on the
generalization error. In contrast, this section provides lower bounds on
the generalization error of any learning algorithm in terms of the
VC-dimension of the hypothesis set used.

These lower bounds are shown by finding for any algorithm a 'bad'
distribution. Since the learning algorithm is arbitrary, it will be
difficult to specify that particular distribution. Instead, it suffices
to prove its existence non-constructively. At a high level, the proof
technique used to achieve this is the probabilistic method of Paul
ErdÃ¶s. In the context of the following proofs, first a lower bound is
given on the expected error over the parameters defining the
distributions. From that, the lower bound is shown to hold for at least
one set of parameters, that is one distribution.

Theorem 3.20 (Lower bound, realizable case)
${Let}\mathcal{H}{be}a{hypothesis}{set}{with}{VC}$ - dimension $d > 1$ .
Then, for any $m \geq 1$ and any learning algorithm $\mathcal{A}$ ,
there exist a distribution $\mathcal{D}$ over $X$ and a target function
$f \in \mathcal{H}$ such that

$$\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{P}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},f}\right) > \frac{d - 1}{32m}}\right\rbrack \geq 1/{100}. \tag{3.31}$$

Proof: Let
$\overline{\mathcal{X}} = \left\{ {{x}_{0},{x}_{1},\ldots ,{x}_{d - 1}}\right\} \subseteq \mathcal{X}$
be a set that is shattered by $\mathcal{H}$ . For any $\epsilon > 0$ ,
we choose $\mathcal{D}$ such that its support is reduced to
$\overline{\mathcal{X}}$ and so that one point $\left( {x}_{0}\right)$

has very high probability $\left( {1 - {8\epsilon }}\right)$ , with the
rest of the probability mass distributed uniformly among the other
points:

$$\underset{\mathcal{D}}{\mathbb{P}}\left\lbrack {x}_{0}\right\rbrack = 1 - {8\epsilon }\;\text{ and }\;\forall i \in \left\lbrack {d - 1}\right\rbrack ,\underset{\mathcal{D}}{\mathbb{P}}\left\lbrack {x}_{i}\right\rbrack = \frac{8\epsilon }{d - 1}. \tag{3.32}$$

With this definition, most samples would contain ${x}_{0}$ and, since
$x$ is shattered, $\mathcal{A}$ can essentially do no better than
tossing a coin when determining the label of a point ${x}_{i}$ not
falling in the training set.

We assume without loss of generality that $\mathcal{A}$ makes no error
on ${x}_{0}$ . For a sample $S$ , we let $\bar{S}$ denote the set of its
elements falling in $\left\{ {{x}_{1},\ldots ,{x}_{d - 1}}\right\}$ ,
and let $\mathcal{S}$ be the set of samples $S$ of size $m$ such that
$\left| \bar{S}\right| \leq \left( {d - 1}\right) /2$ . Now, fix a
sample $S \in S$ , and consider the uniform distribution $\mathcal{U}$
over all labelings $f : \overline{\mathcal{X}} \rightarrow \{ 0,1\}$ ,
which are all in $\mathcal{H}$ since the set is shattered. Then, the
following lower bound holds:

$$\underset{f \sim \mathcal{U}}{\mathbb{E}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},f}\right) }\right\rbrack = \mathop{\sum }\limits_{f}\mathop{\sum }\limits_{{x \in \overline{\mathcal{X}}}}{1}_{{h}_{S}\left( x\right) \neq f\left( x\right) }\mathbb{P}\left\lbrack x\right\rbrack \mathbb{P}\left\lbrack f\right\rbrack$$

$$\geq \mathop{\sum }\limits_{f}\mathop{\sum }\limits_{{x \notin \bar{S}}}{1}_{{h}_{S}\left( x\right) \neq f\left( x\right) }\mathbb{P}\left\lbrack x\right\rbrack \mathbb{P}\left\lbrack f\right\rbrack$$

$$= \mathop{\sum }\limits_{{x \notin \bar{S}}}\left( {\mathop{\sum }\limits_{f}{1}_{{h}_{S}\left( x\right) \neq f\left( x\right) }\mathbb{P}\left\lbrack f\right\rbrack }\right) \mathbb{P}\left\lbrack x\right\rbrack$$

$$= \frac{1}{2}\mathop{\sum }\limits_{{x \notin \bar{S}}}\mathbb{P}\left\lbrack x\right\rbrack \geq \frac{1}{2}\frac{d - 1}{2}\frac{8\epsilon }{d - 1} = {2\epsilon } \tag{3.33}$$

The first lower bound holds because we remove non-negative terms from
the summation when we only consider $x \notin \bar{S}$ instead of all
$x$ in $\overline{\mathcal{X}}$ . After rearranging terms, the
subsequent equality holds since we are taking an expectation over
$f \in \mathcal{H}$ with uniform weight on each $f$ and $\mathcal{H}$
shatters $\overline{\mathcal{X}}$ . The final lower bound holds due to
the definitions of $\mathcal{D}$ and $\bar{S}$ , the latter which
implies that
$\left| {\overline{\mathcal{X}} - \bar{S}}\right| \geq \left( {d - 1}\right) /2$
.

Since (3.33) holds for all $S \in \mathcal{S}$ , it also holds in
expectation over all $S \in \mathcal{S}$ :
${\mathbb{E}}_{S \in \mathcal{S}}\left\lbrack {{\mathbb{E}}_{f \sim \mathcal{U}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},f}\right) }\right\rbrack }\right\rbrack \geq {2\epsilon }$
. By Fubini's theorem, the expectations can be per-

muted, thus,

$$\underset{f \sim \mathcal{U}}{\mathbb{E}}\left\lbrack {\underset{S \in \mathcal{S}}{\mathbb{E}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},f}\right) }\right\rbrack }\right\rbrack \geq {2\epsilon } \tag{3.34}$$

This implies that
${\mathbb{E}}_{S \in \mathcal{S}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) }\right\rbrack \geq {2\epsilon }$
for at least one labeling ${f}_{0} \in \mathcal{H}$ . Decomposing this
expectation into two parts and using
${R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) \leq {\mathbb{P}}_{\mathcal{D}}\left\lbrack {\overline{\mathcal{X}} - \left\{ {x}_{0}\right\} }\right\rbrack$
, we obtain:

$$\begin{matrix} \underset{S \in \mathcal{S}}{\mathbb{E}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) }\right\rbrack = \mathop{\sum }\limits_{{S : {R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) }}\mathbb{P}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) }\right\rbrack + \mathop{\sum }\limits_{{S : {R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) }}\mathbb{P}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) }\right\rbrack \\ S : {R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) < \epsilon \end{matrix}$$

$$\leq \underset{\mathcal{D}}{\mathbb{P}}\left\lbrack {\overline{\mathcal{X}} - \left\{ {x}_{0}\right\} }\right\rbrack \underset{S \in \mathcal{S}}{\mathbb{P}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) \geq \epsilon }\right\rbrack + \epsilon \underset{S \in \mathcal{S}}{\mathbb{P}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) < \epsilon }\right\rbrack$$

$$\leq {8\epsilon }\underset{S \in \mathcal{S}}{\mathbb{P}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) \geq \epsilon }\right\rbrack + \epsilon \left( {1 - \underset{S \in \mathcal{S}}{\mathbb{P}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) \geq \epsilon }\right\rbrack }\right) .$$

Collecting terms in
${\mathbb{P}}_{S \in \mathcal{S}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) \geq \epsilon }\right\rbrack$
yields

$$\underset{S \in \mathcal{S}}{\mathbb{P}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) \geq \epsilon }\right\rbrack \geq \frac{1}{7\epsilon }\left( {{2\epsilon } - \epsilon }\right) = \frac{1}{7}. \tag{3.35}$$

Thus, the probability over all samples $S$ (not necessarily in
$\mathcal{S}$ ) can be lower bounded as

$$\underset{S}{\mathbb{P}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) \geq \epsilon }\right\rbrack \geq \underset{S \in \mathcal{S}}{\mathbb{P}}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) \geq \epsilon }\right\rbrack \mathbb{P}\left\lbrack \mathcal{S}\right\rbrack \geq \frac{1}{7}\mathbb{P}\left\lbrack \mathcal{S}\right\rbrack . \tag{3.36}$$

This leads us to find a lower bound for
$\mathbb{P}\left\lbrack \mathcal{S}\right\rbrack$ . By the
multiplicative Chernoff bound (Theorem D.4), for any $\gamma > 0$ , the
probability that more than $\left( {d - 1}\right) /2$ points are drawn
in a sample of size $m$ verifies:

$$1 - \mathbb{P}\left\lbrack \mathcal{S}\right\rbrack = \mathbb{P}\left\lbrack {{S}_{m} \geq {8\epsilon m}\left( {1 + \gamma }\right) }\right\rbrack \leq {e}^{-{8\epsilon m}\frac{{\gamma }^{2}}{3}}. \tag{3.37}$$

Therefore, for $\epsilon = \left( {d - 1}\right) /\left( {32m}\right)$
and $\gamma = 1$ ,

$$\mathbb{P}\left\lbrack {{S}_{m} \geq \frac{d - 1}{2}}\right\rbrack \leq {e}^{-\left( {d - 1}\right) /{12}} \leq {e}^{-1/{12}} \leq 1 - {7\delta }, \tag{3.38}$$

for $\delta \leq {.01}$ . Thus
$\mathbb{P}\left\lbrack \mathcal{S}\right\rbrack \geq {7\delta }$ and
${\mathbb{P}}_{S}\left\lbrack {{R}_{\mathcal{D}}\left( {{h}_{S},{f}_{0}}\right) \geq \epsilon }\right\rbrack \geq \delta$
.

The theorem shows that for any algorithm $\mathcal{A}$ , there exists a
'bad' distribution over $\mathcal{X}$ and a target function $f$ for
which the error of the hypothesis returned by $\mathcal{A}$ is a
constant times $\frac{d}{m}$ with some constant probability. This
further demonstrates the key role played by the VC-dimension in
learning. The result implies in particular that PAC-learning in the
realizable case is not possible when the VC-dimension is infinite.

Note that the proof shows a stronger result than the statement of the
theorem: the distribution $\mathcal{D}$ is selected independently of the
algorithm $\mathcal{A}$ . We now present a theorem giving a lower bound
in the non-realizable case. The following two lemmas will be needed for
the proof.

Lemma 3.21 Let $\alpha$ be a uniformly distributed random variable
taking values in $\left\{ {{\alpha }_{ - },{\alpha }_{ + }}\right\}$ ,
where ${\alpha }_{ - } = \frac{1}{2} - \frac{\epsilon }{2}$ and
${\alpha }_{ + } = \frac{1}{2} + \frac{\epsilon }{2}$ , and let $S$ be a
sample of $m \geq 1$ random variables ${X}_{1},\ldots ,{X}_{m}$ taking
values in $\{ 0,1\}$ and drawn i.i.d. according to the distribution
${\mathcal{D}}_{\alpha }$ defined by
${\mathbb{P}}_{{\mathcal{D}}_{\alpha }}\left\lbrack {X = 1}\right\rbrack = \alpha$
. Let $h$ be a function from ${X}^{m}$ to
$\left\{ {{\alpha }_{ - },{\alpha }_{ + }}\right\}$ , then the following
holds:

$$\underset{\alpha }{\mathbb{E}}\left\lbrack {\underset{S \sim {\mathcal{D}}_{\alpha }^{m}}{\mathbb{P}}\left\lbrack {h\left( S\right) \neq \alpha }\right\rbrack }\right\rbrack \geq \Phi \left( {2\lceil m/2\rceil ,\epsilon }\right) , \tag{3.39}$$

where
$\Phi \left( {m,\epsilon }\right) = \frac{1}{4}\left( {1 - \sqrt{1 - \exp \left( {-\frac{m{\epsilon }^{2}}{1 - {\epsilon }^{2}}}\right) }}\right)$
for all $m$ and $\epsilon$ .

Proof: The lemma can be interpreted in terms of an experiment with two
coins with biases ${\alpha }_{ - }$ and ${\alpha }_{ + }$ . It implies
that for a discriminant rule $h\left( S\right)$ based on a sample $S$
drawn from ${\mathcal{D}}_{{\alpha }_{ - }}$ or
${\mathcal{D}}_{{\alpha }_{ + }}$ , to determine which coin was tossed,
the sample size $m$ must be at least
$\Omega \left( {1/{\epsilon }^{2}}\right)$ . The proof is left as an
exercise (exercise D.3).

We will make use of the fact that for any fixed $\epsilon$ the function
$m \mapsto \Phi \left( {m,x}\right)$ is convex, which is not hard to
establish.

Lemma 3.22 Let $Z$ be a random variable taking values in
$\left\lbrack {0,1}\right\rbrack$ . Then, for any
$\gamma \in \lbrack 0,1)$ ,

$$\mathbb{P}\left\lbrack {z > \gamma }\right\rbrack \geq \frac{\mathbb{E}\left\lbrack Z\right\rbrack - \gamma }{1 - \gamma } > \mathbb{E}\left\lbrack Z\right\rbrack - \gamma \tag{3.40}$$

Proof: Since the values taken by $Z$ are in
$\left\lbrack {0,1}\right\rbrack$ ,

$$\mathbb{E}\left\lbrack Z\right\rbrack = \mathop{\sum }\limits_{{z \leq \gamma }}\mathbb{P}\left\lbrack {Z = z}\right\rbrack z + \mathop{\sum }\limits_{{z > \gamma }}\mathbb{P}\left\lbrack {Z = z}\right\rbrack z$$

$$\leq \mathop{\sum }\limits_{{z \leq \gamma }}\mathbb{P}\left\lbrack {Z = z}\right\rbrack \gamma + \mathop{\sum }\limits_{{z > \gamma }}\mathbb{P}\left\lbrack {Z = z}\right\rbrack$$

$$= \gamma \mathbb{P}\left\lbrack {Z \leq \gamma }\right\rbrack + \mathbb{P}\left\lbrack {Z > \gamma }\right\rbrack$$

$$= \gamma \left( {1 - \mathbb{P}\left\lbrack {Z > \gamma }\right\rbrack }\right) + \mathbb{P}\left\lbrack {Z > \gamma }\right\rbrack$$

$$= \left( {1 - \gamma }\right) \mathbb{P}\left\lbrack {Z > \gamma }\right\rbrack + \gamma$$

which concludes the proof.

Theorem 3.23 (Lower bound, non-realizable case)
${Let}\;\mathcal{H}\;{be}\;a\;{hypothesis}\;{set}\;{with}\;{VC}$ -
dimension $d > 1$ . Then, for any $m \geq 1$ and any learning algorithm
$\mathcal{A}$ , there exists a distribution $\mathcal{D}$ over
$\mathcal{X} \times \{ 0,1\}$ such that:

$$\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{P}}\left\lbrack {{R}_{\mathcal{D}}\left( {h}_{S}\right) - \mathop{\inf }\limits_{{h \in \mathcal{H}}}{R}_{\mathcal{D}}\left( h\right) > \sqrt{\frac{d}{320m}}}\right\rbrack \geq 1/{64}. \tag{3.41}$$

Equivalently, for any learning algorithm, the sample complexity verifies

$$m \geq \frac{d}{{320}{\epsilon }^{2}}. \tag{3.42}$$

Proof: Let
$\bar{x} = \left\{ {{x}_{1},\ldots ,{x}_{d}}\right\} \subseteq x$ be a
set shattered by $\mathcal{H}$ . For any
$\alpha \in \left\lbrack {0,1}\right\rbrack$ and any vector
$\mathbf{\sigma } = {\left( {\sigma }_{1},\ldots ,{\sigma }_{d}\right) }^{\top } \in \{ - 1, + 1{\} }^{d}$
, we define a distribution ${\mathcal{D}}_{\mathbf{\sigma }}$ with
support $\bar{x} \times \{ 0,1\}$ as follows:

$$\forall i \in \left\lbrack d\right\rbrack ,\;\underset{{\mathcal{D}}_{\mathbf{\sigma }}}{\mathbb{P}}\left\lbrack \left( {{x}_{i},1}\right) \right\rbrack = \frac{1}{d}\left( {\frac{1}{2} + \frac{{\sigma }_{i}\alpha }{2}}\right) . \tag{3.43}$$

Thus, the label of each point
${x}_{i},i \in \left\lbrack d\right\rbrack$ , follows the distribution
${\mathbb{P}}_{{\mathcal{D}}_{\mathbf{\sigma }}}\left\lbrack {\cdot \mid {x}_{i}}\right\rbrack$
, that of a biased coin where the bias is determined by the sign of
${\sigma }_{i}$ and the magnitude of $\alpha$ . To determine the most
likely label of each point ${x}_{i}$ , the learning algorithm will
therefore need to estimate
${\mathbb{P}}_{{\mathcal{D}}_{\mathbf{\sigma }}}\left\lbrack {1 \mid {x}_{i}}\right\rbrack$
with an accuracy better than $\alpha$ . To make this further difficult,
$\alpha$ and $\mathbf{\sigma }$ will be selected based on the algorithm,
requiring, as in lemma ${3.21},\Omega \left( {1/{\alpha }^{2}}\right)$
instances of each point ${x}_{i}$ in the training sample.

Clearly, the Bayes classifier ${h}_{{\mathcal{D}}_{\sigma }}^{ * }$ is
defined by
${h}_{{\mathcal{D}}_{\sigma }}^{ * }\left( {x}_{i}\right) = {\operatorname{argmax}}_{y \in \{ 0,1\} }\mathbb{P}\left\lbrack {y \mid {x}_{i}}\right\rbrack =$
${1}_{{\sigma }_{i} > 0}$ for all
$i \in \left\lbrack d\right\rbrack .{h}_{{\mathcal{D}}_{\sigma }}^{ * }$
is in $\mathcal{H}$ since $\overline{\mathcal{X}}$ is shattered. For all
$h \in \mathcal{H}$ ,

$${R}_{{\mathcal{D}}_{\mathbf{\sigma }}}\left( h\right) - {R}_{{\mathcal{D}}_{\mathbf{\sigma }}}\left( {h}_{{\mathcal{D}}_{\mathbf{\sigma }}}^{ * }\right) = \frac{1}{d}\mathop{\sum }\limits_{{x \in \overline{\mathcal{X}}}}\left( {\frac{\alpha }{2} + \frac{\alpha }{2}}\right) {1}_{h\left( x\right) \neq {h}_{{\mathcal{D}}_{\mathbf{\sigma }}}^{ * }\left( x\right) } = \frac{\alpha }{d}\mathop{\sum }\limits_{{x \in \overline{\mathcal{X}}}}{1}_{h\left( x\right) \neq {h}_{{\mathcal{D}}_{\mathbf{\sigma }}}^{ * }\left( x\right) }. \tag{3.44}$$

Let ${h}_{S}$ denote the hypothesis returned by the learning algorithm
$\mathcal{A}$ after receiving a labeled sample $S$ drawn according to
${\mathcal{D}}_{\mathbf{\sigma }}$ . We will denote by
${\left| S\right| }_{x}$ the number of occurrences of a point $x$ in $S$
. Let $\mathcal{U}$ denote the uniform distribution over
$\{ - 1, + 1{\} }^{d}$ . Then, in view of (3.44), the following holds:

$$\underset{\begin{matrix} {\sigma \sim {\mathcal{U}}_{m}} \\ {S \sim {\mathcal{D}}_{\sigma }} \end{matrix}}{\mathbb{E}}\left\lbrack {\frac{1}{\alpha }\left\lbrack {{R}_{{\mathcal{D}}_{\sigma }}\left( {h}_{S}\right) - {R}_{{\mathcal{D}}_{\sigma }}\left( {h}_{{\mathcal{D}}_{\sigma }}^{ * }\right) }\right\rbrack }\right\rbrack$$

$$= \frac{1}{d}\mathop{\sum }\limits_{{x \in \overline{\mathcal{X}}}}\underset{\begin{matrix} {\sigma \sim {\mathcal{U}}_{m}} \\ {S \sim {\mathcal{D}}_{\sigma }^{m}} \end{matrix}}{\mathbb{E}}\left\lbrack {1}_{{h}_{S}\left( x\right) \neq {h}_{{\mathcal{D}}_{\sigma }}^{ * }\left( x\right) }\right\rbrack$$

$$= \frac{1}{d}\mathop{\sum }\limits_{{x \in \overline{\mathcal{X}}}}\underset{\sigma \sim \mathcal{U}}{\mathbb{E}}\left\lbrack {\underset{S \sim {\mathcal{D}}_{\sigma }^{m}}{\mathbb{P}}\left\lbrack {{h}_{S}\left( x\right) \neq {h}_{{\mathcal{D}}_{\sigma }}^{ * }\left( x\right) }\right\rbrack }\right\rbrack$$

$$= \frac{1}{d}\mathop{\sum }\limits_{{x \in \overline{\mathcal{X}}}}\mathop{\sum }\limits_{{n = 0}}^{m}\underset{\sigma \sim \mathcal{U}}{\mathbb{E}}\left\lbrack {\underset{S \sim {\mathcal{D}}_{\sigma }^{m}}{\mathbb{P}}\left\lbrack {{h}_{S}\left( x\right) \neq {h}_{{\mathcal{D}}_{\sigma }}^{ * }\left( x\right) \mid {\left| S\right| }_{x} = n}\right\rbrack \mathbb{P}\left\lbrack {{\left| S\right| }_{x} = n}\right\rbrack }\right\rbrack$$

$$\geq \frac{1}{d}\mathop{\sum }\limits_{{x \in \overline{\mathcal{X}}}}\mathop{\sum }\limits_{{n = 0}}^{m}\Phi \left( {n + 1,\alpha }\right) \mathbb{P}\left\lbrack {{\left| S\right| }_{x} = n}\right\rbrack \tag{lemma 3.21}$$

$$\geq \frac{1}{d}\mathop{\sum }\limits_{{x \in \overline{\mathcal{X}}}}\Phi \left( {m/d + 1,\alpha }\right) \;\left( \right. \text{convexity of}\Phi \left( {\cdot ,\alpha }\right) \text{and Jensenâs ineq.)}$$

$$= \Phi \left( {m/d + 1,\alpha }\right) \text{.}$$

Since the expectation over $\mathbf{\sigma }$ is lower-bounded by
$\Phi \left( {m/d + 1,\alpha }\right)$ , there must exist some
$\mathbf{\sigma } \in \{ - 1, + 1{\} }^{d}$ for which

$$\text{(3.38)} \tag{3.45}$$

Then, by lemma 3.22, for that $\mathbf{\sigma }$ , for any
$\gamma \in \left\lbrack {0,1}\right\rbrack$ ,

$$\underset{S \sim {\mathcal{D}}_{\sigma }^{m}}{\mathbb{P}}\left\lbrack {\frac{1}{\alpha }\left\lbrack {{R}_{{\mathcal{D}}_{\sigma }}\left( {h}_{S}\right) - {R}_{{\mathcal{D}}_{\sigma }}\left( {h}_{{\mathcal{D}}_{\sigma }}^{ * }\right) }\right\rbrack > {\gamma u}}\right\rbrack > \left( {1 - \gamma }\right) u, \tag{3.46}$$

where $u = \Phi \left( {m/d + 1,\alpha }\right)$ . Selecting $\delta$
and $\epsilon$ such that $\delta \leq \left( {1 - \gamma }\right) u$ and
$\epsilon \leq {\gamma \alpha u}$ gives

$$\underset{S \sim {\mathcal{D}}_{\sigma }^{m}}{\mathbb{P}}\left\lbrack {{R}_{{\mathcal{D}}_{\sigma }}\left( {h}_{S}\right) - {R}_{{\mathcal{D}}_{\sigma }}\left( {h}_{{\mathcal{D}}_{\sigma }}^{ * }\right) > \epsilon }\right\rbrack > \delta . \tag{3.47}$$

To satisfy the inequalities defining $\epsilon$ and $\delta$ , let
$\gamma = 1 - {8\delta }$ . Then,

$$\delta \leq \left( {1 - \gamma }\right) u \Leftrightarrow u \geq \frac{1}{8} \tag{3.48}$$

$$\Leftrightarrow \frac{1}{4}\left( {1 - \sqrt{1 - \exp \left( {-\frac{\left( {m/d + 1}\right) {\alpha }^{2}}{1 - {\alpha }^{2}}}\right) }}\right) \geq \frac{1}{8} \tag{3.49}$$

$$\Leftrightarrow \frac{\left( {m/d + 1}\right) {\alpha }^{2}}{1 - {\alpha }^{2}} \leq \log \frac{4}{3} \tag{3.50}$$

$$\Leftrightarrow \frac{m}{d} \leq \left( {\frac{1}{{\alpha }^{2}} - 1}\right) \log \frac{4}{3} - 1 \tag{3.51}$$

Selecting $\alpha = {8\epsilon }/\left( {1 - {8\delta }}\right)$ gives
$\epsilon = {\gamma \alpha }/8$ and the condition

$$\frac{m}{d} \leq \left( {\frac{{\left( 1 - 8\delta \right) }^{2}}{{64}{\epsilon }^{2}} - 1}\right) \log \frac{4}{3} - 1 \tag{3.52}$$

Let $f\left( {1/{\epsilon }^{2}}\right)$ denote the right-hand side. We
are seeking a sufficient condition of the form
$m/d \leq \omega /{\epsilon }^{2}$ . Since $\epsilon \leq 1/{64}$ , to
ensure that
$\omega /{\epsilon }^{2} \leq f\left( {1/{\epsilon }^{2}}\right)$ , it
suffices to impose
$\frac{\omega }{{\left( 1/{64}\right) }^{2}} = f\left( \frac{1}{{\left( 1/{64}\right) }^{2}}\right)$
. This condition gives

$$\omega = {\left( 7/{64}\right) }^{2}\log \left( {4/3}\right) - {\left( 1/{64}\right) }^{2}\left( {\log \left( {4/3}\right) + 1}\right) \approx {.003127} \geq 1/{320} = {.003125}.$$

Thus, ${\epsilon }^{2} \leq \frac{1}{{320}\left( {m/d}\right) }$ is
sufficient to ensure the inequalities.

The theorem shows that for any algorithm $\mathcal{A}$ , in the
non-realizable case, there exists a 'bad' distribution over
$X \times \{ 0,1\}$ such that the error of the hypothesis returned by
$\mathcal{A}$ is a constant times $\sqrt{\frac{d}{m}}$ with some
constant probability. The VC-dimension appears as a critical quantity in
learning in this general setting as well. In particular, with an
infinite VC-dimension, agnostic PAC-learning is not possible.
