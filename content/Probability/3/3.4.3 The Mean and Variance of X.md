For $n = 1$ , the binomial distribution becomes the Bernoulli distribution. 
From Example 3.18, the mean value of a Bernoulli variable is $\mu = p$ , so the expected number of $S$ 's on any single trial is $p$ . 
Since a binomial experiment consists of $n$ trials, intuition suggests that for 
$$X \sim \operatorname{Bin}\left( {n,p}\right)$$ 
$$E\left( X\right) = {np}$$
the product of the number of trials and the probability of success on a single trial. 
The expression for $V\left( X\right)$ is not so intuitive.

> [!proposition]
> If $X \sim \operatorname{Bin}\left( {n,p}\right)$ , then $E\left( X\right) = {np},V\left( X\right) = {np}\left( {1 - p}\right) = {npq}$ , and ${\sigma }_{X} = \sqrt{npq}$ (where $q = 1 - p$ ).

Thus, calculating the mean and variance of a binomial rv does not necessitate evaluating summations. 
The proof of the result for $E\left( X\right)$ is sketched in Exercise 64.

[[EX 3.33 purchase with credit card]]