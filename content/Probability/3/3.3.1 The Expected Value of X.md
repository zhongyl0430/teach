Let $X$ be a discrete rv with set of possible values $D$ and
$\operatorname{pmf}p\left( x\right)$ . The expected value or mean value
of $X$ , denoted by $E\left( X\right)$ or ${\mu }_{X}$ or just $\mu$ ,
is

$$E\left( X\right) = {\mu }_{X} = \mathop{\sum }\limits_{{x \in D}}x \cdot p\left( x\right)$$

EXAMPLE 3.16 For the pmf of $X =$ number of courses in (3.6),

$$\mu = 1 \cdot p\left( 1\right) + 2 \cdot p\left( 2\right) + \cdots + 7 \cdot p\left( 7\right)$$

$$= \left( 1\right) \left( {.01}\right) + 2\left( {.03}\right) + \cdots + \left( 7\right) \left( {.02}\right)$$

$$= {.01} + {.06} + {.39} + {1.00} + {1.95} + {1.02} + {.14} = {4.57}$$

If we think of the population as consisting of the $X$ values
$1,2,\ldots ,7$ , then $\mu = {4.57}$ is the population mean. In the
sequel, we will often refer to $\mu$ as the population mean rather than
the mean of $X$ in the population. Notice that $\mu$ here is not 4, the
ordinary average of $1,\ldots ,7$ , because the distribution puts more
weight on 4,5, and 6 than on other $X$ values.

In Example 3.16, the expected value $\mu$ was 4.57, which is not a
possible value of $X$ . The word expected should be interpreted with
caution because one would not expect to see an $X$ value of 4.57 when a
single student is selected.

EXAMPLE 3.17 Just after birth, each newborn child is rated on a scale
called the Apgar scale. The possible ratings are $0,1,\ldots ,{10}$ ,
with the child's rating determined by color, muscle tone, respiratory
effort, heartbeat, and reflex irritability (the best possible score is
10). Let $X$ be the Apgar score of a randomly selected child born at a
certain hospital during the next year, and suppose that the pmf of $X$
is

::: center
:::

Then the mean value of $X$ is

$$E\left( X\right) = \mu = 0\left( {.002}\right) + 1\left( {.001}\right) + 2\left( {.002}\right)$$

$$+ \cdots + 8\left( {.25}\right) + 9\left( {.12}\right) + {10}\left( {.01}\right)$$

$$= {7.15}$$

Again, $\mu$ is not a possible value of the variable $X$ . Also, because
the variable relates to a future child, there is no concrete existing
population to which $\mu$ refers. Instead, we think of the pmf as a
model for a conceptual population consisting of the values
$0,1,2,\ldots ,{10}$ . The mean value of this conceptual population is
then $\mu = {7.15}$ .

EXAMPLE 3.18 Let $X = 1$ if a randomly selected vehicle passes an
emissions test and $X = 0$ otherwise. Then $X$ is a Bernoulli rv with
$\operatorname{pmf}p\left( 1\right) = p$ and $p\left( 0\right) = 1 - p$
, from which
$E\left( X\right) = 0 \cdot p\left( 0\right) + 1 \cdot p\left( 1\right) = 0\left( {1 - p}\right) + 1\left( p\right) = p$
. That is, the expected value of $X$ is just the probability that $X$
takes on the value 1 . If we conceptualize a population consisting of
$0\mathrm{\;s}$ in proportion $1 - p$ and $1\mathrm{\;s}$ in proportion
$p$ , then the population average is $\mu = p$ .

EXAMPLE 3.19 The general form for the pmf of $X =$ the number of
children born up to and including the first boy is

$$p\left( x\right) = \left\{ \begin{matrix} p{\left( 1 - p\right) }^{x - 1} & x = 1,2,3,\ldots \\ 0 & \text{ otherwise } \end{matrix}\right.$$

From the definition,

$$E\left( X\right) = \mathop{\sum }\limits_{D}x \cdot p\left( x\right) = \mathop{\sum }\limits_{{x = 1}}^{\infty }{xp}{\left( 1 - p\right) }^{x - 1} = p\mathop{\sum }\limits_{{x = 1}}^{\infty }\left\lbrack {-\frac{d}{dp}{\left( 1 - p\right) }^{x}}\right\rbrack \tag{3.9}$$

Interchanging the order of taking the derivative and the summation, the
sum is that of a geometric series. After the sum is computed, the
derivative is taken, resulting in $E\left( X\right) = 1/p$ . If $p$ is
near 1, we expect to see a boy very soon, whereas if $p$ is near 0, we
expect many births before the first boy. For
$p = {.5},E\left( X\right) = 2$ .

There is another frequently used interpretation of $\mu$ . Consider
observing a first value ${x}_{1}$ of $X$ , then a second value ${x}_{2}$
, a third value ${x}_{3}$ , and so on. After doing this a large number
of times, calculate the sample average of the observed
${x}_{\mathrm{i}}^{\prime }\mathrm{s}$ . This average will typically be
quite close to $\mu$ . That is, $\mu$ can be interpreted as the longrun
average observed value of $X$ when the experiment is performed
repeatedly. In Example 3.17, the long-run average Apgar score is
$\mu = {7.15}$ .

EXAMPLE 3.20 Let $X$ , the number of interviews a student has prior to
getting a job, have pmf

$$p\left( x\right) = \left\{ \begin{array}{ll} k/{x}^{2} & x = 1,2,3,\ldots \\ 0 & \text{ otherwise } \end{array}\right.$$

where $k = {\pi }^{2}/6$ insures that ${\sum p}\left( x\right) = 1$ (the
value of $k$ comes from a result in Fourier series). The expected value
of $X$ is

$$\mu = E\left( X\right) = \mathop{\sum }\limits_{{x = 1}}^{\infty }x \cdot \frac{k}{{x}^{2}} = k\mathop{\sum }\limits_{{x = 1}}^{\infty }\frac{1}{x} \tag{3.10}$$

The sum on the right of Equation (3.10) is the famous harmonic series of
mathematics and can be shown to equal $\infty .E\left( X\right)$ is not
finite here because $p\left( x\right)$ does not decrease sufficiently
fast as $x$ increases; statisticians say that the probability
distribution of $X$ has \"a heavy tail.\" If a sequence of $X$ values is
chosen using this distribution, the sample average will not settle down
to some finite number but will tend to grow without bound.

Statisticians use the phrase \"heavy tails\" in connection with any
distribution having a large amount of probability far from $\mu$ (so
heavy tails do not require $\mu = \infty$ ). Such heavy tails make it
difficult to make inferences about $\mu$ .