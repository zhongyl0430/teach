- The probabilities assigned to various events depend on 
	- what is known about the experimental situation 
	- when the assignment is made. 
- Subsequent to the initial assignment, partial information relevant to the outcome of the experiment may become available. 
- Such information may cause us to revise some of our probability assignments. 
- For a particular event $A$ , 
	- we have used $P\left( A\right)$ to represent the probability, assigned to $A$ ; 
	- we now think of $P\left( A\right)$ as the original, or *unconditional probability*, of the event $A$ .

- In this section, we examine how the information "an event $B$ has occurred" affects the probability assigned to $A$ . 
- For example, $A$ might refer to an individual having a particular disease in the presence of certain symptoms. 
	- If a blood test is performed on the individual and the result is negative $\left( {B = \text{negative blood test}}\right)$, 
	- then the probability of having the disease will change 
		- it should decrease, but not usually to zero, since blood tests are not infallible
- We will use the notation $P\left( {A \mid B}\right)$ to represent the **conditional probability of $A$ given that the event $B$ has occurred**. 
	- $B$ is the "conditioning event."

> [!example]
> As an example, 
> - consider the event $A$ that 
> 	- a randomly selected student at your university obtained all desired classes during the previous term's registration cycle. 
> - Presumably $P\left( A\right)$ is not very large. 
> - However, suppose the selected student is an athlete who gets special registration priority (the event $B$ ). 
> - Then $P\left( {A \mid B}\right)$ should be substantially larger than $P\left( A\right)$, 
> 	- although perhaps still not close to 1 .

[[EX 2.24]]

- In Equation (2.2), the conditional probability is expressed as a ratio of *unconditional probabilities*: 
	- The numerator is the probability of the intersection of the two events, 
	- whereas the denominator is the probability of the conditioning event $B$.
- A Venn diagram illuminates this relationship (Figure 2.8).

Figure 2.8 Motivating the definition of conditional probability
![Figure 2.8](01913607-292d-7d0a-a250-4b01870485a1_25_271463.jpg)

- Given that $B$ has occurred, the relevant sample space 
	- is no longer $\mathcal{S}$ 
	- but consists of outcomes in $B$; 
- $A$ has occurred if and only if 
	- one of the outcomes in the intersection occurred, 
- so the conditional probability of $A$ given $B$ is proportional to $P\left( {A \cap B}\right)$ . 
- The proportionality constant $1/P\left( B\right)$ is used to ensure that 
	- the probability $P\left( {B \mid B}\right)$ of the *new* sample space $B$ equals 1 .

[[2.4.1 The Definition of Conditional Probability]]

[[2.4.2 The Multiplication Rule for union of two sets]]

[[2.4.3 Bayes' Theorem]]

[[EXERCISES Section 2.4 (45-69)]]

