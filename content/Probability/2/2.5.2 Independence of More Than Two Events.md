
- The notion of independence of two events can be generalized to collections of more than two events. 
- Although it is possible to extend the definition for two independent events by working in terms of conditional and unconditional probabilities, 
- it is more direct and less cumbersome to proceed along the lines of the last proposition.

> [!definition] mutually independent
> Events ${A}_{1},\ldots ,{A}_{n}$ are **mutually independent** if for every $k\left( {k = 2,3,\ldots ,n}\right)$ and every subset of indices ${i}_{1},{i}_{2},\ldots ,{i}_{k}$ ,
> $$P\left( {{A}_{{i}_{1}} \cap {A}_{{i}_{2}} \cap \ldots \cap {A}_{{i}_{k}}}\right) = P\left( {A}_{{i}_{1}}\right) \cdot P\left( {A}_{{i}_{2}}\right) \cdots \cdot P\left( {A}_{{i}_{k}}\right)$$

- To paraphrase the definition, the events are mutually independent if the probability of the intersection of any subset of the $n$ events is equal to the product of the individual probabilities. 
- In using the multiplication property for more than two independent events, it is legitimate to replace one or more of the ${A}_{i}$'s by their complements 
	- e.g., if ${A}_{1},{A}_{2}$ , and ${A}_{3}$ are independent events, so are ${A}_{1}^{\prime },{A}_{2}^{\prime }$ , and ${A}_{3}^{\prime }$ . 
- As was the case with two events, we frequently specify at the outset of a problem the independence of certain events. 
- The probability of an intersection can then be calculated via multiplication.

[[EX 2.36]]