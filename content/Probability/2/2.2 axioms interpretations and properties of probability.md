---
tags:
  - teach
---
## Content
- [[#2.2.1 Axioms]]
- [[#2.2.2 Interpreting probability]]
- [[#2.2.3 More probability properties]]
- [[#2.2.4 Determining Probabilities Systematically]]
---
## 2.2.1 Axioms

- [[#introduction]]
- [[#axioms of probability]]
-   examples: [[#EX 2.11]] and [[#EX 2.12]]
---
### introduction

> [!definition]
> Given an experiment and a sample space $\mathcal{S}$, the objective of probability is to assign to each event $A$ a number $P\left( A\right)$, called **the probability of the event $A$**, which will give a precise measure of the chance that $A$ will occur:

To ensure that the probability assignments will be consistent with our intuitive notions of probability, all assignments should satisfy the following axioms (basic properties) of probability.

---
### axioms of probability

> [!definition]
> 1.  For any event $A,P\left( A \right) \geq 0$.
> 2.  $P\left( \mathcal{S}\right) = 1$.
> 3.  If ${A}_{1},{A}_{2},{A}_{3},\ldots$ is an infinite collection of disjoint events, then
   > $$
   > P\left( {{A}_{1} \cup {A}_{2} \cup {A}_{3} \cup \cdots }\right) = \sum_{i = 1}^{\infty} P\left( {A}_{i}\right)
   > $$

---
#### remark on 3rd axiom

You might wonder why the third axiom contains no reference to a finite collection of disjoint events. It is because the corresponding property for a finite collection can be derived from our three axioms. We want the axiom list to be as short as possible and not contain any property that can be derived from others on the list. 

---
> [!remark]
> 1. Axiom 1 reflects the intuitive notion that the chance of $A$ occurring should be nonnegative. 
> 2. The sample space is by definition the event that must occur when the experiment is performed $(\mathcal{S}$ contains all possible outcomes), so Axiom 2 says that the maximum possible probability of 1 is assigned to $\mathcal{S}$ . 
> 3. The third axiom formalizes the idea that if we wish the probability that at least one of a number of events will occur and no two of the events can occur simultaneously, then the chance of at least one occurring is the sum of the chances of the individual events.

---
### PROP: probability of null set
> [!property]
> $P \left( \varnothing \right) = 0$ where $\varnothing$ is the null event (the event containing no outcomes whatsoever).

This in turn implies that the property contained in Axiom 3 is valid for a finite collection of disjoint events.

---
#### Proof

First consider the infinite collection $A_1 = \varnothing, A_2 = \varnothing, A_3 = \varnothing,\ldots$.
Since $\varnothing \cap \varnothing = \varnothing$, the events in this collection are disjoint and $\cup A_i = \varnothing$.
The third axiom then gives
$$
P \left( \varnothing \right) = \sum P \left( \varnothing \right)
$$
This can happen only if $P \left( \varnothing \right) = 0$.

---
Now suppose that ${A}_{1},{A}_{2},\ldots ,{A}_{k}$ are disjoint events, and append to these the infinite collection $A_{k+1} = \varnothing$, $A_{k + 2} = \varnothing$, ${A}_{k + 3} = \varnothing$, ... .
Again invoking the third axiom,
$$
\begin{aligned}
P\left( \bigcup_{i = 1}^{k} A_i \right)
&= P \left( \bigcup_{i = 1}^{\infty} A_i \right) \\
&= \sum_{i = 1}^{\infty} P \left( A_i \right) \\
&= \sum_{i = 1}^{k} P \left( A_i \right)
\end{aligned}
$$
as desired.

---
### EX 2.11

Consider tossing a thumbtack in the air. When it comes to rest on the ground, either its point will be up (the outcome $U$) or down (the outcome $D$).
The sample space for this event is therefore $\mathcal{S} = \{ U,D \}$.
The axioms specify $P\left( \mathcal{S} \right) = 1$, so the probability assignment will be completed by determining $P\left( U\right)$ and $P\left( D\right)$ .
Since $U$ and $D$ are disjoint and their union is $\mathcal{S}$, the foregoing proposition implies that
$$
1 = P\left( \mathcal{S}\right) = P\left( U\right) + P\left( D\right)
$$
It follows that $P\left( D\right) = 1 - P\left( U\right)$.

-   One possible assignment of probabilities is $P \left( U \right) = .5$, $P \left( D \right) = .5$
-   another possible assignment is $P \left( U \right) = .75$, $P \left( D \right) = .25$.

In fact, letting $p$ represent any fixed number between 0 and 1, $P\left( U\right) = p$ , $P\left( D\right) = 1 - p$ is an assignment consistent with the axioms.

---
### EX 2.12

Consider testing batteries coming off an assembly line one by one until one having a voltage within prescribed limits is found.
The simple events are
-   ${E}_{1} = \{ S \}$,
-   ${E}_{2} = \{ FS \}$,
-   ${E}_{3} = \{ FFS \}$,
-   ${E}_{4} = \{ FFFS \}$, ...
---
Suppose the probability of any particular battery being satisfactory is .99.
Then it can be shown that
-   $P\left( {E}_{1} \right) = .99$
-   $P\left( {E}_{2} \right) = \left( .01 \right) \left( {.99}\right)$
-   $P\left( {E}_{3}\right) = {\left( {.01}\right) }^{2}\left( {.99}\right) ,\ldots$
is an assignment of probabilities to the simple events that satisfies the axioms.
In particular, because the ${E}_{i}$'s are disjoint and $\mathcal{S} = {E}_{1} \cup {E}_{2} \cup {E}_{3} \cup \ldots$ , it must be the case that
$$

\begin{aligned}
1
&= P\left( \mathcal{S}\right) \\
&= P\left( {E}_{1}\right) + P\left( {E}_{2}\right) + P\left( {E}_{3}\right) + \cdots \\
&= {.99}\left\lbrack {1 + {.01} + {\left( {.01}\right) }^{2} + {\left( {.01}\right) }^{3} + \cdots }\right\rbrack
\end{aligned}

$$
---
Here we have used the formula for the sum of a geometric series:
$$
a + {ar} + a{r}^{2} + a{r}^{3} + \cdots = \frac{a}{1 - r}
$$

However, another legitimate (according to the axioms) probability assignment of the same "geometric" type is obtained by replacing. 99 by any other number $p$ between 0 and 1 (and .01 by $1 - p$ ).

---
## 2.2.2 Interpreting probability

---
### introduction

Examples [[#EX 2.11|2.11]] and [[#EX 2.12|2.12]] show that the axioms do not completely determine an assignment of probabilities to events.
The axioms serve only to rule out assignments inconsistent with our intuitive notions of probability.
In the tack-tossing experiment of Example 2.11, two particular assignments were suggested.
The appropriate or correct assignment depends on the nature of the thumbtack and also on one's interpretation of probability.
The interpretation that is most frequently used and most easily understood is based on the notion of relative frequencies.

---
### relative frequency

Consider an experiment that can be repeatedly performed in an identical and independent fashion, and let $A$ be an event consisting of a fixed set of outcomes of the experiment.

Simple examples of such repeatable experiments include the tack-tossing and die-tossing experiments previously discussed.

---
If the experiment is performed $n$ times, on some of the replications the event $A$ will occur (the outcome will be in the set $A$ ), and on others, $A$ will not occur.

> [!defintion]
> Let $n \left( A \right)$ denote the number of replications on which $A$ does occur.
Then the ratio $n\left( A\right) /n$ is called the **relative frequency** of occurrence of the event $A$ in the sequence of $n$ replications.

---
### example: package

Let $A$ be the event that a package sent within the state of California for ${2}^{\text{nd}}$ day delivery actually arrives within one day.
The results from sending 10 such packages (the first 10 replications) are as follows:

| Package #                 | 1            | 2   | 3    | 4   | 5            | 6            | 7    | 8    | 9            | 10  |
| ------------------------- | ------------ | --- | ---- | --- | ------------ | ------------ | ---- | ---- | ------------ | --- |
| Did A occur?              | $\mathrm{N}$ | Y   | Y    | Y   | $\mathrm{N}$ | $\mathrm{N}$ | Y    | Y    | $\mathrm{N}$ | N   |
| Relative frequency of $A$ | 0            | .5  | .667 | .75 | .6           | .5           | .571 | .625 | .556         | .5  |

---
![[01913607-292d-7d0a-a250-4b01870485a1_9_580462.jpg]]
Figure 2.2

Behavior of relative frequency (a) Initial fluctuation (b) Long-run stabilization
- Figure 2.2(a) shows how the relative frequency $n ( A ) /n$ fluctuates rather substantially over the course of the first 50 replications.
- But as the number of replications continues to increase, Figure 2.2(b) illustrates how the relative frequency stabilizes.

---
### limiting relative frequency

More generally, empirical evidence, based on the results of many such repeatable experiments, indicates that any relative frequency of this sort will stabilize as the number of replications $n$ increases.

> [!definition]
That is, as $n$ gets arbitrarily large, $n\left( A\right) /n$ approaches a limiting value referred to as the **limiting (or long-run) relative frequency** of the event $A$.

---
The objective interpretation of probability identifies this limiting relative frequency with $P\left( A\right)$.
- Suppose that probabilities are assigned to events in accordance with their limiting relative frequencies. Then a statement such as "the probability of a package being delivered within one day of mailing is . 6 " means that of a large number of mailed packages, roughly $60 \\%$ will arrive within one day.
- Similarly, if $B$ is the event that an appliance of a particular type will need service while under warranty, then $P\left( B\right) = .1$ is interpreted to mean that in the long run $10 \\%$ of such appliances will need warranty service.
- This doesn't mean that exactly 1 out of 10 will need service, or that exactly 10 out of 100 will need service, because 10 and 100 are not the long run.
---
### objective relative frequency interpretation

This relative frequency interpretation of probability is said to be **objective** because it rests on a property of the experiment rather than on any particular individual concerned with the experiment.
For example, two different observers of a sequence of coin tosses should both use the same probability assignments since the observers have nothing to do with limiting relative frequency.
In practice, this interpretation is not as objective as it might seem, since the limiting relative frequency of an event will not be known.
Thus we will have to assign probabilities based on our beliefs about the limiting relative frequency of events under study.

---
Fortunately, there are many experiments for which there will be a consensus with respect to probability assignments.
- When we speak of a fair coin, we shall mean $P\left( H\right) = P\left( T\right) = {.5}$
- a fair die is one for which limiting relative frequencies of the six outcomes are all $1/6$ , suggesting probability assignments $P\left( \{ 1\} \right) = \cdots = P\left( \{ 6\} \right) = 1/6$.
---
### subjective relative frequency interpretation

Because the objective interpretation of probability is based on the notion of limiting frequency, its applicability is limited to experimental situations that are repeatable.
Yet the language of probability is often used in connection with situations that are inherently unrepeatable.

> [!example]
> - "The chances are good for a peace agreement"
> - "It is likely that our company will be awarded the contract"
> - "Because their best quarterback is injured, I expect them to score no more than 10 points against us."

---
In such situations we would like, as before, to assign numerical probabilities to various outcomes and events (e.g., the probability is .9 that we will get the contract).
This necessitates adopting an alternative interpretation of these probabilities.
Because different observers may have different prior information and opinions concerning such experimental situations, probability assignments may now differ from individual to individual.
Interpretations in such situations are thus referred to as **subjective**.
The book by Robert Winkler listed in the chapter references gives a very readable survey of several subjective interpretations.

---
## 2.2.3 More probability properties

### PROP: complementary
> [!property]
> For any event $A,P\left( A\right) + P\left( {A}'\right) = 1$ , from which $P\left( A\right) = 1 - P\left( {A}'\right)$.

This proposition is surprisingly useful because there are many situations in which $P\left( {A}'\right)$ is more easily obtained by direct methods than is $P\left( A\right)$ .

---
#### Proof

In Axiom 3, let $k = 2$, ${A}_{1} = A$ , and ${A}_{2} = A'$.
Since by definition of ${A}'$ , $A \cup {A}' = \mathcal{S}$ while $A$ and ${A}'$ are disjoint, $1 = P\left( \mathcal{S}\right) = P\left( {A \cup {A}'}\right) = P\left( A\right) + P\left( {A}'\right)$.

---
#### EX 2.13

Consider a system of five identical components connected in series, as illustrated in Figure 2.3.

![[01913607-292d-7d0a-a250-4b01870485a1_10_680627.jpg]]
Figure 2.3 A system of five components connected in a series

Denote 
- a component that fails by $F$
- one that doesn't fail by $S$ (for success).
---
Let $A$ be the event that the system fails.
For $A$ to occur, at least one of the individual components must fail.

Outcomes in $A$ include $SSFSS$ ( 1, 2, 4, and 5 all work,but 3 does not), $FFSSS$, and so on.
There are in fact 31 different outcomes in $A$.
However, $A\prime$, the event that the system works, consists of the single outcome $SSSSS$.
We will see in Section 2.5 that if $90\\%$ of all such components do not fail and different components fail independently of one another, then $P \left( A\prime \right) = P \left( SSSSS \right) = {.9}^{5} = .59$.
Thus $P\left( A \right) = 1 - .59 = .41$; so among a large number of such systems, roughly $41 \%$ will fail.

---
### PROP: no more than 1

In general, the foregoing proposition is useful when the event of interest can be expressed as "at least ...," since then the complement "less than ..." may be easier to work with (in some problems, "more than ..." is easier to deal with than "at most ...").
When you are having difficulty calculating $P\left( A\right)$ directly, think of determining $P\left( {A}'\right)$.

> [!property]
> For any event $A,P\left( A\right) \leq 1$.

---
#### Proof

This is because $1 = P\left( A\right) + P\left( {A}'\right) \geq P\left( A\right)$ since $P\left( {A}'\right) \geq 0$.

---
### PROP: addition rule

When events $A$ and $B$ are mutually exclusive, $P\left( {A \cup B}\right) = P\left( A\right) + P\left( B\right)$.
For events that are not mutually exclusive, adding $P\left( A\right)$ and $P\left( B\right)$ results in "double-counting" outcomes in the intersection.
The next result, the **addition rule** for a double union probability, shows how to correct for this.

> [!property]
> For any two events $A$ and $B$ ,
> $$
> P\left( {A \cup B}\right) = P\left( A\right) + P\left( B\right) - P\left( {A \cap B}\right)
> $$

---
#### Proof

Note first that $A \cup B$ can be decomposed into two disjoint events, $A$ and $B \cap {A}'$; the latter is the part of $B$ that lies outside $A$ (see Figure 2.4).
Furthermore, $B$ itself is the union of the two disjoint events $A \cap B$ and ${A}' \cap B$ , so $P\left( B\right) =$ $P\left( {A \cap B}\right) + P\left( {{A}' \cap B}\right)$ . Thus
$$
\begin{aligned}
P\left( A \cup B \right)
&= P\left( A \right) + P\left( B \cap A' \right) \\
&= P\left( A \right) + \left\lbrack P\left( B \right) - P\left( A \cap B \right)  \right\rbrack \\
&= P\left( A \right) + P\left( B \right) - P\left( A \cap B \right)
\end{aligned}
$$
---
![[01913607-292d-7d0a-a250-4b01870485a1_11_871099.jpg]]
Figure 2.4 Representing $A \cup B$ as a union of disjoint events

---
#### EX 2.14

In a certain residential suburb, 
- ${60}\\%$ of all households get Internet service from the local cable company, 
- ${80}\\%$ get television service from that company, 
- ${50}\\%$ get both services from that company.

If a household is randomly selected, 
- what is the probability that it gets at least one of these two services from the company 
- what is the probability that it gets exactly one of these services from the company?

---
With
-   $A$ = { gets Internet service }
-   $B$ = { gets TV service },
the given information implies that $P\left( A\right) = .6$, $P\left( B\right) = .8$, and $P\left( {A \cap B}\right) = .5$.
The foregoing proposition now yields
$$

\begin{align}
&P \left( \text{ subscribes to at least one of the two services } \right) \\
= &P \left( {A \cup B}\right) \\
= &P\left( A\right) + P\left( B\right) - P\left( {A \cap B}\right) \\
= &{.6} + {.8} - {.5} = {.9}
\end{align}

$$
---
The event that a household subscribes only to tv service can be written as ${A}' \cap B$. 
Now Figure 2.4 implies that
$$
\begin{aligned}
.9
&= P\left( A \cup B \right) = P\left( A \right) + P\left( A' \cap B \right) \\
&= .6 + P\left( A' \cap B \right)
\end{aligned}
$$
from which $P\left( A' \cap B \right) = .3$.

---
Similarly, $P\left( A \cap B' \right) = P \left( A \cup B \right) - P\left( B\right) = .1$.
This is all illustrated in Figure 2.5, from which we see that
$$

\begin{align}
P\left( \text{exactly one} \right)
&= P\left( {A \cap {B}'}\right) + P\left( {{A}' \cap B}\right) \\
&= {.1} + {.3} = {.4}
\end{align}

$$

![[01913607-292d-7d0a-a250-4b01870485a1_11_202687.jpg]]
Figure 2.5 Probabilities for Example 2.14

---
### PROP: addition rule for triple

The addition rule for a triple union probability is similar to the foregoing rule.

For any three events $A,B$ , and $C$ ,
$$
\begin{aligned}
P \left( A \cup B \cup C \right)
= &P\left( A\right) + P\left( B\right) + P\left( C\right) \\
&- P\left( A \cap B \right) - P\left( A \cap C \right) - P\left( B \cap C \right) \\
&+ P\left( A \cap B \cap C \right)
\end{aligned}
$$
---
This can be verified by examining the Venn diagram of $A \cup B \cup C$ , shown in Figure 2.6. When $P\left( A\right) ,P\left( B\right)$ , and $P\left( C\right)$ are added, the intersection probabilities $P\left( {A \cap B}\right)$ , $P\left( {A \cap C}\right)$ , and $P\left( {B \cap C}\right)$ are all counted twice. Each one must therefore be subtracted. But then $P\left( {A \cap B \cap C}\right)$ has been added in three times and subtracted out three times, so it must be added back. In general, the probability of a union of $k$ events is obtained by summing individual event probabilities, subtracting double intersection probabilities, adding triple intersection probabilities, subtracting quadruple intersection probabilities, and so on.

![[01913607-292d-7d0a-a250-4b01870485a1_12_715289.jpg]]
Figure 2.6 $A \cup B \cup C$

---
## 2.2.4 Determining Probabilities Systematically

Consider a sample space that is either finite or "countably infinite" (the latter means that outcomes can be listed in an infinite sequence, so there is a first outcome, a second outcome, a third outcome, and so on-for example, the battery testing scenario of Example 2.12). Let ${E}_{1},{E}_{2},{E}_{3},\ldots$ denote the corresponding simple events, each consisting of a single outcome. A sensible strategy for probability computation is to first determine each simple event probability, with the requirement that ${\sum P}\left( {E}_{i}\right) = 1$ . Then the probability of any compound event $A$ is computed by adding together the $P\left( {E}_{i}\right)$ 's for all $E$ 's in $A$ :
$$
P\left( A\right) = \mathop{\sum }\limits_{{\text{all }E;s\text{ in }A}}P\left( {E}_{i}\right)
$$
---
### EX 2.15

During off-peak hours a commuter train has five cars. Suppose a commuter is twice as likely to select the middle car (#3) as to select either adjacent car (#2 or #4), and is twice as likely to select either adjacent car as to select either end car (#1 or #5).
Let $p_{i}$ = $P \left( \right.$ car $i$ is selected $\left. \right)$ = $P \left( {E}_{i}\right)$.
Then we have ${p}_{3} = 2{p}_{2} = 2{p}_{4}$ and ${p}_{2} = 2{p}_{1} = 2{p}_{5} = {p}_{4}$.
This gives
$$
1 = \sum P\left( {E}_{i}\right) = {p}_{1} + 2{p}_{1} + 4{p}_{1} + 2{p}_{1} + {p}_{1} = {10}{p}_{1}
$$
implying ${p}_{1} = {p}_{5} = .1$, ${p}_{2} = {p}_{4} = .2$, ${p}_{3} = .4$. The probability that one of the three middle cars is selected (a compound event) is then ${p}_{2} + {p}_{3} + {p}_{4} = .8$.

---
## 2.2.5 Equally likely outcomes

In many experiments consisting of $N$ outcomes, it is reasonable to assign equal probabilities to all $N$ simple events. These include such obvious examples as tossing a fair coin or fair die once or twice (or any fixed number of times), or selecting one or several cards from a well-shuffled deck of 52 . With $p = P\left( {E}_{i}\right)$ for every $i$,
$$
1 = \sum_{i = 1}^{N} P\left( {E}_{i} \right)
= \sum_{i = 1}^{N} p = p \cdot N
$$
so $p = \frac{1}{N}$. That is, if there are $N$ equally likely outcomes, the probability for each is $1/N$ .

Now consider an event $A$ , with $N\left( A\right)$ denoting the number of outcomes contained in $A$ . Then
$$
P\left( A\right) = \sum_{{E}_{i} \text{ in } A} P\left( {E}_{i}\right)
= \sum_{{E}_{i} \text{ in } A} \frac{1}{N}
= \frac{N\left( A\right)}{N}
$$
Thus when outcomes are equally likely, computing probabilities reduces to counting: determine both the number of outcomes $N\left( A\right)$ in $A$ and the number of outcomes $N$ in $\mathcal{S}$ , and form their ratio.

---
### EX 2.16

You have 
- six unread mysteries
- six unread science fiction books on your bookshelf. 
And
- The first three of each type are hardcover
- the last three are paperback. 

Consider randomly selecting one of the six mysteries and then randomly selecting one of the six science fiction books to take on a post-finals vacation to Acapulco (after all, you need something to read on the beach). 

Number the mysteries $1,2,\ldots ,6$ , and do the same for the science fiction books. Then each outcome is a pair of numbers such as $\left( {4,1}\right)$, and there are $N = {36}$ possible outcomes (For a visual of this situation, refer to the table in Example 2.3 and delete the first row and column). 

With random selection as described, the 36 outcomes are equally likely. Nine of these outcomes are such that both selected books are paperbacks (those in the lower right-hand corner of the referenced table): $\left( {4,4}\right)$ , $\left( {4,5}\right) ,\ldots ,\left( {6,6}\right)$ . So the probability of the event $A$ that both selected books are paperbacks is
$$
P\left( A\right) = \frac{N\left( A\right) }{N} = \frac{9}{36} = {.25}
$$
---