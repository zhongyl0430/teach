## 5.2 Expected Values, Covariance, and Correlation

Any function $h\left( X\right)$ of a single rv $X$ is itself a random variable. However, we saw that to compute $E\left\lbrack {h\left( X\right) }\right\rbrack$ , it is not necessary to obtain the probability distribution of $h\left( X\right)$ . Instead, $E\left\lbrack {h\left( X\right) }\right\rbrack$ is computed as a weighted average of $h\left( x\right)$ values, where the weight function is the pmf $p\left( x\right)$ or $\operatorname{pdf}f\left( x\right)$ of $X$ . A similar result holds for a function $h\left( {X, Y}\right)$ of two jointly distributed random variables.

---

PROPOSITION

---

Let $X$ and $Y$ be jointly distributed rv’s with $\operatorname{pmf}p\left( {x, y}\right)$ or $\operatorname{pdf}f\left( {x, y}\right)$ according to whether the variables are discrete or continuous. Then the expected value of a function $h\left( {X, Y}\right)$ , denoted by $E\left\lbrack {h\left( {X, Y}\right) }\right\rbrack$ or ${\mu }_{h\left( {X, Y}\right) }$ , is given by

$$
E\left\lbrack {h\left( {X, Y}\right) }\right\rbrack = \left\{ \begin{array}{ll} \mathop{\sum }\limits_{x}\mathop{\sum }\limits_{y}h\left( {x, y}\right) \cdot p\left( {x, y}\right) & \text{ if }X\text{ and }Y\text{ are discrete } \\ {\int }_{-\infty }^{\infty }{\int }_{-\infty }^{\infty }h\left( {x, y}\right) \cdot f\left( {x, y}\right) {dxdy} & \text{ if }X\text{ and }Y\text{ are continuous } \end{array}\right.
$$

EXAMPLE 5.13 Five friends have purchased tickets to a certain concert. If the tickets are for seats 1-5 in a particular row and the tickets are randomly distributed among the five, what is the expected number of seats separating any particular two of the five? Let $X$ and $Y$ denote the seat numbers of the first and second individuals, respectively. Possible $\left( {X, Y}\right)$ pairs are $\{ \left( {1,2}\right) ,\left( {1,3}\right) ,\ldots ,\left( {5,4}\right) \}$ , and the joint pmf of $\left( {X, Y}\right)$ is

$$
p\left( {x, y}\right) = \left\{ \begin{array}{ll} \frac{1}{20} & x = 1,\ldots ,5;y = 1,\ldots ,5;x \neq y \\ 0 & \text{ otherwise } \end{array}\right.
$$

The number of seats separating the two individuals is $h\left( {X, Y}\right) = \left| {X - Y}\right| - 1$ . The accompanying table gives $h\left( {x, y}\right)$ for each possible $\left( {x, y}\right)$ pair.

<table><thead><tr><th rowspan="2">$h\left( {x, y}\right)$</th><th colspan="5">$x$</th></tr><tr><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tr><td rowspan="6">1 2 $y$ 3 4 5</td><td>$-$</td><td>0</td><td>1</td><td>2</td><td>3</td></tr><tr><td>0</td><td>$-$</td><td>0</td><td>1</td><td>2</td></tr><tr><td>1</td><td>0</td><td>$-$</td><td>0</td><td>1</td></tr><tr><td>2</td><td>1</td><td>0</td><td>$-$</td><td>0</td></tr><tr><td>3</td><td>2</td><td>1</td><td>0</td><td>$-$</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr></table>

Thus

$$
E\left\lbrack {h\left( {X, Y}\right) }\right\rbrack = \mathop{\sum }\limits_{\left( x, y\right) }h\left( {x, y}\right) \cdot p\left( {x, y}\right) = \mathop{\sum }\limits_{\substack{{x = 1} \\ {x \neq y} }}^{5}\mathop{\sum }\limits_{\substack{{y = 1} \\ {x \neq y} }}^{5}\left( {\left| {x - y}\right| - 1}\right) \cdot \frac{1}{20} = 1
$$

EXAMPLE 5.14 In Example 5.5, the joint pdf of the amount $X$ of almonds and amount $Y$ of cashews in a 1-lb can of nuts was

$$
f\left( {x, y}\right) = \left\{ \begin{matrix} {24xy} & 0 \leq x \leq 1,0 \leq y \leq 1, x + y \leq 1 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

If 1 lb of almonds costs the company $\$ {1.50},1\mathrm{{lb}}$ of cashews costs $\$ {2.25}$ , and $1\mathrm{{lb}}$ of peanuts costs $\$ {.75}$ , then the total cost of the contents of a can is

$$
h\left( {X, Y}\right) = \left( {1.5}\right) X + \left( {2.25}\right) Y + \left( {.75}\right) \left( {1 - X - Y}\right) = {.75} + {.75X} + {1.5Y}
$$

(since $1 - X - Y$ of the weight consists of peanuts). The expected total cost is

$$
E\left\lbrack {h\left( {X, Y}\right) }\right\rbrack = {\int }_{-\infty }^{\infty }{\int }_{-\infty }^{\infty }h\left( {x, y}\right) \cdot f\left( {x, y}\right) {dxdy}
$$

$$
= {\int }_{0}^{1}{\int }_{0}^{1 - x}\left( {{.75} + {.75x} + {1.5y}}\right) \cdot {24xydydx} = \$ {1.65}
$$

The method of computing the expected value of a function $h\left( {{X}_{1},\ldots ,{X}_{n}}\right)$ of $n$ random variables is similar to that for two random variables. If the ${X}_{i}$ ’s are discrete, $E\left\lbrack {h\left( {{X}_{1},\ldots ,{X}_{n}}\right) }\right\rbrack$ is an $n$ -dimensional sum; if the ${X}_{i}$ ’s are continuous, it is an $n$ - dimensional integral.

### 5.2.1 Covariance

When two random variables $X$ and $Y$ are not independent, it is frequently of interest to assess how strongly they are related to one another.

DEFINITION

The covariance between two rv’s $X$ and $Y$ is

$$
\operatorname{Cov}\left( {X, Y}\right) = E\left\lbrack {\left( {X - {\mu }_{X}}\right) \left( {Y - {\mu }_{Y}}\right) }\right\rbrack
$$

$$
= \left\{ \begin{array}{ll} \mathop{\sum }\limits_{x}\mathop{\sum }\limits_{y}\left( {x - {\mu }_{X}}\right) \left( {y - {\mu }_{Y}}\right) p\left( {x, y}\right) & X, Y\text{ discrete } \\ {\int }_{-\infty }^{\infty }{\int }_{-\infty }^{\infty }\left( {x - {\mu }_{X}}\right) \left( {y - {\mu }_{Y}}\right) f\left( {x, y}\right) {dxdy} & X, Y\text{ continuous } \end{array}\right.
$$

That is, since $X - {\mu }_{X}$ and $Y - {\mu }_{Y}$ are the deviations of the two variables from their respective mean values, the covariance is the expected product of deviations. Note that $\operatorname{Cov}\left( {X, X}\right) = E\left\lbrack {\left( X - {\mu }_{X}\right) }^{2}\right\rbrack = V\left( X\right)$ .

The rationale for the definition is as follows. Suppose $X$ and $Y$ have a strong positive relationship to one another, by which we mean that large values of $X$ tend to occur with large values of $Y$ and small values of $X$ with small values of $Y$ . Then most of the probability mass or density will be associated with $\left( {x - {\mu }_{X}}\right)$ and $\left( {y - {\mu }_{Y}}\right)$ , either both positive (both $X$ and $Y$ above their respective means) or both negative, so the product $\left( {x - {\mu }_{X}}\right) \left( {y - {\mu }_{Y}}\right)$ will tend to be positive. Thus for a strong positive relationship, $\operatorname{Cov}\left( {X, Y}\right)$ should be quite positive. For a strong negative relationship, the signs of $\left( {x - {\mu }_{X}}\right)$ and $\left( {y - {\mu }_{Y}}\right)$ will tend to be opposite, yielding a negative product. Thus for a strong negative relationship, $\operatorname{Cov}\left( {X, Y}\right)$ should be quite negative. If $X$ and $Y$ are not strongly related, positive and negative products will tend to cancel one another, yielding a covariance near 0 . Figure 5.4 illustrates the different possibilities. The covariance depends on both the set of possible pairs and the probabilities. In Figure 5.4, the probabilities could be changed without altering the set of possible pairs, and this could drastically change the value of $\operatorname{Cov}\left( {X, Y}\right)$ .

![0192609f-6f5c-74c9-8588-c1ef28b2184d_17_639_187_1109_348_0.jpg](images/0192609f-6f5c-74c9-8588-c1ef28b2184d_17_639_187_1109_348_0.jpg)

Figure ${5.4p}\left( {x, y}\right) = 1/{10}$ for each of ten pairs corresponding to indicated points:

(a) positive covariance; (b) negative covariance; (c) covariance near zero

EXAMPLE 5.15 The joint and marginal pmf’s for $X =$ automobile policy deductible amount and $Y =$ homeowner policy deductible amount in Example 5.1 were

<table><tr><td colspan="12">$y$</td></tr><tr><td>$p\left( {x, y}\right)$</td><td>500</td><td>1000</td><td>5000</td><td>$x$</td><td>100</td><td>500</td><td>1000</td><td>$y$</td><td>500</td><td>1000</td><td>5000</td></tr><tr><td>100</td><td>.30</td><td>.05</td><td>0</td><td>${p}_{X}\left( x\right)$</td><td>.35</td><td>.40</td><td>.25</td><td>${p}_{Y}\left( y\right)$</td><td>.55</td><td>.35</td><td>.10</td></tr><tr><td>$x$500</td><td>.15</td><td>.20</td><td>.05</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1000</td><td>.10</td><td>.10</td><td>.05</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>

from which ${\mu }_{X} = {\sum x}{p}_{X}\left( x\right) = {485}$ and ${\mu }_{Y} = {1125}$ . Therefore,

$$
\operatorname{Cov}\left( {X, Y}\right) = \mathop{\sum }\limits_{\left( x, y\right) }\left( {x - {485}}\right) \left( {y - {1125}}\right) p\left( {x, y}\right)
$$

$$
= \left( {{100} - {485}}\right) \left( {{500} - {1125}}\right) \left( {.30}\right) + \ldots
$$

$$
+ \left( {{1000} - {485}}\right) \left( {{5000} - {1125}}\right) \left( {.05}\right)
$$

$$
= {136},{875}
$$

The following shortcut formula for $\operatorname{Cov}\left( {X, Y}\right)$ simplifies the computations.

PROPOSITION

$$
\operatorname{Cov}\left( {X, Y}\right) = E\left( {XY}\right) - {\mu }_{X} \cdot {\mu }_{Y}
$$

According to this formula, no intermediate subtractions are necessary; only at the end of the computation is ${\mu }_{X} \cdot {\mu }_{Y}$ subtracted from $E\left( {XY}\right)$ . The proof involves expanding $\left( {X - {\mu }_{X}}\right) \left( {Y - {\mu }_{Y}}\right)$ and then carrying the summation or integration through to each individual term.

---

EXAMPLE 5.16

(Example 5.5 were

continued)

---

The joint and marginal pdf’s of $X =$ amount of almonds and $Y =$ amount of cashews

$$
f\left( {x, y}\right) = \left\{ \begin{matrix} {24xy} & 0 \leq x \leq 1,0 \leq y \leq 1, x + y \leq 1 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

$$
{f}_{X}\left( x\right) = \left\{ \begin{matrix} {12x}{\left( 1 - x\right) }^{2} & 0 \leq x \leq 1 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

with ${f}_{Y}\left( y\right)$ obtained by replacing $x$ by $y$ in ${f}_{X}\left( x\right)$ . It is easily verified that ${\mu }_{X} = {\mu }_{Y} = \frac{2}{5}$ , and

$$
E\left( {XY}\right) = {\int }_{-\infty }^{\infty }{\int }_{-\infty }^{\infty }{xyf}\left( {x, y}\right) {dxdy} = {\int }_{0}^{1}{\int }_{0}^{1 - x}{xy} \cdot {24xydydx}
$$

$$
= 8{\int }_{0}^{1}{x}^{2}{\left( 1 - x\right) }^{3}{dx} = \frac{2}{15}
$$

Thus $\operatorname{Cov}\left( {X, Y}\right) = 2/{15} - \left( {2/5}\right) \left( {2/5}\right) = 2/{15} - 4/{25} = - 2/{75}$ . A negative covariance is reasonable here because more almonds in the can implies fewer cashews.

It might appear that the relationship in the insurance example is quite strong since $\operatorname{Cov}\left( {X, Y}\right) = {136},{875}$ , whereas $\operatorname{Cov}\left( {X, Y}\right) = - 2/{75}$ in the nut example would seem to imply quite a weak relationship. Unfortunately, the covariance has a serious defect that makes it impossible to interpret a computed value. In the insurance example, suppose we had expressed the deductible amount in cents rather than in dollars. Then ${100X}$ would replace $X,{100Y}$ would replace $Y$ , and the resulting covariance would be $\operatorname{Cov}\left( {{100X},{100Y}}\right) = \left( {100}\right) \left( {100}\right) \operatorname{Cov}\left( {X, Y}\right) = 1,{368},{750},{000}$ . If, on the other hand, the deductible amount had been expressed in hundreds of dollars, the computed covariance would have been (.01)(.01)(136,875) $= {13.6875}$ . The defect of covariance is that its computed value depends critically on the units of measurement. Ideally, the choice of units should have no effect on a measure of strength of relationship. This is achieved by scaling the covariance.

### 5.2.2 Correlation
DEFINITION

The correlation coefficient of $X$ and $Y$ , denoted by $\operatorname{Corr}\left( {X, Y}\right) ,{\rho }_{X, Y}$ , or just $\rho$ , is defined by

$$
{\rho }_{X, Y} = \frac{\operatorname{Cov}\left( {X, Y}\right) }{{\sigma }_{X} \cdot {\sigma }_{Y}}
$$

EXAMPLE 5.17 It is easily verified that in the insurance scenario of Example 5.15, $E\left( {X}^{2}\right) = {353},{500}$ , ${\sigma }_{X}^{2} = {353},{500} - {\left( {485}\right) }^{2} = {118},{275},\;{\sigma }_{X} = {343.911},\;E\left( {Y}^{2}\right) = 2,{987},{500},\;{\sigma }_{Y}^{2} =$ 1,721,875, and ${\sigma }_{Y} = {1312.202}$ . This gives

$$
\rho = \frac{136.875}{\left( {{343},{911}}\right) \left( {1312.202}\right) } = {.303}
$$

The following proposition shows that $\rho$ remedies the defect of $\operatorname{Cov}\left( {X, Y}\right)$ and also suggests how to recognize the existence of a strong (linear) relationship.

PROPOSITION 1. If $a$ and $c$ are either both positive or both negative,

$$
\operatorname{Corr}\left( {{aX} + b,{cY} + d}\right) = \operatorname{Corr}\left( {X, Y}\right)
$$

2. For any two rv’s $X$ and $Y, - 1 \leq \rho \leq 1$ . The two variables are said to be uncorrelated when $\rho = 0$ .

Statement 1 says precisely that the correlation coefficient is not affected by a linear change in the units of measurement (if, say, $X =$ temperature in ${}^{ \circ }\mathrm{C}$ , then ${9X}/5 + {32} =$ temperature in ${}^{ \circ }\mathrm{F}$ ). According to Statement 2, the strongest possible positive relationship is evidenced by $\rho = + 1$ , the strongest possible negative relationship corresponds to $\rho = - 1$ , and $\rho = 0$ indicates the absence of a relationship. The proof of the first statement is sketched in Exercise 35, and that of the second appears in Supplementary Exercise 87 at the end of the chapter. For descriptive purposes, the relationship will be described as strong if $\left| \rho \right| \geq {.8}$ , moderate if ${.5} < \left| \rho \right| < {.8}$ , and weak if $\left| \rho \right| \leq {.5}$ .

If we think of $p\left( {x, y}\right)$ or $f\left( {x, y}\right)$ as prescribing a mathematical model for how the two numerical variables $X$ and $Y$ are distributed in some population (height and weight, verbal SAT score and quantitative SAT score, etc.), then $\rho$ is a population characteristic or parameter that measures how strongly $X$ and $Y$ are related in the population. In Chapter 12, we will consider taking a sample of pairs $\left( {{x}_{1},{y}_{1}}\right) ,\ldots$ , $\left( {{x}_{n},{y}_{n}}\right)$ from the population. The sample correlation coefficient $r$ will then be defined and used to make inferences about $\rho$ .

The correlation coefficient $\rho$ is actually not a completely general measure of the strength of a relationship.

---

PROPOSITION

---

1. If $X$ and $Y$ are independent, then $\rho = 0$ , but $\rho = 0$ does not imply independence.

2. $\rho = 1$ or -1 iff $Y = {aX} + b$ for some numbers $a$ and $b$ with $a \neq 0$ .

This proposition says that $\rho$ is a measure of the degree of linear relationship between $X$ and $Y$ , and only when the two variables are perfectly related in a linear manner will $\rho$ be as positive or negative as it can be. However, if $\left| \rho \right| < < 1$ , there may still be a strong relationship between the two variables, just one that is not linear. And even if $\left| \rho \right|$ is close to 1, it may be that the relationship is really nonlinear but can be well approximated by a straight line.

EXAMPLE 5.18 Let $X$ and $Y$ be discrete rv’s with joint pmf

$$
p\left( {x, y}\right) = \left\{ \begin{array}{ll} {.25} & \left( {x, y}\right) = \left( {-4,1}\right) ,\left( {4, - 1}\right) ,\left( {2,2}\right) ,\left( {-2, - 2}\right) \\ 0 & \text{ otherwise } \end{array}\right.
$$

The points that receive positive probability mass are identified on the $\left( {x, y}\right)$ coordinate system in Figure 5.5. It is evident from the figure that the value of $X$ is completely determined by the value of $Y$ and vice versa, so the two variables are completely dependent. However, by symmetry ${\mu }_{X} = {\mu }_{Y} = 0$ and $E\left( {XY}\right) = \left( {-4}\right) \left( {.25}\right) +$ $\left( {-4}\right) \left( {.25}\right) + \left( 4\right) \left( {.25}\right) + \left( 4\right) \left( {.25}\right) = 0$ . The covariance is then $\operatorname{Cov}\left( {X, Y}\right) =$ $E\left( {XY}\right) - {\mu }_{X} \cdot {\mu }_{Y} = 0$ and thus ${\rho }_{X, Y} = 0$ . Although there is perfect dependence, there is also complete absence of any linear relationship! A value of $\rho$ near 1 does not necessarily imply that increasing the value of $X$ causes $Y$ to increase. It implies only that large $X$ values are associated with large $Y$ values. For example, in the population of children, vocabulary size and number of cavities are quite positively correlated, but it is certainly not true that cavities cause vocabulary to grow. Instead, the values of both these variables tend to increase as the value of age, a third variable, increases. For children of a fixed age, there is probably a low correlation between number of cavities and vocabulary size. In summary, association (a high correlation) is not the same as causation.

![0192609f-6f5c-74c9-8588-c1ef28b2184d_19_933_1877_533_290_0.jpg](images/0192609f-6f5c-74c9-8588-c1ef28b2184d_19_933_1877_533_290_0.jpg)

Figure 5.5 The population of pairs for Example 5.18

### 5.2.3 The Bivariate Normal Distribution

Just as the most useful univariate distribution in statistical practice is the normal distribution, the most useful joint distribution for two rv’s $X$ and $Y$ is the bivariate normal distribution. The pdf is somewhat complicated:

$$
f\left( {x, y}\right) = \frac{1}{{2\pi }{\sigma }_{1}{\sigma }_{2}\sqrt{1 - {\rho }^{2}}}\exp
$$

$$
\left( {-\frac{1}{2\left( {1 - {\rho }^{2}}\right) }\left\lbrack {{\left( \frac{x - {\mu }_{1}}{{\sigma }_{1}}\right) }^{2} - {2\rho }\left( \frac{x - {\mu }_{1}}{{\sigma }_{1}}\right) \left( \frac{y - {\mu }_{2}}{{\sigma }_{2}}\right) + {\left( \frac{y - {\mu }_{2}}{{\sigma }_{2}}\right) }^{2}}\right\rbrack }\right)
$$

A graph of this pdf, the density surface, appears in Figure 5.6. It follows (after some tricky integration) that the marginal distribution of $X$ is normal with mean value ${\mu }_{1}$ and standard deviation ${\sigma }_{1}$ , and similarly the marginal distribution of $Y$ is normal with mean ${\mu }_{2}$ and standard deviation ${\sigma }_{2}$ . The fifth parameter of the distribution is $\rho$ , which can be shown to be the correlation coefficient between $X$ and $Y$ .

![0192609f-6f5c-74c9-8588-c1ef28b2184d_20_632_1202_810_391_0.jpg](images/0192609f-6f5c-74c9-8588-c1ef28b2184d_20_632_1202_810_391_0.jpg)

Figure 5.6 A graph of the bivariate normal pdf

It is not at all straightforward to integrate the bivariate normal pdf in order to calculate probabilities. Instead, selected software packages employ numerical integration techniques for this purpose.

EXAMPLE 5.19 Many students applying for college take the SAT, which for a few years consisted of three components: Critical Reading, Mathematics, and Writing. While some colleges used all three components to determine admission, many only looked at the first two (reading and math). Let $X$ and $Y$ denote the Critical Reading and Mathematics scores, respectively, for a randomly selected student. According to the College Board website, the population of students taking the exam in Fall 2012 had the following characteristics: ${\mu }_{1} = {496},{\sigma }_{1} = {114},{\mu }_{2} = {514},{\sigma }_{2} = {117}$ .

Suppose that $X$ and $Y$ have (approximately, since both variables are discrete) a bivariate normal distribution with correlation coefficient $\rho = {.25}$ . The Matlab software package gives $P\left( {X \leq {650}, Y \leq {650}}\right) = P\left( \text{both scores are at most 650}\right) = {.8097}$ .

It can also be shown that the conditional distribution of $Y$ given that $X = x$ is normal. This can be seen geometrically by slicing the density surface with a plane perpendicular to the $\left( {x, y}\right)$ passing through the value $x$ on that axis; the result is a normal curve sketched out on the slicing plane. The conditional mean value is ${\mu }_{Y \cdot x} = \left( {{\mu }_{2} - \rho {\mu }_{1}{\sigma }_{2}/{\sigma }_{1}}\right) + \rho {\sigma }_{2}x/{\sigma }_{1}$ , a linear function of $x$ , and the conditional variance is ${\sigma }_{Y \cdot x}^{2} = \left( {1 - {\rho }^{2}}\right) {\sigma }_{2}^{2}$ . The closer the correlation coefficient is to 1 or -1, the less variability there is in the conditional distribution. Analogous results hold for the conditional distribution of $X$ given that $Y = y$ .

The bivariate normal distribution can be generalized to the multivariate normal distribution. Its density function is quite complicated, and the only way to write it compactly is to employ matrix notation. If a collection of variables has this distribution, then the marginal distribution of any single variable is normal, the conditional distribution of any single variable given values of the other variables is normal, the joint marginal distribution of any pair of variables is bivariate normal, and the joint marginal distribution of any subset of three or more of the variables is again multivariate normal.

### EXERCISES Section 5.2 (22-36)

22. An instructor has given a short quiz consisting of two parts. For a randomly selected student, let $X =$ the number of points earned on the first part and $Y =$ the number of points earned on the second part. Suppose that the joint pmf of $X$ and $Y$ is given in the accompanying table.

<table><thead><tr><th rowspan="2">$p\left( {x, y}\right)$</th><th colspan="4">$y$</th></tr><tr><th>0</th><th>5</th><th>10</th><th>15</th></tr></thead><tr><td rowspan="3">0 $x$ 5 10</td><td>.02</td><td>.06</td><td>.02</td><td>.10</td></tr><tr><td>.04</td><td>.15</td><td>.20</td><td>.10</td></tr><tr><td>.01</td><td>.15</td><td>.14</td><td>.01</td></tr></table>

a. If the score recorded in the grade book is the total number of points earned on the two parts, what is the expected recorded score $E\left( {X + Y}\right)$ ?

b. If the maximum of the two scores is recorded, what is the expected recorded score?

23. The difference between the number of customers in line at the express checkout and the number in line at the super-express checkout in Exercise 3 is ${X}_{1} - {X}_{2}$ . Calculate the expected difference.

24. Six individuals, including A and B, take seats around a circular table in a completely random fashion. Suppose the seats are numbered $1,\ldots ,6$ . Let $X = \mathrm{A}$ ’s seat number and $Y =$ B’s seat number. If A sends a written message around the table to $\mathrm{B}$ in the direction in which they are closest, how many individuals (including A and B) would you expect to handle the message?

25. A surveyor wishes to lay out a square region with each side having length $L$ . However, because of a measurement error, he instead lays out a rectangle in which the north-south sides both have length $X$ and the east-west sides both have length $Y$ . Suppose that $X$ and $Y$ are independent and that each is uniformly distributed on the interval $\left\lbrack {L - A, L + A}\right\rbrack$ (where $0 < A < L)$ . What is the expected area of the resulting rectangle?

26. Consider a small ferry that can accommodate cars and buses. The toll for cars is $\$ 3$ , and the toll for buses is $\$ {10}$ . Let $X$ and $Y$ denote the number of cars and buses, respectively, carried on a single trip. Suppose the joint distribution of $X$ and $Y$ is as given in the table of Exercise 7. Compute the expected revenue from a single trip.

27. Annie and Alvie have agreed to meet for lunch between noon (0:00 P.M.) and 1:00 P.M. Denote Annie's arrival time by $X$ , Alvie’s by $Y$ , and suppose $X$ and $Y$ are independent with pdf's

$$
{f}_{X}\left( x\right) = \left\{ \begin{array}{ll} 3{x}^{2} & 0 \leq x \leq 1 \\ 0 & \text{ otherwise } \end{array}\right.
$$

$$
{f}_{Y}\left( y\right) = \left\{ \begin{matrix} {2y} & 0 \leq y \leq 1 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

What is the expected amount of time that the one who arrives first must wait for the other person? [Hint: $h\left( {X, Y}\right) = \left| {X - Y}\right|$ .]

28. Show that if $X$ and $Y$ are independent rv’s, then $E\left( {XY}\right) = E\left( X\right) \cdot E\left( Y\right)$ . Then apply this in Exercise 25. [Hint: Consider the continuous case with $f\left( {x, y}\right) =$ $\left. {{f}_{X}\left( x\right) \cdot {f}_{Y}\left( y\right) \text{.}}\right\rbrack$

29. Compute the correlation coefficient $\rho$ for $X$ and $Y$ of Example 5.16 (the covariance has already been computed).

30. a. Compute the covariance for $X$ and $Y$ in Exercise 22.

b. Compute $\rho$ for $X$ and $Y$ in the same exercise.

31. a. Compute the covariance between $X$ and $Y$ in Exercise 9.

b. Compute the correlation coefficient $\rho$ for this $X$ and $Y$ .

32. Reconsider the minicomputer component lifetimes $X$ and $Y$ as described in Exercise 12. Determine $E\left( {XY}\right)$ . What can be said about $\operatorname{Cov}\left( {X, Y}\right)$ and $\rho$ ?

33. Use the result of Exercise 28 to show that when $X$ and $Y$ are independent, $\operatorname{Cov}\left( {X, Y}\right) = \operatorname{Corr}\left( {X, Y}\right) = 0$ .

34. a. Recalling the definition of ${\sigma }^{2}$ for a single rv $X$ , write a formula that would be appropriate for computing the variance of a function $h\left( {X, Y}\right)$ of two random variables. [Hint: Remember that variance is just a special expected value.]

b. Use this formula to compute the variance of the recorded score $h\left( {X, Y}\right) \left\lbrack { = \max \left( {X, Y}\right) }\right\rbrack$ in part (b) of Exercise 22.

35. a. Use the rules of expected value to show that $\operatorname{Cov}\left( {{aX} + b,{cY} + d}\right) = {ac}\operatorname{Cov}\left( {X, Y}\right)$ .

b. Use part (a) along with the rules of variance and standard deviation to show that $\operatorname{Corr}({aX} + b$ , ${cY} + d) = \operatorname{Corr}\left( {X, Y}\right)$ when $a$ and $c$ have the same sign.

c. What happens if $a$ and $c$ have opposite signs?

36. Show that if $Y = {aX} + b\left( {a \neq 0}\right)$ , then $\operatorname{Corr}\left( {X, Y}\right) = + 1$ or -1 . Under what conditions will $\rho = + 1$ ?