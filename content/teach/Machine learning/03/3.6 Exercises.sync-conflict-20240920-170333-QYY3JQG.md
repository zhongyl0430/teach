# 3.6 Exercises

3.1 Growth function of intervals in $\mathbb{R}$ . Let $\mathcal{H}$ be
the set of intervals in $\mathbb{R}$ . The VC-dimension of $\mathcal{H}$
is 2. Compute its shattering coefficient
${\Pi }_{\mathcal{H}}\left( m\right) ,m \geq 0$ . Compare your result
with the general bound for growth functions.

3.2 Growth function and Rademacher complexity of thresholds in
$\mathbb{R}$ . Let $\mathcal{H}$ be the family of threshold functions
over the real line:
$\mathcal{H} = \left\{ {x \mapsto {1}_{x \leq \theta } : \theta \in \mathbb{R}}\right\} \cup \{ x \mapsto$
$\left. {{1}_{x \geq \theta } : \theta \in \mathbb{R}}\right\}$ . Give
an upper bound on the growth function
${\Pi }_{m}\left( \mathcal{H}\right)$ . Use that to derive an upper
bound on ${\mathfrak{R}}_{m}\left( \mathcal{H}\right)$ .

3.3 Growth function of linear combinations. A linearly separable
labeling of a set $\mathcal{X}$ of vectors in ${\mathbb{R}}^{d}$ is a
classification of $\mathcal{X}$ into two sets ${\mathcal{X}}^{ + }$ and
${\mathcal{X}}^{ - }$ with ${\mathcal{X}}^{ + } =$
$\{ \mathbf{x} \in X : \mathbf{w} \cdot \mathbf{x} > 0\}$ and
${X}^{ - } = \{ \mathbf{x} \in X : \mathbf{w} \cdot \mathbf{x} < 0\}$
for some $\mathbf{w} \in {\mathbb{R}}^{d}$ .

Let $X = \left\{ {{\mathbf{x}}_{1},\ldots ,{\mathbf{x}}_{m}}\right\}$ be
a subset of ${\mathbb{R}}^{d}$ .

\(a\) Let $\left\{ {{\mathcal{X}}^{ + },{\mathcal{X}}^{ - }}\right\}$ be
a dichotomy of $\mathcal{X}$ and let
${\mathbf{x}}_{m + 1} \in {\mathbb{R}}^{d}$ . Show that
$\left\{ {{\mathcal{X}}^{ + } \cup }\right.$
$\left\{ {\mathbf{x}}_{m + 1}\right\} ,{\mathcal{X}}^{ - }\}$ and
$\left\{ {{\mathcal{X}}^{ + },{\mathcal{X}}^{ - } \cup \left\{ {\mathbf{x}}_{m + 1}\right\} }\right\}$
are linearly separable by a hyperplane going through the origin if and
only if $\left\{ {{\mathcal{X}}^{ + },{\mathcal{X}}^{ - }}\right\}$ is
linearly separable by a hyperplane going through the origin and
${\mathbf{x}}_{m + 1}$ .

\(b\) Let
$\mathcal{X} = \left\{ {{\mathbf{x}}_{1},\ldots ,{\mathbf{x}}_{m}}\right\}$
be a subset of ${\mathbb{R}}^{d}$ such that any $k$ -element subset of
$x$ with $k \leq d$ is linearly independent. Then, show that the number
of linearly separable labelings of $X$ is
$C\left( {m,d}\right) = 2\mathop{\sum }\limits_{{k = 0}}^{{d - 1}}\left( \begin{matrix} m - 1 \\ k \end{matrix}\right)$
. (Hint: prove by induction that
$C\left( {m + 1,d}\right) = C\left( {m,d}\right) + C\left( {m,d - 1}\right)$
.

\(c\) Let ${f}_{1},\ldots ,{f}_{p}$ be $p$ functions mapping
${\mathbb{R}}^{d}$ to $\mathbb{R}$ . Define $\mathcal{F}$ as the family
of classifiers based on linear combinations of these functions:

$$\mathcal{F} = \left\{ {x \mapsto \operatorname{sgn}\left( {\mathop{\sum }\limits_{{k = 1}}^{p}{a}_{k}{f}_{k}\left( x\right) }\right) : {a}_{1},\ldots ,{a}_{p} \in \mathbb{R}}\right\} .$$

Define $\Psi$ by
$\Psi \left( x\right) = \left( {{f}_{1}\left( x\right) ,\ldots ,{f}_{p}\left( x\right) }\right)$
. Assume that there exists ${x}_{1},\ldots ,{x}_{m} \in$
${\mathbb{R}}^{d}$ such that every $p$ -subset of
$\left\{ {\Psi \left( {x}_{1}\right) ,\ldots ,\Psi \left( {x}_{m}\right) }\right\}$
is linearly independent. Then, show that

$${\Pi }_{\mathcal{F}}\left( m\right) = 2\mathop{\sum }\limits_{{i = 0}}^{{p - 1}}\left( \begin{matrix} m - 1 \\ i \end{matrix}\right) .$$

3.4 Lower bound on growth function. Prove that Sauer's lemma (theorem
3.17) is tight, i.e., for any set $x$ of $m > d$ elements, show that
there exists a hypothesis class $\mathcal{H}$ of VC-dimension $d$ such
that
${\Pi }_{\mathcal{H}}\left( m\right) = \mathop{\sum }\limits_{{i = 0}}^{d}\left( \begin{matrix} m \\ i \end{matrix}\right)$
.

3.5 Finer Rademacher upper bound. Show that a finer upper bound on the
Rademacher complexity of the family $\mathcal{G}$ can be given in terms
of
${\mathbb{E}}_{S}\left\lbrack {\Pi \left( {\mathcal{G},S}\right) }\right\rbrack$
, where $\Pi \left( {\mathcal{G},S}\right)$ is the number of ways to
label the points in sample $S$ .

3.6 Singleton hypothesis class. Consider the trivial hypothesis set
$\mathcal{H} = \left\{ {h}_{0}\right\}$ .

\(a\) Show that ${\mathfrak{R}}_{m}\left( \mathcal{H}\right) = 0$ for
any $m > 0$ .

\(b\) Use a similar construction to show that Massart's lemma (theorem
3.7) is tight.

3.7 Two function hypothesis class. Let $\mathcal{H}$ be a hypothesis set
reduced to two functions:
$\mathcal{H} = \left\{ {{h}_{-1},{h}_{+1}}\right\}$ and let
$S = \left( {{x}_{1},\ldots ,{x}_{m}}\right) \subseteq \mathfrak{X}$ be
a sample of size $m$ .

\(a\) Assume that ${h}_{-1}$ is the constant function taking value -1
and ${h}_{+1}$ the constant function taking the value +1 . What is the
VC-dimension $d$ of $\mathcal{H}$ ? Upper bound the empirical Rademacher
complexity ${\widehat{\Re }}_{S}\left( \mathcal{H}\right)$ (Hint:
express ${\widehat{\mathfrak{R}}}_{S}\left( \mathcal{H}\right)$ in terms
of the absolute value of a sum of Rademacher variables and apply
Jensen's inequality) and compare your bound with $\sqrt{d/m}$ .

\(b\) Assume that ${h}_{-1}$ is the constant function taking value -1
and ${h}_{+1}$ the function taking value -1 everywhere except at
${x}_{1}$ where it takes the value +1. What is the VC-dimension $d$ of
$\mathcal{H}$ ? Compute the empirical Rademacher complexity
${\widehat{\mathfrak{R}}}_{S}\left( \mathcal{H}\right)$ .

3.8 Rademacher identities. Fix $m \geq 1$ . Prove the following
identities for any $\alpha \in \mathbb{R}$ and any two hypothesis sets
$\mathcal{H}$ and ${\mathcal{H}}^{\prime }$ of functions mapping from
$\mathcal{X}$ to $\mathbb{R}$ :

\(a\)
${\Re }_{m}\left( {\alpha \mathcal{H}}\right) = \left| \alpha \right| {\Re }_{m}\left( \mathcal{H}\right)$
.

\(b\)
${\Re }_{m}\left( {\mathcal{H} + {\mathcal{H}}^{\prime }}\right) = {\Re }_{m}\left( \mathcal{H}\right) + {\Re }_{m}\left( {\mathcal{H}}^{\prime }\right)$
.

\(c\)
${\mathfrak{R}}_{m}\left( \left\{ {\max \left( {h,{h}^{\prime }}\right) : h \in \mathcal{H},{h}^{\prime } \in {\mathcal{H}}^{\prime }}\right\} \right) \leq {\mathfrak{R}}_{m}\left( \mathcal{H}\right) + {\mathfrak{R}}_{m}\left( {\mathcal{H}}^{\prime }\right)$
,

where $\max \left( {h,{h}^{\prime }}\right)$ denotes the function
$x \mapsto \mathop{\max }\limits_{{x \in X}}\left( {h\left( x\right) ,{h}^{\prime }\left( x\right) }\right)$
(Hint: you could use the identity
$\max \left( {a,b}\right) = \frac{1}{2}\left\lbrack {a + b + \left| {a - b}\right| }\right\rbrack$
valid for all $a,b \in \mathbb{R}$ and Talagrand's contraction lemma
(see lemma 5.7)).

3.9 Rademacher complexity of intersection of concepts. Let
${\mathcal{H}}_{1}$ and ${\mathcal{H}}_{2}$ be two families of functions
mapping $X$ to $\{ 0,1\}$ and let
$\mathcal{H} = \left\{ {{h}_{1}{h}_{2} : {h}_{1} \in {\mathcal{H}}_{1},{h}_{2} \in }\right.$
$\left. {\mathcal{H}}_{2}\right\}$ . Show that the empirical Rademacher
complexity of $\mathcal{H}$ for any sample $S$ of size $m$ can be
bounded as follows:

$${\widehat{\mathfrak{R}}}_{S}\left( H\right) \leq {\widehat{\mathfrak{R}}}_{S}\left( {\mathcal{H}}_{1}\right) + {\widehat{\mathfrak{R}}}_{S}\left( {\mathcal{H}}_{2}\right)$$

Hint: use the Lipschitz function
$x \mapsto \max \left( {0,x - 1}\right)$ and Talagrand's contraction
lemma.

Use that to bound the Rademacher complexity
${\mathfrak{R}}_{m}\left( \mathcal{U}\right)$ of the family
$\mathcal{U}$ of intersections of two concepts ${c}_{1}$ and ${c}_{2}$
with ${c}_{1} \in {\mathcal{C}}_{1}$ and ${c}_{2} \in {\mathcal{C}}_{2}$
in terms of the Rademacher complexities of ${\mathcal{C}}_{1}$ and
${\mathcal{C}}_{2}$ .

3.10 Rademacher complexity of prediction vector. Let
$S = \left( {{x}_{1},\ldots ,{x}_{m}}\right)$ be a sample of size $m$
and fix $h : \mathcal{X} \rightarrow \mathbb{R}$ .

\(a\) Denote by $\mathbf{u}$ the vector of predictions of $h$ for
$S : \mathbf{u} = \left\lbrack \begin{matrix} h\left( {x}_{1}\right) \\ \vdots \\ h\left( {x}_{m}\right) \end{matrix}\right\rbrack$
. Give an upper bound on the empirical Rademacher complexity
${\widehat{\mathfrak{R}}}_{S}\left( \mathcal{H}\right)$ of
$\mathcal{H} =$ $\{ h, - h\}$ in terms of
$\parallel \mathbf{u}{\parallel }_{2}$ (Hint: express
${\widehat{\Re }}_{S}\left( \mathcal{H}\right)$ in terms of the
expectation of an absolute value and apply Jensen's inequality). Suppose
that $h\left( {x}_{i}\right) \in$ $\{ 0, - 1, + 1\}$ for all
$i \in \left\lbrack m\right\rbrack$ . Express the bound on the
Rademacher complexity in terms of the sparsity measure
$n = \left| \left\{ {i \mid h\left( {x}_{i}\right) \neq 0}\right\} \right|$
. What is that upper bound for the extreme values of the sparsity
measure?

\(b\) Let $\mathcal{F}$ be a family of functions mapping $x$ to
$\mathbb{R}$ . Give an upper bound on the empirical Rademacher
complexity of $\mathcal{F} + h = \{ f + h : f \in \mathcal{F}\}$ and
that of
$\mathcal{F} \pm h = \left( {\mathcal{F} + h}\right) \cup \left( {\mathcal{F} - h}\right)$
in terms of ${\widehat{\mathfrak{R}}}_{S}\left( \mathcal{F}\right)$ and
$\parallel \mathbf{u}{\parallel }_{2}$ .

3.11 Rademacher complexity of regularized neural networks. Let the input
space be $\mathcal{X} = {\mathbb{R}}^{{n}_{1}}$ . In this problem, we
consider the family of regularized neural networks defined by the
following set of functions mapping $X$ to $\mathbb{R}$ :

$$\mathcal{H} = \left\{ {\mathbf{x} \mapsto \mathop{\sum }\limits_{{j = 1}}^{{n}_{2}}{w}_{j}\sigma \left( {{\mathbf{u}}_{j} \cdot \mathbf{x}}\right) : \parallel \mathbf{w}{\parallel }_{1} \leq {\Lambda }^{\prime },{\begin{Vmatrix}{\mathbf{u}}_{j}\end{Vmatrix}}_{2} \leq \Lambda ,\forall j \in \left\lbrack {n}_{2}\right\rbrack }\right\} ,$$

where $\sigma$ is an $L$ -Lipschitz function. As an example, $\sigma$
could be the sigmoid function which is 1-Lipschitz.

\(a\) Show that
${\widehat{\Re }}_{S}\left( \mathcal{H}\right) = \frac{{\Lambda }^{\prime }}{m}{\mathbb{E}}_{\mathbf{\sigma }}\left\lbrack {\mathop{\sup }\limits_{{\parallel \mathbf{u}{\parallel }_{2} \leq \Lambda }}\left| {\mathop{\sum }\limits_{{i = 1}}^{m}{\sigma }_{i}\sigma \left( {\mathbf{u} \cdot {\mathbf{x}}_{i}}\right) }\right| }\right\rbrack$
.

\(b\) Use the following form of Talagrand's lemma valid for all
hypothesis sets $\mathcal{H}$ and $L$ -Lipschitz function $\Phi$ :

$$\frac{1}{m}\mathbb{E}\left\lbrack {\mathop{\sup }\limits_{{h \in \mathcal{H}}}\left| {\mathop{\sum }\limits_{{i = 1}}^{m}{\sigma }_{i}\left( {\Phi \circ h}\right) \left( {x}_{i}\right) }\right| }\right\rbrack \leq \frac{L}{m}\mathbb{E}\left\lbrack {\mathop{\sup }\limits_{{h \in \mathcal{H}}}\left| {\mathop{\sum }\limits_{{i = 1}}^{m}{\sigma }_{i}h\left( {x}_{i}\right) }\right| }\right\rbrack ,$$

to upper bound ${\widehat{\mathfrak{R}}}_{S}\left( \mathcal{H}\right)$
in terms of the empirical Rademacher complexity of
${\mathcal{H}}^{\prime }$ , where ${\mathcal{H}}^{\prime }$ is defined
by

$${\mathcal{H}}^{\prime } = \left\{ {\mathbf{x} \mapsto s\left( {\mathbf{u} \cdot \mathbf{x}}\right) : \parallel \mathbf{u}{\parallel }_{2} \leq \Lambda ,s \in \{ - 1, + 1\} }\right\} .$$

\(c\) Use the Cauchy-Schwarz inequality to show that

$${\widehat{\mathfrak{R}}}_{S}\left( {\mathcal{H}}^{\prime }\right) = \frac{\Lambda }{m}\underset{\sigma }{\mathbb{E}}\left\lbrack {\begin{Vmatrix}\mathop{\sum }\limits_{{i = 1}}^{m}{\sigma }_{i}{\mathbf{x}}_{i}\end{Vmatrix}}_{2}\right\rbrack .$$

\(d\) Use the inequality
${\mathbb{E}}_{\mathbf{y}}\left\lbrack {\parallel \mathbf{v}{\parallel }_{2}}\right\rbrack \leq \sqrt{{\mathbb{E}}_{\mathbf{v}}\left\lbrack {\parallel \mathbf{v}{\parallel }_{2}^{2}}\right\rbrack }$
, which holds by Jensen's inequality to upper bound
${\widehat{\mathfrak{R}}}_{S}\left( {\mathcal{H}}^{\prime }\right)$ .

\(e\) Assume that for all
$\mathbf{x} \in S,\parallel \mathbf{x}{\parallel }_{2} \leq r$ for some
$r > 0$ . Use the previous questions to derive an upper bound on the
Rademacher complexity of $\mathcal{H}$ in terms of $r$ .

3.12 Rademacher complexity. Professor Jesetoo claims to have found a
better bound on the Rademacher complexity of any hypothesis set
$\mathcal{H}$ of functions taking values in $\{ - 1, + 1\}$ , in terms
of its VC-dimension VCdim $\left( \mathcal{H}\right)$ . His bound is of
the form
${\mathfrak{R}}_{m}\left( \mathcal{H}\right) \leq O\left( \frac{\operatorname{VCdim}\left( \mathcal{H}\right) }{m}\right)$
. Can you show that Professor Jesetoo's claim cannot be correct? (Hint:
consider a hypothesis set $\mathcal{H}$ reduced to just two simple
functions.)

3.13 VC-dimension of union of $k$ intervals. What is the VC-dimension of
subsets of the real line formed by the union of $k$ intervals?

3.14 VC-dimension of finite hypothesis sets. Show that the VC-dimension
of a finite hypothesis set $\mathcal{H}$ is at most
${\log }_{2}\left| \mathcal{H}\right|$ .

3.15 VC-dimension of subsets. What is the VC-dimension of the set of
subsets ${I}_{\alpha }$ of the real line parameterized by a single
parameter
$\alpha : {I}_{\alpha } = \left\lbrack {\alpha ,\alpha + 1}\right\rbrack \cup \lbrack \alpha + 2, + \infty )$
?

3.16 VC-dimension of axis-aligned squares and triangles.

\(a\) What is the VC-dimension of axis-aligned squares in the plane?

\(b\) Consider right triangles in the plane with the sides adjacent to
the right angle both parallel to the axes and with the right angle in
the lower left corner. What is the VC-dimension of this family?

3.17 VC-dimension of closed balls in ${\mathbb{R}}^{n}$ . Show that the
VC-dimension of the set of all closed balls in ${\mathbb{R}}^{n}$ ,
i.e., sets of the form
$\left\{ {x \in {\mathbb{R}}^{n} : {\begin{Vmatrix}x - {x}_{0}\end{Vmatrix}}^{2} \leq r}\right\}$
for some ${x}_{0} \in {\mathbb{R}}^{n}$ and $r \geq 0$ , is less than or
equal to $n + 2$ .

3.18 VC-dimension of ellipsoids. What is the VC-dimension of the set of
all ellipsoids in ${\mathbb{R}}^{n}$ ?

3.19 VC-dimension of a vector space of real functions. Let $F$ be a
finite-dimensional vector space of real functions on
${\mathbb{R}}^{n},\dim \left( F\right) = r < \infty$ . Let $\mathcal{H}$
be the set of hypotheses:

$$\mathcal{H} = \{ \{ x : f\left( x\right) \geq 0\} : f \in F\} .$$

Show that $d$ , the VC-dimension of $\mathcal{H}$ , is finite and that
$d \leq r$ . (Hint: select an arbitrary set of $m = r + 1$ points and
consider linear mapping $u : F \rightarrow {\mathbb{R}}^{m}$ defined by:
$u\left( f\right) = \left( {f\left( {x}_{1}\right) ,\ldots ,f\left( {x}_{m}\right) }\right)$
.)

3.20 VC-dimension of sine functions. Consider the hypothesis family of
sine functions (Example 3.16):
$\{ x \rightarrow \sin \left( {\omega x}\right) : \omega \in \mathbb{R}\}$
.

\(a\) Show that for any $x \in \mathbb{R}$ the points $x,{2x},{3x}$ and
${4x}$ cannot be shattered by this family of sine functions.

\(b\) Show that the VC-dimension of the family of sine functions is
infinite. (Hint: show that $\left\{ {{2}^{-i} : i \leq m}\right\}$ can
be shattered for any $m > 0$ .)

3.21 VC-dimension of union of halfspaces. Provide an upper bound on the
VC-dimension of the class of hypotheses described by the unions of $k$
halfspaces.

3.22 VC-dimension of intersection of halfspaces. Consider the class
${\mathcal{C}}_{k}$ of convex intersections of $k$ halfspaces. Give
lower and upper bound estimates for
$\operatorname{VCdim}\left( {\mathcal{C}}_{k}\right)$ .

3.23 VC-dimension of intersection concepts.

\(a\) Let ${\mathcal{C}}_{1}$ and ${\mathcal{C}}_{2}$ be two concept
classes. Show that for any concept class
$\mathcal{C} = \left\{ {{c}_{1} \cap {c}_{2} : {c}_{1} \in {\mathcal{C}}_{1},{c}_{2} \in {\mathcal{C}}_{2}}\right\}$

$${\Pi }_{\mathcal{C}}\left( m\right) \leq {\Pi }_{{\mathcal{C}}_{1}}\left( m\right) {\Pi }_{{\mathcal{C}}_{2}}\left( m\right) \tag{3.53}$$

\(b\) Let $\mathcal{C}$ be a concept class with VC-dimension $d$ and let
${\mathcal{C}}_{s}$ be the concept class formed by all intersections of
$s$ concepts from $\mathcal{C},s \geq 1$ . Show that the VC-dimension of
${\mathcal{C}}_{s}$ is bounded by ${2ds}{\log }_{2}\left( {3s}\right)$ .
(Hint: show that ${\log }_{2}\left( {3x}\right) <$
${9x}/\left( {2e}\right)$ for any $x \geq 2$ .)

3.24 VC-dimension of union of concepts. Let $\mathcal{A}$ and
$\mathcal{B}$ be two sets of functions mapping from $X$ into $\{ 0,1\}$
, and assume that both $\mathcal{A}$ and $\mathcal{B}$ have finite
VC-dimension, with
$\operatorname{VCdim}\left( \mathcal{A}\right) = {d}_{\mathcal{A}}$ and
$\operatorname{VCdim}\left( \mathcal{B}\right) = {d}_{\mathcal{B}}$ .
Let $\mathcal{C} = \mathcal{A} \cup \mathcal{B}$ be the union of
$\mathcal{A}$ and $\mathcal{B}$ .

\(a\) Prove that for all
$m,{\Pi }_{\mathcal{C}}\left( m\right) \leq {\Pi }_{\mathcal{A}}\left( m\right) + {\Pi }_{\mathcal{B}}\left( m\right)$
.

\(b\) Use Sauer's lemma to show that for
$m \geq {d}_{\mathcal{A}} + {d}_{\mathcal{B}} + 2,{\Pi }_{\mathcal{C}}\left( m\right) < {2}^{m}$
, and give a bound on the VC-dimension of $\mathcal{C}$ .

3.25 VC-dimension of symmetric difference of concepts. For two sets
$\mathcal{A}$ and $\mathcal{B}$ , let $\mathcal{A}\Delta \mathcal{B}$
denote the symmetric difference of $\mathcal{A}$ and $\mathcal{B}$ ,
i.e.,
$\mathcal{A}\Delta \mathcal{B} = \left( {\mathcal{A} \cup \mathcal{B}}\right) - \left( {\mathcal{A} \cap \mathcal{B}}\right)$
. Let $\mathcal{H}$ be a non-empty family of subsets of $x$ with finite
VC-dimension. Let $\mathcal{A}$ be an element of $\mathcal{H}$ and
define
$\mathcal{H}\Delta \mathcal{A} = \{ {X\Delta }\mathcal{A} : X \in \mathcal{H}\}$
. Show that

$$\operatorname{VCdim}\left( {\mathcal{H}\Delta \mathcal{A}}\right) = \operatorname{VCdim}\left( \mathcal{H}\right)$$

3.26 Symmetric functions. A function
$h : \{ 0,1{\} }^{n} \rightarrow \{ 0,1\}$ is symmetric if its value is
uniquely determined by the number of 1's in the input. Let $\mathcal{C}$
denote the set of all symmetric functions.

\(a\) Determine the VC-dimension of $\mathcal{C}$ .

\(b\) Give lower and upper bounds on the sample complexity of any
consistent PAC learning algorithm for $\mathcal{C}$ .

\(c\) Note that any hypothesis $h \in \mathcal{C}$ can be represented by
a vector $\left( {{y}_{0},{y}_{1},\ldots ,{y}_{n}}\right)$
$\in \{ 0,1{\} }^{n + 1}$ , where ${y}_{i}$ is the value of $h$ on
examples having precisely ${i1}$ 's. Devise a consistent learning
algorithm for $\mathcal{C}$ based on this representation.

3.27 VC-dimension of neural networks. {#vc-dimension-of-neural-networks. .unnumbered}

Let $\mathcal{C}$ be a concept class over ${\mathbb{R}}^{r}$ with
VC-dimension $d$ . A $\mathcal{C}$ -neural network with one intermediate
layer is a concept defined over ${\mathbb{R}}^{n}$ that can be
represented by a directed acyclic graph such as that of Figure 3.7, in
which the input nodes are those at the bottom and in which each other
node is labeled with a concept $c \in \mathcal{C}$ .

The output of the neural network for a given input vector
$\left( {{x}_{1},\ldots ,{x}_{n}}\right)$ is obtained as follows. First,
each of the $n$ input nodes is labeled with the corresponding value
${x}_{i} \in \mathbb{R}$ . Next, the value at a node $u$ in the higher
layer and labeled with $c$ is obtained by applying $c$ to the values of
the input nodes admitting an

::: center
![image](images/019145d2-cc04-715a-aec7-5f9a00e87681_28_466127.jpg){width="30%"}
:::

Figure 3.7

A neural network with one intermediate layer.

edge ending in $u$ . Note that since $c$ takes values in $\{ 0,1\}$ ,
the value at $u$ is in $\{ 0,1\}$ . The value at the top or output node
is obtained similarly by applying the corresponding concept to the
values of the nodes admitting an edge to the output node.

\(a\) Let $\mathcal{H}$ denote the set of all neural networks defined as
above with $k \geq 2$ internal nodes. Show that the growth function
${\Pi }_{\mathcal{H}}\left( m\right)$ can be upper bounded in terms of
the product of the growth functions of the hypothesis sets defined at
each intermediate layer.

\(b\) Use that to upper bound the VC-dimension of the C-neural networks
(Hint: you can use the implication
$m = {2x}{\log }_{2}\left( {xy}\right) \Rightarrow m > x{\log }_{2}\left( {ym}\right)$
valid for $m \geq 1$ , and $x,y > 0$ with ${xy} > 4$ ).

\(c\) Let $\mathcal{C}$ be the family of concept classes defined by
threshold functions $\mathcal{C} =$
$\left\{ {\operatorname{sgn}\left( {\mathop{\sum }\limits_{{j = 1}}^{r}{w}_{j}{x}_{j}}\right) : \mathbf{w} \in {\mathbb{R}}^{r}}\right\}$
. Give an upper bound on the VC-dimension of $\mathcal{H}$ in terms of
$k$ and $r$ .

3.28 VC-dimension of convex combinations. Let $\mathcal{H}$ be a family
of functions mapping from an input space $X$ to $\{ - 1, + 1\}$ and let
$T$ be a positive integer. Give an upper bound on the VC-dimension of
the family of functions ${\mathcal{F}}_{T}$ defined by

$$\mathcal{F} = \left\{ {\operatorname{sgn}\left( {\mathop{\sum }\limits_{{t = 1}}^{T}{\alpha }_{t}{h}_{t}}\right) : {h}_{t} \in \mathcal{H},{\alpha }_{t} \geq 0,\mathop{\sum }\limits_{{t = 1}}^{T}{\alpha }_{t} \leq 1}\right\} .$$

(Hint: you can use exercise 3.27 and its solution).

3.29 Infinite VC-dimension.

\(a\) Show that if a concept class $\mathcal{C}$ has infinite
VC-dimension, then it is not PAC-learnable.

\(b\) In the standard PAC-learning scenario, the learning algorithm
receives all examples first and then computes its hypothesis. Within
that setting, PAC-learning of concept classes with infinite VC-dimension
is not possible as seen in the previous question.

Imagine now a different scenario where the learning algorithm can
alternate between drawing more examples and computation. The objective
of this problem is to prove that PAC-learning can then be possible for
some concept classes with infinite VC-dimension.

Consider for example the special case of the concept class $\mathcal{C}$
of all subsets of natural numbers. Professor Vitres has an idea for the
first stage of a learning algorithm $L$ PAC-learning $\mathcal{C}$ . In
the first stage, $L$ draws a sufficient number of points $m$ such that
the probability of drawing a point beyond the maximum value $M$ observed
be small with high confidence. Can you complete Professor Vitres' idea
by describing the second stage of the algorithm so that it PAC-learns
$\mathcal{C}$ ? The description should be augmented with the proof that
$L$ can PAC-learn C.

3.30 VC-dimension generalization bound - realizable case. In this
exercise we show that the bound given in corollary 3.19 can be improved
to $O\left( \frac{d\log \left( {m/d}\right) }{m}\right)$ in the
realizable setting. Assume we are in the realizable scenario, i.e. the
target concept is included in our hypothesis class $\mathcal{H}$ . We
will show that if a hypothesis $h$ is consistent with a sample
$S \sim {\mathcal{D}}^{m}$ then for any $\epsilon > 0$ such that
${m\epsilon } \geq 8$

$$\mathbb{P}\left\lbrack {R\left( h\right) > \epsilon }\right\rbrack \leq 2{\left\lbrack \frac{2em}{d}\right\rbrack }^{d}{2}^{-{m\epsilon }/2}. \tag{3.54}$$

\(a\) Let ${\mathcal{H}}_{S} \subseteq \mathcal{H}$ be the subset of
hypotheses consistent with the sample $S$ , let
${\widehat{R}}_{S}\left( h\right)$ denote the empirical error with
respect to the sample $S$ and define ${S}^{\prime }$ as another
independent sample drawn from ${\mathcal{D}}^{m}$ . Show that the
following inequality holds for any ${h}_{0} \in {\mathcal{H}}_{S}$ :

$$\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{h \in {\mathcal{H}}_{S}}}\left| {{\widehat{R}}_{S}\left( h\right) - {\widehat{R}}_{{S}^{\prime }}\left( h\right) }\right| > \frac{\epsilon }{2}}\right\rbrack \geq \mathbb{P}\left\lbrack {B\left( {m,\epsilon }\right) > \frac{m\epsilon }{2}}\right\rbrack \mathbb{P}\left\lbrack {R\left( {h}_{0}\right) > \epsilon }\right\rbrack ,$$

where $B\left( {m,\epsilon }\right)$ is a binomial random variable with
parameters $\left( {m,\epsilon }\right)$ . (Hint: prove and use the fact
that
$\mathbb{P}\left\lbrack {{\widehat{R}}_{S}\left( h\right) \geq \frac{\epsilon }{2}}\right\rbrack \geq \mathbb{P}\left\lbrack {{\widehat{R}}_{S}\left( h\right) > \frac{\epsilon }{2} \land R\left( h\right) > \epsilon }\right\rbrack$
.)

\(b\) Prove that
$\mathbb{P}\left\lbrack {B\left( {m,\epsilon }\right) > \frac{m\epsilon }{2}}\right\rbrack \geq \frac{1}{2}$
. Use this inequality along with the result from (a) to show that for
any ${h}_{0} \in {\mathcal{H}}_{S}$

$$\mathbb{P}\left\lbrack {R\left( {h}_{0}\right) > \epsilon }\right\rbrack \leq 2\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{h \in {\mathcal{H}}_{S}}}\left| {{\widehat{R}}_{S}\left( h\right) - {\widehat{R}}_{{S}^{\prime }}\left( h\right) }\right| > \frac{\epsilon }{2}}\right\rbrack .$$

\(c\) Instead of drawing two samples, we can draw one sample $T$ of size
${2m}$ then uniformly at random split it into $S$ and ${S}^{\prime }$ .
The right hand side of part (b) can then be rewritten as:

$$\mathbb{P}\left\lbrack {\mathop{\sup }\limits_{{h \in {\mathcal{H}}_{S}}}\left| {{\widehat{R}}_{S}\left( h\right) - {\widehat{R}}_{{S}^{\prime }}\left( h\right) }\right| > \frac{\epsilon }{2}}\right\rbrack = \underset{\begin{matrix} {T \sim {\mathcal{D}}^{2m},} \\ {T \rightarrow \left\lbrack {S,{S}^{\prime }}\right\rbrack } \end{matrix}}{\mathbb{P}}\left\lbrack {\exists h \in \mathcal{H} : {\widehat{R}}_{S}\left( h\right) = 0 \land {\widehat{R}}_{{S}^{\prime }}\left( h\right) > \frac{\epsilon }{2}}\right\rbrack .$$

Let ${h}_{0}$ be a hypothesis such that
${\widehat{R}}_{T}\left( {h}_{0}\right) > \frac{\epsilon }{2}$ and let
$l > \frac{m\epsilon }{2}$ be the total number of errors ${h}_{0}$ makes
on $T$ . Show that the probability of all $l$ errors falling into
${S}^{\prime }$ is upper bounded by ${2}^{-l}$ .

\(d\) Part (b) implies that for any $h \in \mathcal{H}$

$$\underset{\begin{matrix} {T \sim {\mathcal{D}}^{2m} : } \\ {T \rightarrow \left( {S,{S}^{\prime }}\right) } \end{matrix}}{\mathbb{P}}\left\lbrack {{\widehat{R}}_{S}\left( h\right) = 0 \land {\widehat{R}}_{{S}^{\prime }}\left( h\right) > \frac{\epsilon }{2} \mid {\widehat{R}}_{T}\left( {h}_{0}\right) > \frac{\epsilon }{2}}\right\rbrack \leq {2}^{-l}.$$

Use this bound to show that for any $h \in \mathcal{H}$

$$\underset{\begin{matrix} {T \sim {\mathcal{D}}^{2m};} \\ {T \rightarrow \left( {S,{S}^{\prime }}\right) } \end{matrix}}{\mathbb{P}}\left\lbrack {{\widehat{R}}_{S}\left( h\right) = 0 \land {\widehat{R}}_{{S}^{\prime }}\left( h\right) > \frac{\epsilon }{2}}\right\rbrack \leq {2}^{-\frac{\epsilon m}{2}}.$$

\(e\) Complete the proof of inequality (3.54) by using the union bound
to upper bound
${\mathbb{P}}_{T \sim {\mathcal{D}}^{2}m} : \left\lbrack {\exists h \in \mathcal{H} : {\widehat{R}}_{S}\left( h\right) = 0 \land {\widehat{R}}_{{S}^{\prime }}\left( h\right) > \frac{\epsilon }{2}}\right\rbrack$
. Show that we can achieve a high probability generalization bound that
is of the order $O\left( \frac{d\log \left( {m/d}\right) }{m}\right)$ .

3.31 Generalization bound based on covering numbers. Let $\mathcal{H}$
be a family of functions mapping $x$ to a subset of real numbers
$y \subseteq \mathbb{R}$ . For any $\epsilon > 0$ , the covering number
$\mathcal{N}\left( {\mathcal{H},\epsilon }\right)$ of $\mathcal{H}$ for
the ${L}_{\infty }$ norm is the minimal $k \in \mathbb{N}$ such that
$\mathcal{H}$ can be covered with $k$ balls of radius $\epsilon$ , that
is, there exists
$\left\{ {{h}_{1},\ldots ,{h}_{k}}\right\} \subseteq \mathcal{H}$ such
that, for all $h \in \mathcal{H}$ , there exists $i \leq k$ with
${\begin{Vmatrix}h - {h}_{i}\end{Vmatrix}}_{\infty } = \mathop{\max }\limits_{{x \in X}}\left| {h\left( x\right) - {h}_{i}\left( x\right) }\right| \leq \epsilon$
. In particular, when $\mathcal{H}$ is a compact set, a finite covering
can be extracted from a covering of $\mathcal{H}$ with balls of radius
$\epsilon$ and thus $\mathcal{N}\left( {\mathcal{H},\epsilon }\right)$
is finite.

Covering numbers provide a measure of the complexity of a class of
functions: the larger the covering number, the richer is the family of
functions. The objective of this problem is to illustrate this by
proving a learning bound in the case of the squared loss. Let
$\mathcal{D}$ denote a distribution over $X \times \mathcal{Y}$
according to which

labeled examples are drawn. Then, the generalization error of
$h \in \mathcal{H}$ for the squared loss is defined by
$R\left( h\right) = {\mathbb{E}}_{\left( {x,y}\right) \sim \mathcal{D}}\left\lbrack {\left( h\left( x\right) - y\right) }^{2}\right\rbrack$
and its empirical error for a labeled sample
$S = \left( {\left( {{x}_{1},{y}_{1}}\right) ,\ldots ,\left( {{x}_{m},{y}_{m}}\right) }\right)$
by
${\widehat{R}}_{S}\left( h\right) = \frac{1}{m}\mathop{\sum }\limits_{{i = 1}}^{m}{\left( h\left( {x}_{i}\right) - {y}_{i}\right) }^{2}$
. We will assume that $\mathcal{H}$ is bounded, that is there exists
$M > 0$ such that $\left| {h\left( x\right) - y}\right| \leq M$ for all
$\left( {x,y}\right) \in X \times y$ . The following is the
generalization bound proven in this problem:

$$\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{P}}\left\lbrack {\mathop{\sup }\limits_{{h \in \mathcal{H}}}\left| {R\left( h\right) - {\widehat{R}}_{S}\left( h\right) }\right| \geq \epsilon }\right\rbrack \leq \mathcal{N}\left( {\mathcal{H},\frac{\epsilon }{8M}}\right) 2\exp \left( \frac{-m{\epsilon }^{2}}{2{M}^{4}}\right) . \tag{3.55}$$

The proof is based on the following steps.

\(a\) Let
${L}_{S} = R\left( h\right) - {\widehat{R}}_{S}\left( h\right)$ , then
show that for all ${h}_{1},{h}_{2} \in \mathcal{H}$ and any labeled
sample $S$ , the following inequality holds:

$$\left| {{L}_{S}\left( {h}_{1}\right) - {L}_{S}\left( {h}_{2}\right) }\right| \leq {4M}{\begin{Vmatrix}{h}_{1} - {h}_{2}\end{Vmatrix}}_{\infty }.$$

\(b\) Assume that $\mathcal{H}$ can be covered by $k$ subsets
${\mathcal{B}}_{1},\ldots ,{\mathcal{B}}_{k}$ , that is
$\mathcal{H} = {\mathcal{B}}_{1} \cup$ $\ldots \cup {\mathcal{B}}_{k}$ .
Then, show that, for any $\epsilon > 0$ , the following upper bound
holds:

$$\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{P}}\left\lbrack {\mathop{\sup }\limits_{{h \in \mathcal{H}}}\left| {{L}_{S}\left( h\right) }\right| \geq \epsilon }\right\rbrack \leq \mathop{\sum }\limits_{{i = 1}}^{k}\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{P}}\left\lbrack {\mathop{\sup }\limits_{{h \in {\mathcal{B}}_{i}}}\left| {{L}_{S}\left( h\right) }\right| \geq \epsilon }\right\rbrack .$$

\(c\) Finally, let
$k = \mathcal{N}\left( {\mathcal{H},\frac{\epsilon }{8M}}\right)$ and
let ${\mathcal{B}}_{1},\ldots ,{\mathcal{B}}_{k}$ be balls of radius
$\epsilon /\left( {8M}\right)$ centered at ${h}_{1},\ldots ,{h}_{k}$
covering $\mathcal{H}$ . Use part (a) to show that for all
$i \in \left\lbrack k\right\rbrack$ ,

$$\underset{S \sim {\mathcal{D}}^{m}}{\mathbb{P}}\left\lbrack {\mathop{\sup }\limits_{{h \in {\mathcal{B}}_{i}}}\left| {{L}_{S}\left( h\right) }\right| \geq \epsilon }\right\rbrack \leq \underset{S \sim {\mathcal{D}}^{m}}{\mathbb{P}}\left\lbrack {\left| {{L}_{S}\left( {h}_{i}\right) }\right| \geq \frac{\epsilon }{2}}\right\rbrack ,$$

and apply Hoeffding's inequality (theorem D.2) to prove (3.55).