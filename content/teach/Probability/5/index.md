---
title: 5 Joint Probability Distributions and Random Samples
---
# Content
- [[5.1 Jointly Distributed Random Variables]]
- [[5.2 Expected Values, Covariance, and Correlation]]
- [[5.3 Statistics and Their Distributions]]
- [[5.4 The Distribution of the Sample Mean]]
- [[5.5 The Distribution of a Linear Combination]]
- [[SUPPLEMENTARY EXERCISES (75â€“96)]]
- [[teach/Probability/5/BIBLIOGRAPHY|BIBLIOGRAPHY]]
# Introduction
In Chapters 3 and 4 we developed probability models for a single random variable. Many problems in probability and statistics involve working simultaneously with two or more random variables. For example, $X$ and $Y$ might be the height and weight, respectively, of a randomly selected individual. Or ${X}_{1},{X}_{2}$ , and ${X}_{3}$ might be the number of purchases made with Visa, MasterCard, and American Express credit cards, respectively, at a store on a particular day. In Section 5.1 we discuss probability models for the joint (i.e., simultaneous) behavior of two or more random variables. The very important concept of independence of several random variables is then introduced and explored. Section 5.2 considers the expected value of a function of two or more random variables 
e.g., the expected value of $Y/{X}^{2}$ , the body mass index (BMI) when $X$ is expressed in $\mathrm{{cm}}$ and $Y$ is expressed in $\mathrm{{kg}}$ . 
This leads to a discussion of covariance and correlation as measures of the degree of association between two variables. At the end of the section, the bivariate normal distribution is introduced as a generalization of the univariate normal distribution.

Sections 5.3 and 5.4 consider functions of the $n$ variables ${X}_{1},{X}_{2},\ldots ,{X}_{n}$ that constitute a sample from some population or distribution (for example, a sample of weights of newborn children). The most important function of this type is the sample average $\left( {{X}_{1} + {X}_{2} + \cdots  + {X}_{n}}\right) /n$ . We will call any such function, itself a random variable, a statistic. Methods from probability are used to obtain information about the distribution of a statistic. The premier result of this type is the Central Limit Theorem (CLT), the basis for many inferential procedures involving large sample sizes. The last section of the chapter deals with linear functions of the form ${a}_{1}{X}_{1} + \cdots  + {a}_{n}{X}_{n}$ where the ${a}_{i}^{\prime }s$ are numerical constants.