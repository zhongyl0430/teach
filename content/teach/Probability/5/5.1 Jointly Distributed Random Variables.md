## 5.1 Jointly Distributed Random Variables

There are many experimental situations in which more than one random variable (rv) will be of interest to an investigator. We first consider joint probability distributions for two random variables. The "pure" cases, in which both variables are discrete or both are continuous, are the ones most frequently encountered in practice.

### 5.1.1 Two Discrete Random Variables

The probability mass function (pmf) of a single discrete rv $X$ specifies how much probability mass is placed on each possible $X$ value. The joint pmf of two discrete rv’s $X$ and $Y$ describes how much probability mass is placed on each possible pair of values $\left( {x, y}\right)$ .

---

DEFINITION

---

Let $X$ and $Y$ be two discrete rv’s defined on the sample space $\mathcal{S}$ of an experiment. The joint probability mass function $p\left( {x, y}\right)$ is defined for each pair of numbers $\left( {x, y}\right)$ by

$$
p\left( {x, y}\right) = P\left( {X = x\text{ and }Y = y}\right)
$$

It must be the case that $p\left( {x, y}\right) \geq 0$ and $\mathop{\sum }\limits_{x}\mathop{\sum }\limits_{y}p\left( {x, y}\right) = 1$ .

Now let $A$ be any particular set consisting of pairs of $\left( {x, y}\right)$ values (e.g., $A = \{ \left( {x, y}\right) : x + y = 5\}$ or $\{ \left( {x, y}\right) : \max \left( {x, y}\right) \leq 3\} )$ . Then the probability $P\left\lbrack {\left( {X, Y}\right) \in A}\right\rbrack$ that the random pair $\left( {X, Y}\right)$ lies in the set $A$ is obtained by summing the joint pmf over pairs in $A$ :

$$
P\left\lbrack {\left( {X, Y}\right) \in A}\right\rbrack = \mathop{\sum }\limits_{\left( x, y\right) }\mathop{\sum }\limits_{{ \in A}}p\left( {x, y}\right)
$$

EXAMPLE 5.1 Anyone who purchases an insurance policy for a home or automobile must specify a deductible amount, the amount of loss to be absorbed by the policyholder before the insurance company begins paying out. Suppose that a particular company offers auto deductible amounts of $\$ {100},\$ {500}$ , and $\$ {1000}$ , and homeowner deductible amounts of $\$ {500},\$ {1000}$ , and $\$ {2000}$ . Consider randomly selecting someone who has both auto and homeowner insurance with this company, and let $X =$ the amount of the auto policy deductible and $Y =$ the amount of the homeowner policy deductible. The joint pmf of these two variables appears in the accompanying joint probability table:

<table><thead><tr><th colspan="4">$y$</th></tr><tr><th>$p\left( {x, y}\right)$</th><th>500</th><th>1000</th><th>5000</th></tr></thead><tr><td>100</td><td>.30</td><td>.05</td><td>0</td></tr><tr><td>$x$500</td><td>.15</td><td>.20</td><td>.05</td></tr><tr><td>1000</td><td>.10</td><td>.10</td><td>.05</td></tr></table>

According to this joint pmf, there are nine possible $\left( {X, Y}\right)$ pairs: $\left( {{100},{500}}\right) ,({100}$ , ${1000}),\ldots$ , and finally $\left( {{1000},{5000}}\right)$ . The probability of $\left( {{100},{500}}\right)$ is $p\left( {{100},{500}}\right) =$ $P\left( {X = {100}, Y = {500}}\right) = {.30}$ . Clearly $p\left( {x, y}\right) \geq 0$ , and it is easily confirmed that the sum of the nine displayed probabilities is 1 . The probability $P\left( {X = Y}\right)$ is computed by summing $p\left( {x, y}\right)$ over the two $\left( {x, y}\right)$ pairs for which the two deductible amounts are identical:

$$
P\left( {X = Y}\right) = p\left( {{500},{500}}\right) + p\left( {{1000},{1000}}\right) = {.15} + {.10} = {.25}
$$

Similarly, the probability that the auto deductible amount is at least \$500 is the sum of all probabilities corresponding to $\left( {x, y}\right)$ pairs for which $x \geq {500}$ ; this is the sum of the probabilities in the bottom two rows of the joint probability table:

$$
P\left( {X \geq {500}}\right) = {.15} + {.20} + {.05} + {.10} + {.10} + {.05} = {.65}
$$

Once the joint pmf of the two variables $X$ and $Y$ is available, it is in principle straightforward to obtain the distribution of just one of these variables. As an example, let $X$ and $Y$ be the number of statistics and mathematics courses, respectively, currently being taken by a randomly selected statistics major. Suppose that we wish the distribution of $X$ , and that when $X = 2$ , the only possible values of $Y$ are 0,1, and 2 . Then

$$
{p}_{X}\left( 2\right) = P\left( {X = 2}\right) = P\left\lbrack {\left( {X, Y}\right) = \left( {2,0}\right) \text{ or }\left( {2,1}\right) \text{ or }\left( {2,2}\right) }\right\rbrack
$$

$$
= p\left( {2,0}\right) + p\left( {2,1}\right) + p\left( {2,2}\right)
$$

That is, the joint pmf is summed over all pairs of the form $\left( {2, y}\right)$ . More generally, for any possible value $x$ of $X$ , the probability ${p}_{X}\left( x\right)$ results from holding $x$ fixed and summing the joint $\operatorname{pmf}p\left( {x, y}\right)$ over all $y$ for which the pair $\left( {x, y}\right)$ has positive probability mass. The same strategy applies to obtaining the distribution of $Y$ by itself.

DEFINITION

The marginal probability mass function of $X$ , denoted by ${p}_{X}\left( x\right)$ , is given by

$$
{p}_{X}\left( x\right) = \mathop{\sum }\limits_{{y : p\left( {x, y}\right) > 0}}p\left( {x, y}\right) \;\text{ for each possible value }x
$$

Similarly, the marginal probability mass function of $Y$ is

$$
{p}_{Y}\left( y\right) = \mathop{\sum }\limits_{{x : p\left( {x, y}\right) > 0}}p\left( {x, y}\right) \;\text{ for each possible value }y.
$$

The use of the word marginal here is a consequence of the fact that if the joint pmf is displayed in a rectangular table as in Example 5.1, then the row totals give the marginal pmf of $X$ and the column totals give the marginal pmf of $Y$ . Once these marginal pmf’s are available, the probability of any event involving only $X$ or only $Y$ can be calculated.

---

EXAMPLE 5.2

(Example 5.1

continued)

---

Possible $X$ values are $x = {100},{500}$ , and 1000 . Computing row totals from the joint probability table yields

$$
{p}_{X}\left( {100}\right) = p\left( {{100},{500}}\right) + p\left( {{100},{1000}}\right) + p\left( {{100},{5000}}\right) = {.30} + {.05} + 0 = {.35}
$$

$$
{p}_{X}\left( {500}\right) = {.15} + {.20} + {.05} = {.40},{p}_{X}\left( {1000}\right) = 1 - \left( {{.35} + {.40}}\right) = {.25}
$$

The marginal pmf of $X$ is then

$$
{p}_{X}\left( x\right) = \left\{ \begin{array}{ll} {.35} & x = {100} \\ {.40} & x = {500} \\ {.25} & x = {1000} \\ 0 & \text{ otherwise } \end{array}\right.
$$

From this pmf, $P\left( {X \geq {500}}\right) = {.40} + {.25} = {.65}$ , which we already calculated in Example 5.1. Similarly, the marginal pmf of $Y$ is obtained from the column totals as

$$
{p}_{Y}\left( y\right) = \left\{ \begin{matrix} {.55} & y = {500} \\ {.35} & y = {1000} \\ {.10} & y = {5000} \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

### 5.1.2 Two Continuous Random Variables

The probability that the observed value of a continuous rv $X$ lies in a one-dimensional set $A$ (such as an interval) is obtained by integrating the pdf $f\left( x\right)$ over the set $A$ . Similarly, the probability that the pair $\left( {X, Y}\right)$ of continuous rv’s falls in a two-dimensional set $A$ (such as a rectangle) is obtained by integrating a function called the joint density function.

---

DEFINITION

---

Let $X$ and $Y$ be continuous rv’s. A joint probability density function $f\left( {x, y}\right)$ for these two variables is a function satisfying $f\left( {x, y}\right) \geq 0$ and ${\int }_{-\infty }^{\infty }{\int }_{-\infty }^{\infty }f\left( {x, y}\right) {dxdy} = 1$ . Then for any two-dimensional set $A$

$$
P\left\lbrack {\left( {X, Y}\right) \in A}\right\rbrack = {\int }_{A}\int f\left( {x, y}\right) {dxdy}
$$

In particular, if $A$ is the two-dimensional rectangle $\{ \left( {x, y}\right) : a \leq x \leq b, c \leq y \leq d\}$ , then

$$
P\left\lbrack {\left( {X, Y}\right) \in A}\right\rbrack = P\left( {a \leq X \leq b, c \leq Y \leq d}\right) = {\int }_{a}^{b}{\int }_{c}^{d}f\left( {x, y}\right) {dydx}
$$

We can visualize $f\left( {x, y}\right)$ as specifying a surface at height $f\left( {x, y}\right)$ above the point $\left( {x, y}\right)$ in a three-dimensional coordinate system. Then $P\left\lbrack {\left( {X, Y}\right) \in A}\right\rbrack$ is the volume underneath this surface and above the region $A$ , analogous to the area under a curve in the case of a single rv. This is illustrated in Figure 5.1.

![0192609f-6f5c-74c9-8588-c1ef28b2184d_3_935_1684_593_299_0.jpg](images/0192609f-6f5c-74c9-8588-c1ef28b2184d_3_935_1684_593_299_0.jpg)

Figure ${5.1P}\left\lbrack {\left( {X, Y}\right) \in A}\right\rbrack =$ volume under density surface above $A$

(AMPLE 5.3 A bank operates both a drive-up facility and a walk-up window. On a randomly selected day, let $X =$ the proportion of time that the drive-up facility is in use (at least one customer is being served or waiting to be served) and $Y =$ the proportion of time

that the walk-up window is in use. Then the set of possible values for $\left( {X, Y}\right)$ is the rectangle $D = \{ \left( {x, y}\right) : 0 \leq x \leq 1,0 \leq y \leq 1\}$ . Suppose the joint pdf of $\left( {X, Y}\right)$ is given by

$$
f\left( {x, y}\right) = \left\{ \begin{matrix} \frac{6}{5}\left( {x + {y}^{2}}\right) & 0 \leq x \leq 1,0 \leq y \leq 1 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

To verify that this is a legitimate pdf, note that $f\left( {x, y}\right) \geq 0$ and

$$
{\int }_{-\infty }^{\infty }{\int }_{-\infty }^{\infty }f\left( {x, y}\right) {dxdy} = {\int }_{0}^{1}{\int }_{0}^{1}\frac{6}{5}\left( {x + {y}^{2}}\right) {dxdy}
$$

$$
= {\int }_{0}^{1}{\int }_{0}^{1}\frac{6}{5}{xdxdy} + {\int }_{0}^{1}{\int }_{0}^{1}\frac{6}{5}{y}^{2}{dxdy}
$$

$$
= {\int }_{0}^{1}\frac{6}{5}{xdx} + {\int }_{0}^{1}\frac{6}{5}{y}^{2}{dy} = \frac{6}{10} + \frac{6}{15} = 1
$$

The probability that neither facility is busy more than one-quarter of the time is

$$
P\left( {0 \leq X \leq \frac{1}{4},0 \leq Y \leq \frac{1}{4}}\right) = {\int }_{0}^{1/4}{\int }_{0}^{1/4}\frac{6}{5}\left( {x + {y}^{2}}\right) {dxdy}
$$

$$
= \frac{6}{5}{\int }_{0}^{1/4}{\int }_{0}^{1/4}{xdxdy} + \frac{6}{5}{\int }_{0}^{1/4}{\int }_{0}^{1/4}{y}^{2}{dxdy}
$$

$$
= {\left. \frac{6}{20} \cdot \frac{{x}^{2}}{2}\right| }_{x = 0}^{x = 1/4} + {\left. \frac{6}{20} \cdot \frac{{y}^{3}}{3}\right| }_{y = 0}^{y = 1/4} = \frac{7}{640}
$$

$$
= {.0109}
$$

The marginal pdf of each variable can be obtained in a manner analogous to what we did in the case of two discrete variables. The marginal pdf of $X$ at the value $x$ results from holding $x$ fixed in the pair $\left( {x, y}\right)$ and integrating the joint pdf over $y$ . Integrating the joint pdf with respect to $x$ gives the marginal pdf of $Y$ .

---

DEFINITION

---

The marginal probability density functions of $X$ and $Y$ , denoted by ${f}_{X}\left( x\right)$ and ${f}_{Y}\left( y\right)$ , respectively, are given by

$$
{f}_{X}\left( x\right) = {\int }_{-\infty }^{\infty }f\left( {x, y}\right) {dy}\;\text{ for } - \infty < x < \infty
$$

$$
{f}_{Y}\left( y\right) = {\int }_{-\infty }^{\infty }f\left( {x, y}\right) {dx}\;\text{ for } - \infty < y < \infty
$$

EXAMPLE 5.4 The marginal pdf of $X$ , which gives the probability distribution of busy time for the (Example 5.3 drive-up facility without reference to the walk-up window, is

continued)

$$
{f}_{X}\left( x\right) = {\int }_{-\infty }^{\infty }f\left( {x, y}\right) {dy} = {\int }_{0}^{1}\frac{6}{5}\left( {x + {y}^{2}}\right) {dy} = \frac{6}{5}x + \frac{2}{5}
$$

for $0 \leq x \leq 1$ and 0 otherwise. The marginal pdf of $Y$ is

$$
{f}_{Y}\left( y\right) = \left\{ \begin{matrix} \frac{6}{5}{y}^{2} + \frac{3}{5} & 0 \leq y \leq 1 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

Then

$$
P\left( {{.25} \leq Y \leq {.75}}\right) = {\int }_{.25}^{.75}{f}_{Y}\left( y\right) {dy} = \frac{37}{80} = {.4625}
$$

In Example 5.3, the region of positive joint density was a rectangle, which made computation of the marginal pdf's relatively easy. Consider now an example in which the region of positive density is more complicated.

EXAMPLE 5.5 A nut company markets cans of deluxe mixed nuts containing almonds, cashews, and peanuts. Suppose the net weight of each can is exactly $1\mathrm{{lb}}$ , but the weight contribution of each type of nut is random. Because the three weights sum to 1 , a joint probability model for any two gives all necessary information about the weight of the third type. Let $X =$ the weight of almonds in a selected can and $Y =$ the weight of cashews. Then the region of positive density is $D = \{ \left( {x, y}\right) : 0 \leq x \leq 1,0 \leq y \leq 1, x + y \leq 1\}$ , the shaded region pictured in Figure 5.2.

![0192609f-6f5c-74c9-8588-c1ef28b2184d_5_978_895_444_326_0.jpg](images/0192609f-6f5c-74c9-8588-c1ef28b2184d_5_978_895_444_326_0.jpg)

Figure 5.2 Region of positive density for Example 5.5

Now let the joint pdf for $\left( {X, Y}\right)$ be

$$
f\left( {x, y}\right) = \left\{ \begin{matrix} {24xy} & 0 \leq x \leq 1,0 \leq y \leq 1, x + y \leq 1 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

For any fixed $x, f\left( {x, y}\right)$ increases with $y$ ; for fixed $y, f\left( {x, y}\right)$ increases with $x$ . This is appropriate because the word deluxe implies that most of the can should consist of almonds and cashews rather than peanuts, so that the density function should be large near the upper boundary and small near the origin. The surface determined by $f\left( {x, y}\right)$ slopes upward from zero as $\left( {x, y}\right)$ moves away from either axis.

Clearly, $f\left( {x, y}\right) \geq 0$ . To verify the second condition on a joint pdf, recall that a double integral is computed as an iterated integral by holding one variable fixed (such as $x$ as in Figure 5.2), integrating over values of the other variable lying along the straight line passing through the value of the fixed variable, and finally integrating over all possible values of the fixed variable. Thus

$$
{\int }_{-\infty }^{\infty }{\int }_{-\infty }^{\infty }f\left( {x, y}\right) {dydx} = {\int }_{D}\int f\left( {x, y}\right) {dydx} = {\int }_{0}^{1}\left\{ {{\int }_{0}^{1 - x}{24xydy}}\right\} {dx}
$$

$$
= {\int }_{0}^{1}{24x}\left\{ {\left. \frac{{y}^{2}}{2}\right| }_{y = 0}^{y = 1 - x}\right\} {dx} = {\int }_{0}^{1}{12x}{\left( 1 - x\right) }^{2}{dx} = 1
$$

To compute the probability that the two types of nuts together make up at most ${50}\%$ of the can, let $A = \{ \left( {x, y}\right) : 0 \leq x \leq 1,0 \leq y \leq 1$ , and $x + y \leq {.5}\}$ , as shown in Figure 5.3. Then

$$
P\left( {\left( {X, Y}\right) \in A}\right) = {\iint }_{A}f\left( {x, y}\right) {dxdy} = {\int }_{0}^{.5}{\int }_{0}^{{.5} - x}{24xydydx} = {.0625}
$$

![0192609f-6f5c-74c9-8588-c1ef28b2184d_6_799_469_564_433_0.jpg](images/0192609f-6f5c-74c9-8588-c1ef28b2184d_6_799_469_564_433_0.jpg)

Figure 5.3 Computing $P\left\lbrack {\left( {X, Y}\right) \in A}\right\rbrack$ for Example 5.5

The marginal pdf for almonds is obtained by holding $X$ fixed at $x$ and integrating the joint pdf $f\left( {x, y}\right)$ along the vertical line through $x$ :

$$
{f}_{X}\left( x\right) = {\int }_{-\infty }^{\infty }f\left( {x, y}\right) {dy} = \left\{ \begin{matrix} {\int }_{0}^{1 - x}{24xydy} = {12x}{\left( 1 - x\right) }^{2} & 0 \leq x \leq 1 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

By symmetry of $f\left( {x, y}\right)$ and the region $D$ , the marginal pdf of $Y$ is obtained by replacing $x$ and $X$ in ${f}_{X}\left( x\right)$ by $y$ and $Y$ , respectively.

### 5.1.3 Independent Random Variables

In many situations, information about the observed value of one of the two variables $X$ and $Y$ gives information about the value of the other variable. In Example 5.1, the marginal probability of $X$ at $x = {100}$ is .35 and at $x = {1000}$ is .25 . However, if we learn that $Y = {5000}$ , the last column of the joint probability table tells us that $X$ can’t possibly be 100 and the other two possibilities, 500 and 1000, are now equally likely. Thus knowing the value of $Y$ changes the distribution of $X$ ; in such situations it is natural to say that there is a dependence between the two variables.

In Chapter 2, we pointed out that one way of defining independence of two events is via the condition $P\left( {A \cap B}\right) = P\left( A\right) \cdot P\left( B\right)$ . Here is an analogous definition for the independence of two rv's.

Two random variables $X$ and $Y$ are said to be independent if for every pair of $x$ and $y$ values

$$
p\left( {x, y}\right) = {p}_{X}\left( x\right) \cdot {p}_{Y}\left( y\right) \;\text{when}X\text{and}Y\text{are discrete}
$$

or

(5.1)

$$
f\left( {x, y}\right) = {f}_{X}\left( x\right) \cdot {f}_{Y}\left( y\right) \;\text{when}X\text{and}Y\text{are continuous}
$$

If (5.1) is not satisfied for all $\left( {x, y}\right)$ , then $X$ and $Y$ are said to be dependent.

The definition says that two variables are independent if their joint pmf or pdf is the product of the two marginal pmf's or pdf's. Intuitively, independence says that knowing the value of one of the variables does not provide additional information about what the value of the other variable might be. That is, the distribution of one variable does not depend on the value of the other variable.

EXAMPLE 5.6 In the insurance situation of Examples 5.1 and 5.2,

$$
p\left( {{1000},{5000}}\right) = {.05} \neq \left( {.10}\right) \left( {.25}\right) = {p}_{X}\left( {1000}\right) \cdot {p}_{Y}\left( {5000}\right)
$$

so $X$ and $Y$ are not independent. In fact, the joint probability table has an entry which is 0 , yet the corresponding row and column totals are both positive. Independence of $X$ and $Y$ requires that every entry in the joint probability table be the product of the corresponding row and column marginal probabilities.

---

(Example 5.5

continued)

---

Because $f\left( {x, y}\right)$ has the form of a product, $X$ and $Y$ would appear to be independent. However, although ${f}_{X}\left( {3/4}\right) = {f}_{Y}\left( {3/4}\right) = 9/{16}, f\left( {3/4,3/4}\right) = 0 \neq 9/{16} \cdot 9/{16}$ , so the variables are not in fact independent. To be independent, $f\left( {x, y}\right)$ must have the form $g\left( x\right) \cdot h\left( y\right)$ and the region of positive density must be a rectangle whose sides are parallel to the coordinate axes.

Independence of two random variables is most useful when the description of the experiment under study suggests that $X$ and $Y$ have no effect on one another. Then once the marginal pmf's or pdf's have been specified, the joint pmf or pdf is simply the product of the two marginal functions. It follows that

$$
P\left( {a \leq X \leq b, c \leq Y \leq d}\right) = P\left( {a \leq X \leq b}\right) \cdot P\left( {c \leq Y \leq d}\right)
$$

E 5.8 Suppose that the lifetimes of two components are independent of one another and that the first lifetime, ${X}_{1}$ , has an exponential distribution with parameter ${\lambda }_{1}$ , whereas the second, ${X}_{2}$ , has an exponential distribution with parameter ${\lambda }_{2}$ . Then the joint pdf is

$$
f\left( {{x}_{1},{x}_{2}}\right) = {f}_{{X}_{1}}\left( {x}_{1}\right) \cdot {f}_{{X}_{2}}\left( {x}_{2}\right)
$$

$$
= \left\{ \begin{matrix} {\lambda }_{1}{e}^{-{\lambda }_{1}{x}_{1}} \cdot {\lambda }_{2}{e}^{-{\lambda }_{2}{x}_{2}} = {\lambda }_{1}{\lambda }_{2}{e}^{-{\lambda }_{1}{x}_{1} - {\lambda }_{2}{x}_{2}} & {x}_{1} > 0,{x}_{2} > 0 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

Let ${\lambda }_{1} = 1/{1000}$ and ${\lambda }_{2} = 1/{1200}$ , so that the expected lifetimes are 1000 hours and 1200 hours, respectively. The probability that both component lifetimes are at least 1500 hours is

$$
P\left( {{1500} \leq {X}_{1},{1500} \leq {X}_{2}}\right) = P\left( {{1500} \leq {X}_{1}}\right) \cdot P\left( {{1500} \leq {X}_{2}}\right)
$$

$$
= {e}^{-{\lambda }_{1}\left( {1500}\right) } \cdot {e}^{-{\lambda }_{2}\left( {1500}\right) }
$$

$$
= \left( {.2231}\right) \left( {.2865}\right) = {.0639}
$$

### 5.1.4 More Than Two Random Variables

To model the joint behavior of more than two random variables, we extend the concept of a joint distribution of two variables.

---

DEFINITION

---

If ${X}_{1},{X}_{2},\ldots ,{X}_{n}$ are all discrete random variables, the joint pmf of the variables is the function

$$
p\left( {{x}_{1},{x}_{2},\ldots ,{x}_{n}}\right) = P\left( {{X}_{1} = {x}_{1},{X}_{2} = {x}_{2},\ldots ,{X}_{n} = {x}_{n}}\right)
$$

If the variables are continuous, the joint pdf of ${X}_{1},\ldots ,{X}_{n}$ is the function $f\left( {{x}_{1},{x}_{2},\ldots ,{x}_{n}}\right)$ such that for any $n$ intervals $\left\lbrack {{a}_{1},{b}_{1}}\right\rbrack ,\ldots ,\left\lbrack {{a}_{n},{b}_{n}}\right\rbrack$ ,

$$
P\left( {{a}_{1} \leq {X}_{1} \leq {b}_{1},\ldots ,{a}_{n} \leq {X}_{n} \leq {b}_{n}}\right) = {\int }_{{a}_{1}}^{{b}_{1}}\ldots {\int }_{{a}_{n}}^{{b}_{n}}f\left( {{x}_{1},\ldots ,{x}_{n}}\right) d{x}_{n}\ldots d{x}_{1}
$$

EXAMPLE 5.9 A binomial experiment consists of $n$ dichotomous (success-failure), homogenous (constant success probability) independent trials. Now consider a trinomial experiment in which each of the $n$ trials can result in one of three possible outcomes. For example, each successive customer at a store might pay with cash, a credit card, or a debit card. The trials are assumed independent. Let ${p}_{1} = P$ (trial results in a type 1 outcome) and define ${p}_{2}$ and ${p}_{3}$ analogously for type 2 and type 3 outcomes. The random variables of interest here are ${X}_{i} =$ the number of trials that result in a type $i$ outcome for $i = 1,2,3$ .

In $n = {10}$ trials, the probability that the first five are type 1 outcomes, the next three are type 2, and the last two are type 3-that is, the probability of the experimental outcome 1111122233-is ${p}_{1}^{5} \cdot {p}_{2}^{3} \cdot {p}_{3}^{2}$ . This is also the probability of the outcome 1122311123, and in fact the probability of any outcome that has exactly five 1 ’s, three 2’s, and two 3’s. Now to determine the probability $P\left( {{X}_{1} = 5,{X}_{2} = 3}\right.$ , and ${X}_{3} = 2$ ), we have to count the number of outcomes that have exactly five 1 ’s, three 2 ’s, and two 3’s. First, there are $\left( \begin{matrix} {10} \\ 5 \end{matrix}\right)$ ways to choose five of the trials to be the type 1 outcomes. Now from the remaining five trials, we choose three to be the type 2 outcomes, which can be done in $\left( \begin{array}{l} 5 \\ 3 \end{array}\right)$ ways. This determines the remaining two trials, which consist of type 3 outcomes. So the total number of ways of choosing five 1 's, three 2's, and two 3's is

$$
\left( \begin{matrix} {10} \\ 5 \end{matrix}\right) \cdot \left( \begin{array}{l} 5 \\ 3 \end{array}\right) = \frac{{10}!}{5!5!} \cdot \frac{5!}{3!2!} = \frac{{10}!}{5!3!2!} = {2520}
$$

Thus we see that $P\left( {{X}_{1} = 5,{X}_{2} = 3,{X}_{3} = 2}\right) = {2520}{p}_{1}^{5} \cdot {p}_{2}^{3} \cdot {p}_{3}^{2}$ . Generalizing this to $n$ trials gives

$$
p\left( {{x}_{1},{x}_{2},{x}_{3}}\right) = P\left( {{X}_{1} = {x}_{1},{X}_{2} = {x}_{2},{X}_{3} = {x}_{3}}\right) = \frac{n!}{{x}_{1}!{x}_{2}!{x}_{3}!}{p}_{1}^{{x}_{1}}{p}_{2}^{{x}_{2}}{p}_{3}^{{x}_{3}}
$$

for ${x}_{1} = 0,1,2,\ldots ;{x}_{2} = 0,1,2,\ldots ;{x}_{3} = 0,1,2,\ldots$ such that ${x}_{1} + {x}_{2} + {x}_{3} = n$ . Notice that whereas there are three random variables here, the third variable ${X}_{3}$ is actually redundant. For example, in the case $n = {10}$ , having ${X}_{1} = 5$ and ${X}_{2} = 3$ implies that ${X}_{3} = 2$ (just as in a binomial experiment there are actually two rv’s-the number of successes and number of failures - but the latter is redundant).

As a specific example, the genetic allele of a pea section can be either AA, Aa, or aa. A simple genetic model specifies $P\left( \mathrm{{AA}}\right) = {.25}, P\left( \mathrm{{Aa}}\right) = {.50}$ , and $P\left( \mathrm{{aa}}\right) = {.25}$ . If the alleles of 10 independently obtained sections are determined, the probability that exactly five of these are $\mathrm{{Aa}}$ and two are $\mathrm{{AA}}$ is

$$
p\left( {2,5,3}\right) = \frac{{10}!}{2!5!3!}{\left( {.25}\right) }^{2}{\left( {.50}\right) }^{5}{\left( {.25}\right) }^{3} = {0.769}
$$

A natural extension of the trinomial scenario is an experiment consisting of $n$ independent and identical trials, in which each trial can result in any one of $r$ possible outcomes. Let ${p}_{i} = P$ (outcome $i$ on any particular trial), and define random variables by ${X}_{i} =$ the number of trials resulting in outcome $i\left( {i = 1,\ldots , r}\right)$ . This is called a multinomial experiment, and the joint pmf of ${X}_{1},\ldots ,{X}_{r}$ is called the multinomial distribution. An argument analogous to the one used to derive the trinomial pmf gives the multinomial pmf as

$$
p\left( {{x}_{1},\ldots ,{x}_{r}}\right)
$$

$$
= \left\{ \begin{matrix} \frac{n!}{\left( {{x}_{1}!}\right) \left( {{x}_{2}!}\right) \cdot \ldots \cdot \left( {{x}_{r}!}\right) } \cdot {p}_{1}^{{x}_{1}} \cdot \ldots \cdot {p}_{r}^{{x}_{r}} & {x}_{i} = 0,1,2,\ldots ;\;{x}_{1} + \cdots + {x}_{r} = n \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

EXAMPLE 5.10 When a certain method is used to collect a fixed volume of rock samples in a region, there are four resulting rock types. Let ${X}_{1},{X}_{2}$ , and ${X}_{3}$ denote the proportion by volume of rock types 1, 2, and 3 in a randomly selected sample (the proportion of rock type 4 is $1 - {X}_{1} - {X}_{2} - {X}_{3}$ , so a variable ${X}_{4}$ would be redundant). If the joint pdf of ${X}_{1},{X}_{2},{X}_{3}$ is

$$
f\left( {{x}_{1},{x}_{2},{x}_{3}}\right)
$$

$$
= \left\{ \begin{matrix} k{x}_{1}{x}_{2}\left( {1 - {x}_{3}}\right) & 0 \leq {x}_{1} \leq 1,0 \leq {x}_{2} \leq 1,0 \leq {x}_{3} \leq 1,{x}_{1} + {x}_{2} + {x}_{3} \leq 1 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

then $k$ is determined by

$$
1 = {\int }_{-\infty }^{\infty }{\int }_{-\infty }^{\infty }{\int }_{-\infty }^{\infty }f\left( {{x}_{1},{x}_{2},{x}_{3}}\right) d{x}_{3}d{x}_{2}d{x}_{1}
$$

$$
= {\int }_{0}^{1}\left\{ {{\int }_{0}^{1 - {x}_{1}}\left\lbrack {{\int }_{0}^{1 - {x}_{1} - {x}_{2}}k{x}_{1}{x}_{2}\left( {1 - {x}_{3}}\right) d{x}_{3}}\right\rbrack d{x}_{2}}\right\} d{x}_{1}
$$

This iterated integral has value $k/{144}$ , so $k = {144}$ . The probability that rocks of types 1 and 2 together account for at most ${50}\%$ of the sample is

$$
P\left( {{X}_{1} + {X}_{2} \leq {.5}}\right) = \;\iiint f\left( {{x}_{1},{x}_{2},{x}_{3}}\right) d{x}_{3}d{x}_{2}d{x}_{1}
$$

$$
\left\{ \begin{matrix} 0 \leq {x}_{i} \leq 1\text{ for }i = 1,2,3 \\ {x}_{1} + {x}_{2} + {x}_{3} \leq 1,{x}_{1} + {x}_{2} \leq {.5} \end{matrix}\right\}
$$

$$
= {\int }_{0}^{.5}\left\{ {{\int }_{0}^{{.5} - {x}_{1}}\left\lbrack {{\int }_{0}^{1 - {x}_{1} - {x}_{2}}{144}{x}_{1}{x}_{2}\left( {1 - {x}_{3}}\right) d{x}_{3}}\right\rbrack d{x}_{2}}\right\} d{x}_{1}
$$

$$
= {.6066}
$$

The notion of independence of more than two random variables is similar to the notion of independence of more than two events.

DEFINITION The random variables ${X}_{1},{X}_{2},\ldots ,{X}_{n}$ are said to be independent if for every subset ${X}_{{i}_{1}},{X}_{{i}_{2}},\ldots ,{X}_{{i}_{k}}$ of the variables (each pair, each triple, and so on), the joint pmf or pdf of the subset is equal to the product of the marginal pmf's or pdf's.

Thus if the variables are independent with $n = 4$ , then the joint pmf or pdf of any two variables is the product of the two marginals, and similarly for any three variables and all four variables together. Intuitively, independence means that learning the values of some variables doesn't change the distribution of the remaining variables. Most importantly, once we are told that $n$ variables are independent, then the joint pmf or pdf is the product of the $n$ marginals.

EXAMPLE 5.11 If ${X}_{1},\ldots ,{X}_{n}$ represent the lifetimes of $n$ components, the components operate independently of one another, and each lifetime is exponentially distributed with parameter $\lambda$ , then for ${x}_{1} \geq 0,{x}_{2} \geq 0,\ldots ,{x}_{n} \geq 0$ ,

$$
f\left( {{x}_{1},{x}_{2},\ldots ,{x}_{n}}\right) = \left( {\lambda {e}^{-\lambda {x}_{1}}}\right) \cdot \left( {\lambda {e}^{-\lambda {x}_{2}}}\right) \cdot \cdots \cdot \left( {\lambda {e}^{-\lambda {x}_{n}}}\right) = {\lambda }^{n}{e}^{-{\lambda \sum }{x}_{i}}
$$

Suppose a system consisting of these components will fail as soon as a single component fails. Let $T$ represent system lifetime. Then the probability that the system lasts past time $t$ is

$$
P\left( {T > t}\right) = P\left( {{X}_{1} > t,\ldots ,{X}_{n} > t}\right) = {\int }_{t}^{\infty }\ldots {\int }_{t}^{\infty }f\left( {{x}_{1},\ldots ,{x}_{n}}\right) d{x}_{1}\ldots d{x}_{n}
$$

$$
= \left( {{\int }_{t}^{\infty }\lambda {e}^{-\lambda {x}_{1}}d{x}_{1}}\right) \ldots \left( {{\int }_{t}^{\infty }\lambda {e}^{-\lambda {x}_{n}}d{x}_{n}}\right)
$$

$$
= {\left( {e}^{-{\lambda t}}\right) }^{n} = {e}^{-{n\lambda t}}
$$

Therefore,

$$
P\left( {\text{ system lifetime } \leq t}\right) = 1 - {e}^{-{n\lambda t}}\;\text{ for }t \geq 0
$$

which shows that system lifetime has an exponential distribution with parameter ${n\lambda }$ ; the expected value of system lifetime is $1/{n\lambda }$ .

A variation on the foregoing scenario appeared in the article "A Method for Correlating Field Life Degradation with Reliability Prediction for Electronic Modules" (Quality and Reliability Engr. Intl., 2005: 715-726). The investigators considered a circuit card with $n$ soldered chip resistors. The failure time of a card is the minimum of the individual solder connection failure times (mileages here). It was assumed that the solder connection failure mileages were independent, that failure mileage would exceed $t$ if and only if the shear strength of a connection exceeded a threshold $d$ , and that each shear strength was normally distributed with a mean value and standard deviation that depended on the value of mileage $t : \mu \left( t\right) = {a}_{1} - {a}_{2}t$ and $\sigma \left( t\right) = {a}_{3} + {a}_{4}t$ (a weld’s shear strength typically deteriorates and becomes more variable as mileage increases). Then the probability that the failure mileage of a card exceeds $t$ is

$$
P\left( {T > t}\right) = {\left( 1 - \Phi \left( \frac{d - \left( {{a}_{1} - {a}_{2}t}\right. }{{a}_{3} + {a}_{4}t}\right) \right) }^{n}
$$

The cited article suggested values for $d$ and the ${a}_{i}$ ’s based on data. In contrast to the exponential scenario, normality of individual lifetimes does not imply normality of system lifetime.

In many experimental situations to be considered in this book, independence is a reasonable assumption, so that specifying the joint distribution reduces to deciding on appropriate marginal distributions.

### 5.1.5 Conditional Distributions

Suppose $X =$ the number of major defects in a randomly selected new automobile and $Y =$ the number of minor defects in that same auto. If we learn that the selected car has one major defect, what now is the probability that the car has at most three minor defects-that is, what is $P\left( {Y \leq 3 \mid X = 1}\right)$ ? Similarly, if $X$ and $Y$ denote the lifetimes of the front and rear tires on a motorcycle, and it happens that $X = {10},{000}$ miles, what now is the probability that $Y$ is at most 15,000 miles, and what is the expected lifetime of the rear tire "conditional on" this value of $X$ ? Questions of this sort can be answered by studying conditional probability distributions.

---

DEFINITION

---

Let $X$ and $Y$ be two continuous rv’s with joint pdf $f\left( {x, y}\right)$ and marginal $X$ pdf ${f}_{X}\left( x\right)$ . Then for any $X$ value $x$ for which ${f}_{X}\left( x\right) > 0$ , the conditional probability density function of $Y$ given that $X = x$ is

$$
{f}_{Y \mid X}\left( {y \mid x}\right) = \frac{f\left( {x, y}\right) }{{f}_{X}\left( x\right) }\; - \infty < y < \infty
$$

If $X$ and $Y$ are discrete, replacing pdf’s by pmf’s in this definition gives the conditional probability mass function of $Y$ when $X = x$ .

Notice that the definition of ${f}_{Y \mid X}\left( {y \mid x}\right)$ parallels that of $P\left( {B \mid A}\right)$ , the conditional probability that $B$ will occur, given that $A$ has occurred. Once the conditional pdf or pmf has been determined, questions of the type posed at the outset of this subsection can be answered by integrating or summing over an appropriate set of $Y$ values.

EXAMPLE 5.12 Reconsider the situation of Examples 5.3 and 5.4 involving $X =$ the proportion of time that a bank’s drive-up facility is busy and $Y =$ the analogous proportion for the walk-up window. The conditional pdf of $Y$ given that $X = {.8}$ is

$$
{f}_{Y \mid X}\left( {y \mid {.8}}\right) = \frac{f\left( {{.8}, y}\right) }{{f}_{X}\left( {.8}\right) } = \frac{{1.2}\left( {{.8} + {y}^{2}}\right) }{{1.2}\left( {.8}\right) + {.4}} = \frac{1}{34}\left( {{24} + {30}{y}^{2}}\right) \;0 < y < 1
$$

The probability that the walk-up facility is busy at most half the time given that $X = {.8}$ is then

$$
P\left( {Y \leq {.5} \mid X = {.8}}\right) = {\int }_{-\infty }^{.5}{f}_{Y \mid X}\left( {y \mid {.8}}\right) {dy} = {\int }_{0}^{.5}\frac{1}{34}\left( {{24} + {30}{y}^{2}}\right) {dy} = {.390}
$$

Using the marginal pdf of $Y$ gives $P\left( {Y \leq {.5}}\right) = {.350}$ . Also $E\left( Y\right) = {.6}$ , whereas the expected proportion of time that the walk-up facility is busy given that $X = {.8}$ (a conditional expectation) is

$$
E\left( {Y \mid X = {.8}}\right) = {\int }_{-\infty }^{\infty }y \cdot {f}_{Y \mid X}\left( {y \mid {.8}}\right) {dy} = \frac{1}{34}{\int }_{0}^{1}y\left( {{24} + {30}{y}^{2}}\right) {dy} = {.574}
$$

If the two variables are independent, the marginal pmf or pdf in the denominator will cancel the corresponding factor in the numerator. The conditional distribution is then identical to the corresponding marginal distribution. 

### EXERCISES Section 5.1 (1-21)
1. A service station has both self-service and full-service islands. On each island, there is a single regular unleaded pump with two hoses. Let $X$ denote the number of hoses being used on the self-service island at a particular time, and let $Y$ denote the number of hoses on the full-service island in use at that time. The joint pmf of $X$ and $Y$ appears in the accompanying tabulation.

<table><thead><tr><th rowspan="2">$p\left( {x, y}\right)$</th><th colspan="4">$y$</th></tr><tr><th></th><th>0</th><th>1</th><th>2</th></tr></thead><tr><td></td><td>0</td><td>.10</td><td>.04</td><td>.02</td></tr><tr><td>$x$</td><td>1</td><td>.08</td><td>.20</td><td>.06</td></tr><tr><td></td><td>2</td><td>.06</td><td>.14</td><td>.30</td></tr></table>

a. What is $P\left( {X = 1\text{and}Y = 1}\right)$ ?

b. Compute $P\left( {X \leq 1\text{and}Y \leq 1}\right)$ .

c. Give a word description of the event $\{ X \neq 0$ and $Y \neq 0\}$ , and compute the probability of this event.

d. Compute the marginal pmf of $X$ and of $Y$ . Using ${p}_{X}\left( x\right)$ , what is $P\left( {X \leq 1}\right)$ ?

e. Are $X$ and $Y$ independent rv’s? Explain.

2. A large but sparsely populated county has two small hospitals, one at the south end of the county and the other at the north end. The south hospital's emergency room has four beds, whereas the north hospital's emergency room has only three beds. Let $X$ denote the number of south beds occupied at a particular time on a given day, and let $Y$ denote the number of north beds occupied at the same time on the same day. Suppose that these two rv’s are independent; that the pmf of $X$ puts probability masses ${.1},{.2},{.3},{.2}$ , and .2 on the $x$ values $0,1,2,3$ , and 4, respectively; and that the pmf of $Y$ distributes probabilities ${.1},{.3},{.4}$ , and .2 on the $y$ values $0,1,2$ , and 3 , respectively.

a. Display the joint pmf of $X$ and $Y$ in a joint probability table.

b. Compute $P\left( {X \leq 1\text{and}Y \leq 1}\right)$ by adding probabilities from the joint pmf, and verify that this equals the product of $P\left( {X \leq 1}\right)$ and $P\left( {Y \leq 1}\right)$ .

c. Express the event that the total number of beds occupied at the two hospitals combined is at most 1 in terms of $X$ and $Y$ , and then calculate this probability.

d. What is the probability that at least one of the two hospitals has no beds occupied?

3. A certain market has both an express checkout line and a superexpress checkout line. Let ${X}_{1}$ denote the number of customers in line at the express checkout at a particular time of day, and let ${X}_{2}$ denote the number of customers in line at the superexpress checkout at the same time. Suppose the joint pmf of ${X}_{1}$ and ${X}_{2}$ is as given in the accompanying table.

${x}_{2}$

<table><thead><tr><th colspan="2"></th><th>0</th><th>1</th><th>2</th><th>3</th></tr></thead><tr><td rowspan="6">${x}_{1}$</td><td>0</td><td>.08</td><td>.07</td><td>.04</td><td>.00</td></tr><tr><td>1</td><td>.06</td><td>.15</td><td>.05</td><td>.04</td></tr><tr><td>2</td><td>.05</td><td>.04</td><td>.10</td><td>.06</td></tr><tr><td>3</td><td>.00</td><td>.03</td><td>.04</td><td>.07</td></tr><tr><td>4</td><td>.00</td><td>.01</td><td>.05</td><td>.06</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr></table>

a. What is $P\left( {{X}_{1} = 1,{X}_{2} = 1}\right)$ , that is, the probability that there is exactly one customer in each line?

b. What is $P\left( {{X}_{1} = {X}_{2}}\right)$ , that is, the probability that the numbers of customers in the two lines are identical?

c. Let $A$ denote the event that there are at least two more customers in one line than in the other line. Express $A$ in terms of ${X}_{1}$ and ${X}_{2}$ , and calculate the probability of this event.

d. What is the probability that the total number of customers in the two lines is exactly four? At least four?

4. Return to the situation described in Exercise 3.

a. Determine the marginal pmf of ${X}_{1}$ , and then calculate the expected number of customers in line at the express checkout.

b. Determine the marginal pmf of ${X}_{2}$ .

c. By inspection of the probabilities $P\left( {{X}_{1} = 4}\right)$ , $P\left( {{X}_{2} = 0}\right)$ , and $P\left( {{X}_{1} = 4,{X}_{2} = 0}\right)$ , are ${X}_{1}$ and ${X}_{2}$ independent random variables? Explain.

5. The number of customers waiting for gift-wrap service at a department store is an rv $X$ with possible values $0,1,2$ ,

3, 4 and corresponding probabilities .1, .2, .3, .25, .15. A randomly selected customer will have 1,2 , or 3 packages for wrapping with probabilities ${.6},{.3}$ , and .1, respectively. Let $Y =$ the total number of packages to be wrapped for the customers waiting in line (assume that the number of packages submitted by one customer is independent of the number submitted by any other customer).

a. Determine $P\left( {X = 3, Y = 3}\right)$ , i.e., $p\left( {3,3}\right)$ .

b. Determine $p\left( {4,{11}}\right)$ .

6. Let $X$ denote the number of Canon SLR cameras sold during a particular week by a certain store. The pmf of $X$ is

<table><tr><td>$x$</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td></tr><tr><td>${p}_{X}\left( x\right)$</td><td>.1</td><td>.2</td><td>.3</td><td>.25</td><td>.15</td></tr></table>

Sixty percent of all customers who purchase these cameras also buy an extended warranty. Let $Y$ denote the number of purchasers during this week who buy an extended warranty.

a. What is $P\left( {X = 4, Y = 2}\right)$ ? [Hint: This probability equals $P\left( {Y = 2 \mid X = 4}\right) \cdot P\left( {X = 4}\right)$ ; now think of the four purchases as four trials of a binomial experiment, with success on a trial corresponding to buying an extended warranty.]

b. Calculate $P\left( {X = Y}\right)$ .

c. Determine the joint pmf of $X$ and $Y$ and then the marginal pmf of $Y$ .

7. The joint probability distribution of the number $X$ of cars and the number $Y$ of buses per signal cycle at a proposed left-turn lane is displayed in the accompanying joint probability table.

<table><thead><tr><th colspan="4">$y$</th></tr><tr><th>$p\left( {x, y}\right)$</th><th>0</th><th>1</th><th>2</th></tr></thead><tr><td>0</td><td>.025</td><td>.015</td><td>.010</td></tr><tr><td>1</td><td>.050</td><td>.030</td><td>.020</td></tr><tr><td>2</td><td>.125</td><td>.075</td><td>.050</td></tr><tr><td>$x$3</td><td>.150</td><td>.090</td><td>.060</td></tr><tr><td>4</td><td>.100</td><td>.060</td><td>.040</td></tr><tr><td>5</td><td>.050</td><td>.030</td><td>.020</td></tr></table>

a. What is the probability that there is exactly one car and exactly one bus during a cycle?

b. What is the probability that there is at most one car and at most one bus during a cycle?

c. What is the probability that there is exactly one car during a cycle? Exactly one bus?

d. Suppose the left-turn lane is to have a capacity of five cars, and that one bus is equivalent to three cars. What is the probability of an overflow during a cycle?

e. Are $X$ and $Y$ independent rv’s? Explain.

8. A stockroom currently has 30 components of a certain type, of which 8 were provided by supplier 1,10 by supplier 2, and 12 by supplier 3 . Six of these are to be randomly selected for a particular assembly. Let $X =$ the number of supplier 1’s components selected, $Y =$ the number of supplier 2’s components selected, and $p\left( {x, y}\right)$ denote the joint pmf of $X$ and $Y$ .

a. What is $p\left( {3,2}\right)$ ? [Hint: Each sample of size 6 is equally likely to be selected. Therefore, $p\left( {3,2}\right) =$ (number of outcomes with $X = 3$ and $Y = 2)/($ total number of outcomes). Now use the product rule for counting to obtain the numerator and denominator.]

b. Using the logic of part (a), obtain $p\left( {x, y}\right)$ . (This can be thought of as a multivariate hypergeometric distribution-sampling without replacement from a finite population consisting of more than two categories.)

9. Each front tire on a particular type of vehicle is supposed to be filled to a pressure of ${26}\mathrm{{psi}}$ . Suppose the actual air pressure in each tire is a random variable $- X$ for the right tire and $Y$ for the left tire, with joint pdf

$$
f\left( {x, y}\right) = \left\{ \begin{matrix} K\left( {{x}^{2} + {y}^{2}}\right) & {20} \leq x \leq {30},{20} \leq y \leq {30} \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

a. What is the value of $K$ ?

b. What is the probability that both tires are underfilled?

c. What is the probability that the difference in air pressure between the two tires is at most 2 psi?

d. Determine the (marginal) distribution of air pressure in the right tire alone.

e. Are $X$ and $Y$ independent rv’s?

10. Annie and Alvie have agreed to meet between 5:00 P.M. and 6:00 P.M. for dinner at a local health-food restaurant. Let $X =$ Annie’s arrival time and $Y =$ Alvie’s arrival time. Suppose $X$ and $Y$ are independent with each uniformly distributed on the interval $\left\lbrack {5,6}\right\rbrack$ .

a. What is the joint pdf of $X$ and $Y$ ?

b. What is the probability that they both arrive between 5:15 and 5:45?

c. If the first one to arrive will wait only ${10}\mathrm{\;{min}}$ before leaving to eat elsewhere, what is the probability that they have dinner at the health-food restaurant? [Hint: The event of interest is $A = \{ \left( {x, y}\right) : \left| {x - y}\right| \leq 1/6\}$ .]

11. Two different professors have just submitted final exams for duplication. Let $X$ denote the number of typographical errors on the first professor’s exam and $Y$ denote the number of such errors on the second exam. Suppose $X$ has a Poisson distribution with parameter ${\mu }_{1}, Y$ has a Poisson distribution with parameter ${\mu }_{2}$ , and $X$ and $Y$ are independent.

a. What is the joint pmf of $X$ and $Y$ ?

b. What is the probability that at most one error is made on both exams combined?

c. Obtain a general expression for the probability that the total number of errors in the two exams is $m$ (where $m$ is a nonnegative integer). [Hint: $A =$ $\{ \left( {x, y}\right) : x + y = m\} = \{ \left( {m,0}\right) ,\left( {m - 1,1}\right) ,\ldots$ , $\left( {1, m - 1}\right) ,\left( {0, m}\right) \}$ . Now sum the joint pmf over $\left( {x, y}\right) \in A$ and use the binomial theorem, which says that

$$
\mathop{\sum }\limits_{{k = 0}}^{m}\left( \begin{matrix} m \\ k \end{matrix}\right) {a}^{k}{b}^{m - k} = {\left( a + b\right) }^{m}
$$

for any $a, b$ .]

12. Two components of a minicomputer have the following joint pdf for their useful lifetimes $X$ and $Y$ :

$$
f\left( {x, y}\right) = \left\{ \begin{matrix} x{e}^{-x\left( {1 + y}\right) } & x \geq 0\text{ and }y \geq 0 \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

a. What is the probability that the lifetime $X$ of the first component exceeds 3 ?

b. What are the marginal pdf’s of $X$ and $Y$ ? Are the two lifetimes independent? Explain.

c. What is the probability that the lifetime of at least one component exceeds 3 ?

13. You have two lightbulbs for a particular lamp. Let $X =$ the lifetime of the first bulb and $Y =$ the lifetime of the second bulb (both in 1000s of hours). Suppose that $X$ and $Y$ are independent and that each has an exponential distribution with parameter $\lambda = 1$ .

a. What is the joint pdf of $X$ and $Y$ ?

b. What is the probability that each bulb lasts at most 1000 hours (i.e., $X \leq 1$ and $Y \leq 1$ )?

c. What is the probability that the total lifetime of the two bulbs is at most 2 ? [Hint: Draw a picture of the region $A = \{ \left( {x, y}\right) : x \geq 0, y \geq 0, x + y \leq 2\}$ before integrating.]

d. What is the probability that the total lifetime is between 1 and 2 ?

14. Suppose that you have ten lightbulbs, that the lifetime of each is independent of all the other lifetimes, and that each lifetime has an exponential distribution with parameter $\lambda$ .

a. What is the probability that all ten bulbs fail before time $t$ ?

b. What is the probability that exactly $k$ of the ten bulbs fail before time $t$ ?

c. Suppose that nine of the bulbs have lifetimes that are exponentially distributed with parameter $\lambda$ and that the remaining bulb has a lifetime that is exponentially distributed with parameter $\theta$ (it is made by another manufacturer). What is the probability that exactly five of the ten bulbs fail before time $t$ ?

15. Consider a system consisting of three components as pictured. The system will continue to function as long as the first component functions and either component 2 or component 3 functions. Let ${X}_{1},{X}_{2}$ , and ${X}_{3}$ denote the lifetimes of components $1,2$ , and 3, respectively. Suppose the ${X}_{i}$ ’s are independent of one another and each ${X}_{i}$ has an exponential distribution with parameter $\lambda$ .

![0192609f-6f5c-74c9-8588-c1ef28b2184d_14_300_1177_415_153_0.jpg](images/0192609f-6f5c-74c9-8588-c1ef28b2184d_14_300_1177_415_153_0.jpg)

a. Let $Y$ denote the system lifetime. Obtain the cumulative distribution function of $Y$ and differentiate to obtain the pdf. [Hint: $F\left( y\right) = P\left( {Y \leq y}\right)$ ; express the event $\{ Y \leq y\}$ in terms of unions and/or intersections of the three events $\left\{ {{X}_{1} \leq y}\right\} ,\left\{ {{X}_{2} \leq y}\right\}$ , and $\left. {\left\{ {{X}_{3} \leq y}\right\} \text{.}}\right\rbrack$

b. Compute the expected system lifetime.

16. a. For $f\left( {{x}_{1},{x}_{2},{x}_{3}}\right)$ as given in Example 5.10, compute the joint marginal density function of ${X}_{1}$ and ${X}_{3}$ alone (by integrating over ${x}_{2}$ ).

b. What is the probability that rocks of types 1 and 3 together make up at most ${50}\%$ of the sample? [Hint: Use the result of part (a).]

c. Compute the marginal pdf of ${X}_{1}$ alone. [Hint: Use the result of part (a).]

17. An ecologist wishes to select a point inside a circular sampling region according to a uniform distribution (in practice this could be done by first selecting a direction and then a distance from the center in that direction). Let $X =$ the $x$ coordinate of the point selected and $Y =$ the $y$ coordinate of the point selected. If the circle is centered at $\left( {0,0}\right)$ and has radius $R$ , then the joint pdf of $X$ and $Y$ is

$$
f\left( {x, y}\right) = \left\{ \begin{matrix} \frac{1}{\pi {R}^{2}} & {x}^{2} + {y}^{2} \leq {R}^{2} \\ 0 & \text{ otherwise } \end{matrix}\right.
$$

a. What is the probability that the selected point is within $R/2$ of the center of the circular region? [Hint: Draw a picture of the region of positive density $D$ . Because $f\left( {x, y}\right)$ is constant on $D$ , computing a probability reduces to computing an area.]

b. What is the probability that both $X$ and $Y$ differ from 0 by at most $R/2$ ?

c. Answer part (b) for $R/\sqrt{2}$ replacing $R/2$ .

d. What is the marginal pdf of $X$ ? Of $Y$ ? Are $X$ and $Y$ independent?

18. Refer to Exercise 1 and answer the following questions:

a. Given that $X = 1$ , determine the conditional pmf of $Y$ -i.e., ${p}_{Y \mid X}\left( {0 \mid 1}\right) ,{p}_{Y \mid X}\left( {1 \mid 1}\right)$ , and ${p}_{Y \mid X}\left( {2 \mid 1}\right)$ .

b. Given that two hoses are in use at the self-service island, what is the conditional pmf of the number of hoses in use on the full-service island?

c. Use the result of part (b) to calculate the conditional probability $P\left( {Y \leq 1 \mid X = 2}\right)$ .

d. Given that two hoses are in use at the full-service island, what is the conditional pmf of the number in use at the self-service island?

19. The joint pdf of pressures for right and left front tires is given in Exercise 9.

a. Determine the conditional pdf of $Y$ given that $X = x$ and the conditional pdf of $X$ given that $Y = y$ .

b. If the pressure in the right tire is found to be ${22}\mathrm{{psi}}$ , what is the probability that the left tire has a pressure of at least 25 psi? Compare this to $P\left( {Y \geq {25}}\right)$ .

c. If the pressure in the right tire is found to be ${22}\mathrm{{psi}}$ , what is the expected pressure in the left tire, and what is the standard deviation of pressure in this tire?

20. Let ${X}_{1},{X}_{2},{X}_{3},{X}_{4},{X}_{5}$ , and ${X}_{6}$ denote the numbers of blue, brown, green, orange, red, and yellow M&M candies, respectively, in a sample of size $n$ . Then these ${X}_{i}$ ’s have a multinomial distribution. According to the M&M Web site, the color proportions are ${p}_{1} = {.24},{p}_{2} = {.13}$ , ${p}_{3} = {.16},{p}_{4} = {.20},{p}_{5} = {.13}$ , and ${p}_{6} = {.14}$ .

a. If $n = {12}$ , what is the probability that there are exactly two M&Ms of each color?

b. For $n = {20}$ , what is the probability that there are at most five orange candies? [Hint: Think of an orange candy as a success and any other color as a failure.]

c. In a sample of ${20}\mathrm{M}\& \mathrm{{Ms}}$ , what is the probability that the number of candies that are blue, green, or orange is at least 10 ?

21. Let ${X}_{1},{X}_{2}$ , and ${X}_{3}$ be the lifetimes of components 1,2, and 3 in a three-component system.

a. How would you define the conditional pdf of ${X}_{3}$ given that ${X}_{1} = {x}_{1}$ and ${X}_{2} = {x}_{2}$ ?

b. How would you define the conditional joint pdf of ${X}_{2}$ and ${X}_{3}$ given that ${X}_{1} = {x}_{1}$ ?